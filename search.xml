<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>FAA 的 UTM 运行概念 2.0</title>
      <link href="/vll-pages/posts/650859ed.html"/>
      <url>/vll-pages/posts/650859ed.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style>]]></content>
      
      
      <categories>
          
          <category> 运行概念 </category>
          
          <category> UTM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 运行概念 </tag>
            
            <tag> UTM </tag>
            
            <tag> FAA </tag>
            
            <tag> 美国 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FAA 的 UAM 运行概念 2.0</title>
      <link href="/vll-pages/posts/48c0ef5d.html"/>
      <url>/vll-pages/posts/48c0ef5d.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 空域管理 </category>
          
          <category> UAM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 运行概念 </tag>
            
            <tag> FAA </tag>
            
            <tag> 空域管理 </tag>
            
            <tag> UAM </tag>
            
            <tag> AAM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/vll-pages/posts/4a17b156.html"/>
      <url>/vll-pages/posts/4a17b156.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>测试一下动态编译</p><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>测试</title>
      <link href="/vll-pages/posts/9daba997.html"/>
      <url>/vll-pages/posts/9daba997.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h1 id="Combining-heterogeneous-spatial-datasets-with-process-based-spatial-fusion-models-A-unifying-framework"><a href="#Combining-heterogeneous-spatial-datasets-with-process-based-spatial-fusion-models-A-unifying-framework" class="headerlink" title="Combining heterogeneous spatial datasets with process-based spatial fusion models: A unifying framework"></a>Combining heterogeneous spatial datasets with process-based spatial fusion models: A unifying framework</h1><p>Author links open overlay panelCraig Wang <sup>a</sup>, Reinhard Furrer <sup>a</sup> <sup>b</sup>, SNC Study Group</p><p>Show more</p><p>Outline</p><p>Add to Mendeley</p><p>Share</p><p>Cite</p><p><a href="https://doi.org/10.1016/j.csda.2021.107240" title="Persistent link using digital object identifier">https://doi.org/10.1016/j.csda.2021.107240</a><a href="https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S0167947321000748&orderBeanReset=true">Get rights and content</a></p><p>Under a Creative Commons <a href="http://creativecommons.org/licenses/by-nc-nd/4.0/">license</a></p><p>open access</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>In modern spatial <a href="https://www.sciencedirect.com/topics/mathematics/statistics" title="Learn more about statistics from ScienceDirect's AI-generated Topic Pages">statistics</a>, the structure of data has become more heterogeneous. Depending on the types of spatial data, different modeling strategies are used. For example, kriging approaches for geostatistical data; Gaussian <a href="https://www.sciencedirect.com/topics/mathematics/markov-random-fields" title="Learn more about Markov random field from ScienceDirect's AI-generated Topic Pages">Markov random field</a> models for <a href="https://www.sciencedirect.com/topics/mathematics/lattices" title="Learn more about lattice from ScienceDirect's AI-generated Topic Pages">lattice</a> data; or log Gaussian Cox process models for point-pattern data. Despite these different modeling choices, the nature of underlying data-generating (latent) processes is often the same, which can be represented by some continuous spatial surfaces. A unifying framework is introduced for process-based multivariate spatial fusion models. The framework can jointly analyze all three aforementioned types of spatial data or any combinations thereof. Moreover, the framework accommodates different likelihoods for geostatistical and lattice data. It is shown that some established approaches, such as linear models of coregionalization, can be viewed as special cases of the proposed framework. A flexible and scalable implementation using R-INLA is provided. Simulation studies confirm that the prediction of latent processes improves as one moves from univariate spatial models to multivariate spatial fusion models. The framework is illustrated via a case study using datasets from a cross-sectional study linked with a national cohort in Switzerland. The differences in underlying spatial risks between respiratory disease and lung cancer are examined in the case study.</p><ul><li><a href="https://www.sciencedirect.com/science/article/pii/S0167947321000840">Previous article in issue</a></li><li><a href="https://www.sciencedirect.com/science/article/pii/S0167947321000876">Next article in issue</a></li></ul><h2 id="Keywords"><a href="#Keywords" class="headerlink" title="Keywords"></a>Keywords</h2><p>Bayesian methods</p><p>Change of support problem</p><p>Data fusion</p><p>Gaussian process</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>In modern spatial <a href="https://www.sciencedirect.com/topics/mathematics/statistics" title="Learn more about statistics from ScienceDirect's AI-generated Topic Pages">statistics</a>, researchers are dealing with increased heterogeneity in the structure of collected spatial data. Different data sources may contain overlapping information concerning the same research questions. In addition, combining different datasets can mitigate the problem of limited sample size and imperfect measurements&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b10">Diggle and Lophaven, 2006</a>). In public health and epidemiology, <a href="https://www.sciencedirect.com/topics/engineering/joints-structural-components" title="Learn more about joint from ScienceDirect's AI-generated Topic Pages">joint</a> analysis of multiple diseases borrows information from related diseases to account for the underlying correlations and to uncover common risk factors.</p><p>Spatial models are useful when residuals exhibit correlation in space after known <a href="https://www.sciencedirect.com/topics/mathematics/covariate" title="Learn more about covariates from ScienceDirect's AI-generated Topic Pages">covariates</a> are accounted for in a regression-type setting. Spatial data has long been classified into three categories, namely geostatistical (point-level) data, <a href="https://www.sciencedirect.com/topics/mathematics/lattices" title="Learn more about lattice from ScienceDirect's AI-generated Topic Pages">lattice</a> (area-level) data and point-pattern data&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b7">Cressie, 1991</a>). Depending on <a href="https://www.sciencedirect.com/topics/computer-science/data-type" title="Learn more about data types from ScienceDirect's AI-generated Topic Pages">data types</a>, different statistical models that capture the residual <a href="https://www.sciencedirect.com/topics/mathematics/spatial-correlation" title="Learn more about spatial correlation from ScienceDirect's AI-generated Topic Pages">spatial correlation</a> are used. In a nutshell, (1) geostatistical data are observations with geo-coordinates. For example, rainfall measurements at locations of weather stations&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b27">Kyriakidis et al., 2001</a>). The strength of dependency can be modeled as a function of the distance separation between two locations. (2) Lattice data can be either gridded or irregularly aligned, and occur in the form of aggregated observation over areas. They are often collected in epidemiology, such as disease prevalence of each district&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b6">Chammartin et al., 2016</a>). Gaussian <a href="https://www.sciencedirect.com/topics/mathematics/markov-random-fields" title="Learn more about Markov random field from ScienceDirect's AI-generated Topic Pages">Markov random field</a> (GMRF) models such as conditionally <a href="https://www.sciencedirect.com/topics/mathematics/autoregressive-model" title="Learn more about autoregressive models from ScienceDirect's AI-generated Topic Pages">autoregressive models</a> are typically used to capture the spatial dependency between neighboring areas. Finally, (3) point-pattern data are observations where the locations themselves are stochastic. They are used to model epidemiological data of case locations&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b16">Gatrell et al., 1996</a>) or other event locations such as epicenter of earthquakes&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b39">Ogata, 1988</a>). One approach to model such data is using a point process, where the <a href="https://www.sciencedirect.com/topics/mathematics/intensity-function-lambda" title="Learn more about intensity function from ScienceDirect's AI-generated Topic Pages">intensity function</a> may depend on <a href="https://www.sciencedirect.com/topics/mathematics/observed-covariates" title="Learn more about observed covariates from ScienceDirect's AI-generated Topic Pages">observed covariates</a>&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b17">Gelfand et al., 2010</a>).</p><p>Despite there are different modeling strategies for different types of data, a common purpose of all the aforementioned statistical models is to capture the residual spatial dependency between different observations after known covariates are accounted for. A natural way to do this is using <a href="https://www.sciencedirect.com/topics/mathematics/gaussian-process" title="Learn more about Gaussian processes from ScienceDirect's AI-generated Topic Pages">Gaussian processes</a> to model continuous spatial surfaces, which represent the underlying scientific process that drives response variables together with observed covariates. It is beneficial to jointly model heterogeneous types of spatial data on the underlying process instead of observed <a href="https://www.sciencedirect.com/topics/computer-science/spatial-support" title="Learn more about spatial support from ScienceDirect's AI-generated Topic Pages">spatial support</a>. Several authors&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b38">Moraga et al., 2017</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b47">Shi and Kang, 2017</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b51">Wang et al., 2018</a>) demonstrated improved prediction performance when multiple datasets were modeled together compared to a single dataset via both simulation study and real-world applications. In addition, better scientific interpretations can be obtained by modeling different data types at the underlying process&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b37">Møller et al., 1998</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b24">Kelsall and Wakefield, 2002</a>).</p><p>In order to model spatial data at a different support than that from the observed data, change of support problem occurs. There are methods proposed to analyze lattice data and point-pattern data using Gaussian process-based models.&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b24">Kelsall and Wakefield (2002)</a> modeled aggregated disease counts using a Gaussian process approach. Point-pattern data can be linked to Gaussian process with a log-Gaussian Cox process (LGCP)&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b37">Møller et al., 1998</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b11">Diggle et al., 2013</a>). Taking a step further, several approaches used Gaussian process as a basis to fuse spatial data of different types. We refer to these approaches as spatial fusion models. Some earlier work on spatial fusion models&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b3">Berrocal et al., 2010</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b35">McMillan et al., 2010</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b45">Sahu et al., 2010</a>) implemented efficient algorithms for specific model structures bundled with their data applications.&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b38">Moraga et al. (2017)</a> proposed a model to analyze spatial data available in both geostatistical and lattice type, with the same set of covariates and a response variable observed at two different spatial resolutions.&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b53">Wilson and Wakefield (2018)</a> extended the work by allowing non-Gaussian response variables.&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b47">Shi and Kang (2017)</a> proposed a fixed rank kriging-based fusion model to combine multiple lattice-type remote sensing datasets.&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b51">Wang et al. (2018)</a> utilized a nearest neighbor Gaussian process to combine geostatistical and lattice data with a flexible model structure.</p><p>In general, spatial fusion models are challenged by the trade-off between flexibility and computational efficiency, i.e.,&nbsp;more flexible modeling structures require more generalizable inferential tools that come with a higher computational cost. For example, the fusion model in&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b53">Wilson and Wakefield (2018)</a> with normally-distributed response variables can take advantage of <a href="https://www.sciencedirect.com/topics/mathematics/stochastic-partial-differential-equation" title="Learn more about stochastic partial differential equation from ScienceDirect's AI-generated Topic Pages">stochastic partial differential equation</a> (SPDE) approaches and <a href="https://www.sciencedirect.com/topics/mathematics/laplace-approximation" title="Learn more about INLA from ScienceDirect's AI-generated Topic Pages">INLA</a> which took few minutes to run. A more flexible modeling structure for Poisson-distributed response requires using Hamiltonian Monte Carlo-based inference method and took several weeks.</p><p>In this paper, we build on the work in&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b51">Wang et al. (2018)</a> and extend fusion modeling in two aspects. In terms of flexibility, our framework incorporates an additional data type, namely point-pattern data. To the best of our knowledge, this is the first spatial fusion framework that incorporates all three types of spatial data. We additionally allow arbitrary combinations of those three data types in multivariate settings. We propose a unifying framework that includes features of several well-established models, such as linear models of coregionalization (LMC)&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b50">Wackernagel, 2003</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b33">MacNab, 2016</a>), spatial factor model&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b52">Wang and Wall, 2003</a>) and shared component model&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b25">Knorr-Held and Best, 2001</a>). In terms of computational cost, we offer an efficient SPDE and INLA-based implementation which significantly reduces computation time from hours to minutes for thousands of observations. Last but not least, we benchmark the performance of our implementation in terms of prediction and parameter estimation in simulation studies.</p><p>The rest of this paper is structured as follows: Section&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#sec2">2</a> introduces the unifying framework and explicitly shows its link to existing spatial models. Section&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#sec3">3</a> discusses implementation strategy using the SPDE approach and INLA. Section&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#sec4">4</a> illustrates the framework using two simulated scenarios and a re-analysis on epidemiological datasets to model respiratory disease risk in Kanton Zurich, Switzerland. Finally, we end with a summary followed by discussions on <a href="https://www.sciencedirect.com/topics/mathematics/identifiability" title="Learn more about identifiability from ScienceDirect's AI-generated Topic Pages">identifiability</a> problems and research outlook in Section&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#sec5">5</a>.</p><h2 id="2-Process-based-spatial-fusion-model"><a href="#2-Process-based-spatial-fusion-model" class="headerlink" title="2. Process-based spatial fusion model"></a>2. Process-based spatial fusion model</h2><h3 id="2-1-The-unifying-framework"><a href="#2-1-The-unifying-framework" class="headerlink" title="2.1. The unifying framework"></a>2.1. The unifying framework</h3><p>For $<math><mrow is="true"><mi is="true">j</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><mi is="true">ℓ</mi></mrow></math>$$<math><mrow is="true"><mi is="true">j</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><mi is="true">ℓ</mi></mrow></math>$, we let $<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mrow is="true"><mo is="true">{</mo><msub is="true"><mrow is="true"><mi is="true">Y</mi></mrow><mrow is="true"><mi is="true">j</mi><mo is="true">,</mo><mn is="true">1</mn></mrow></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">Y</mi></mrow><mrow is="true"><mi is="true">j</mi><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></mrow></msub><mo is="true">}</mo></mrow></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mrow is="true"><mo is="true">{</mo><msub is="true"><mrow is="true"><mi is="true">Y</mi></mrow><mrow is="true"><mi is="true">j</mi><mo is="true">,</mo><mn is="true">1</mn></mrow></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">Y</mi></mrow><mrow is="true"><mi is="true">j</mi><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></mrow></msub><mo is="true">}</mo></mrow></mrow></math>$ denote the $<math><mi is="true">j</mi></math>$$<math><mi is="true">j</mi></math>$th response variable with $<math><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></math>$ observations, with a likelihood that belongs to the <a href="https://www.sciencedirect.com/topics/mathematics/exponential-family" title="Learn more about exponential family from ScienceDirect's AI-generated Topic Pages">exponential family</a>. Each of the $<math><mi is="true">ℓ</mi></math>$$<math><mi is="true">ℓ</mi></math>$ responses can take any of the following <a href="https://www.sciencedirect.com/topics/computer-science/data-type" title="Learn more about data types from ScienceDirect's AI-generated Topic Pages">data types</a>: (i) geostatistical data, observed at locations $<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">s</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">∈</mo><mi is="true">D</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">⊆</mo><msup is="true"><mrow is="true"><mi mathvariant="script" is="true">R</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">s</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">∈</mo><mi is="true">D</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">⊆</mo><msup is="true"><mrow is="true"><mi mathvariant="script" is="true">R</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></mrow></math>$; (ii) <a href="https://www.sciencedirect.com/topics/mathematics/lattices" title="Learn more about lattice from ScienceDirect's AI-generated Topic Pages">lattice</a> data observed at areas $<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">⊂</mo><mi is="true">D</mi></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">⊂</mo><mi is="true">D</mi></mrow></math>$; or (iii) point-pattern data that has been discretized to regular fine grid containing mostly zeros or ones, observed at gridded locations $<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">v</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">∈</mo><mi is="true">D</mi></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">v</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">∈</mo><mi is="true">D</mi></mrow></math>$, where $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></math>$ denotes the number of events in the grid cell containing $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">v</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">v</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></math>$. Further, we let $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></math>$ denote a full (column) rank $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></mrow></math>$ matrix of spatially-referenced <a href="https://www.sciencedirect.com/topics/mathematics/covariate" title="Learn more about covariates from ScienceDirect's AI-generated Topic Pages">covariates</a> that are observed at the same spatial units as the corresponding response variables, $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></math>$ denote a vector $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">×</mo><mn is="true">1</mn></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">×</mo><mn is="true">1</mn></mrow></math>$ of fixed effect coefficients. We assume there is a $<math><mrow is="true"><mi is="true">q</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">×</mo><mn is="true">1</mn></mrow></math>$$<math><mrow is="true"><mi is="true">q</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">×</mo><mn is="true">1</mn></mrow></math>$ vector of zero-mean, unit variance, independent latent <a href="https://www.sciencedirect.com/topics/mathematics/gaussian-process" title="Learn more about Gaussian processes from ScienceDirect's AI-generated Topic Pages">Gaussian processes</a> $<math><mi mathvariant="bold-italic" is="true">w</mi></math>$$<math><mi mathvariant="bold-italic" is="true">w</mi></math>$ having a $<math><mrow is="true"><mi is="true">ℓ</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">×</mo><mi is="true">q</mi></mrow></math>$$<math><mrow is="true"><mi is="true">ℓ</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">×</mo><mi is="true">q</mi></mrow></math>$ <a href="https://www.sciencedirect.com/topics/mathematics/design-matrix" title="Learn more about design matrix from ScienceDirect's AI-generated Topic Pages">design matrix</a> $<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$$<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$ with rows $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></math>$, i.e.&nbsp;$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mi mathvariant="bold-italic" is="true">w</mi></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mi mathvariant="bold-italic" is="true">w</mi></mrow></math>$ is the $<math><mi is="true">j</mi></math>$$<math><mi is="true">j</mi></math>$th <a href="https://www.sciencedirect.com/topics/mathematics/linear-combination" title="Learn more about linear combination from ScienceDirect's AI-generated Topic Pages">linear combination</a> of Gaussian processes. Each Gaussian process is parameterized by its own <a href="https://www.sciencedirect.com/topics/mathematics/covariance-function" title="Learn more about covariance function from ScienceDirect's AI-generated Topic Pages">covariance function</a>. Finally, non-linear operator $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$ subsets and aggregates some components of $<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mi mathvariant="bold-italic" is="true">w</mi></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mi mathvariant="bold-italic" is="true">w</mi></mrow></math>$ such that the linear combination matches the spatial resolution of corresponding response variable. Overall, the framework is formulated as (1)$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">g</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi mathvariant="double-struck" is="true">E</mi><mrow is="true"><mo is="true">[</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">∣</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">,</mo><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">]</mo></mrow><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo linebreak="goodbreak" is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><mspace width="2em" is="true"></mspace><mi is="true">j</mi><mo linebreak="goodbreak" is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><mi is="true">ℓ</mi><mo is="true">,</mo></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">g</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi mathvariant="double-struck" is="true">E</mi><mrow is="true"><mo is="true">[</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">∣</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">,</mo><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">]</mo></mrow><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo linebreak="goodbreak" is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><mspace width="2em" is="true"></mspace><mi is="true">j</mi><mo linebreak="goodbreak" is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><mi is="true">ℓ</mi><mo is="true">,</mo></mrow></math>$where $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">g</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">g</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$ is a link function that corresponds to the <a href="https://www.sciencedirect.com/topics/engineering/conditional-distribution" title="Learn more about conditional distribution from ScienceDirect's AI-generated Topic Pages">conditional distribution</a> of $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></math>$.</p><p>Although the latent processes have a continuous index, we work with a finite set of locations in practice. The set of locations $<math><mi mathvariant="script" is="true">U</mi></math>$$<math><mi mathvariant="script" is="true">U</mi></math>$ to be modeled in the latent processes $<math><mi mathvariant="bold-italic" is="true">w</mi></math>$$<math><mi mathvariant="bold-italic" is="true">w</mi></math>$ comprises of locations where geostatistical data are observed, locations of sampling points for lattice data and gridded locations for point-pattern data. The non-linear operator $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$ takes a different form depending on data types. For geostatistical and point-pattern data, the non-linear operator $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$ subsets $<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mi mathvariant="bold-italic" is="true">w</mi></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mi mathvariant="bold-italic" is="true">w</mi></mrow></math>$ to the corresponding locations of the $<math><mi is="true">j</mi></math>$$<math><mi is="true">j</mi></math>$th response variable. For lattice data, a change of support problem arises&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b21">Gotway and Young, 2002</a>) since we only observe aggregated information while the underlying process is continuous. When the link function is linear, $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$ subsets $<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mi mathvariant="bold-italic" is="true">w</mi></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mi mathvariant="bold-italic" is="true">w</mi></mrow></math>$ to the sampling point locations and aggregates them to the corresponding areas by taking averages. With non-linear link functions, $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$ first applies an inverse link function and then aggregates.</p><p>When modeling lattice data, we employ a sampling-points <a href="https://www.sciencedirect.com/topics/computer-science/approximation-algorithm" title="Learn more about approximation from ScienceDirect's AI-generated Topic Pages">approximation</a> approach to <a href="https://www.sciencedirect.com/topics/mathematics/stochastic-integral" title="Learn more about stochastic integrals from ScienceDirect's AI-generated Topic Pages">stochastic integrals</a>&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b18">Gelfand et al., 2001</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b13">Fuentes and Raftery, 2005</a>) for aggregating latent processes. Let $<math><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">s</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup></math>$$<math><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">s</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup></math>$ denote the set of all sampling points and let $<math><mi is="true">H</mi></math>$$<math><mi is="true">H</mi></math>$ denote the number of sampling points of each area. Further, to simplify the notation, we denote with $<math><mi is="true">w</mi></math>$$<math><mi is="true">w</mi></math>$ an arbitrary component of the $<math><mi is="true">q</mi></math>$$<math><mi is="true">q</mi></math>$ spatial latent process. For the $<math><mi is="true">i</mi></math>$$<math><mi is="true">i</mi></math>$th area $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></math>$ in $<math><mi is="true">j</mi></math>$$<math><mi is="true">j</mi></math>$th response, under a linear link function we obtain (2)$<math><mrow is="true"><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">|</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub><mo is="true">|</mo></mrow></mrow><mrow is="true"><mo linebreak="badbreak" is="true">−</mo><mn is="true">1</mn></mrow></msup><msub is="true"><mrow is="true"><mo linebreak="badbreak" is="true">∫</mo></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi><mo linebreak="badbreak" is="true">∈</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></mrow></msub><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold-italic" is="true">u</mi><mo is="true">)</mo></mrow><mi is="true">d</mi><mi mathvariant="bold-italic" is="true">u</mi><mo linebreak="goodbreak" is="true">≈</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">H</mi></mrow></mfrac><munder is="true"><mrow is="true"><mo linebreak="badbreak" is="true">∑</mo></mrow><mrow is="true"><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">s</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mo linebreak="badbreak" is="true">∈</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></mrow></munder><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">s</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>$$<math><mrow is="true"><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">|</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub><mo is="true">|</mo></mrow></mrow><mrow is="true"><mo linebreak="badbreak" is="true">−</mo><mn is="true">1</mn></mrow></msup><msub is="true"><mrow is="true"><mo linebreak="badbreak" is="true">∫</mo></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi><mo linebreak="badbreak" is="true">∈</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></mrow></msub><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold-italic" is="true">u</mi><mo is="true">)</mo></mrow><mi is="true">d</mi><mi mathvariant="bold-italic" is="true">u</mi><mo linebreak="goodbreak" is="true">≈</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">H</mi></mrow></mfrac><munder is="true"><mrow is="true"><mo linebreak="badbreak" is="true">∑</mo></mrow><mrow is="true"><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">s</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mo linebreak="badbreak" is="true">∈</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></mrow></munder><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">s</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>$with $<math><mrow is="true"><mo is="true">|</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub><mo is="true">|</mo></mrow></math>$$<math><mrow is="true"><mo is="true">|</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub><mo is="true">|</mo></mrow></math>$ being the area of $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></math>$, i.e.&nbsp;the latent process at area $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></math>$ is approximated by aggregating the process at sampling points within the area. Ecological bias will arise from non-linear link functions&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b22">Greenland, 1992</a>), that is, the sum of non-linear functions applied to individual $<math><mi is="true">w</mi></math>$$<math><mi is="true">w</mi></math>$ is different from the non-linear function applied to the sum of individual $<math><mi is="true">w</mi></math>$$<math><mi is="true">w</mi></math>$’s. For a general link function $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">g</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">g</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$, we can avoid such bias by using the following approximation instead of <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#fd2">(2)</a>, (3)$<math><mrow is="true"><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">g</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo class="biggl" fence="true" is="true">(</mo><mrow is="true"><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">|</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub><mo is="true">|</mo></mrow></mrow><mrow is="true"><mo linebreak="badbreak" is="true">−</mo><mn is="true">1</mn></mrow></msup><msub is="true"><mrow is="true"><mo linebreak="badbreak" is="true">∫</mo></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi><mo linebreak="badbreak" is="true">∈</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></mrow></msub><msubsup is="true"><mrow is="true"><mi is="true">g</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mo linebreak="badbreak" is="true">−</mo><mn is="true">1</mn></mrow></msubsup><mfenced open="(" close=")" is="true"><mrow is="true"><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold-italic" is="true">u</mi><mo is="true">)</mo></mrow></mrow></mfenced><mi is="true">d</mi><mi mathvariant="bold-italic" is="true">u</mi></mrow><mo class="biggr" fence="true" is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">≈</mo><msub is="true"><mrow is="true"><mi is="true">g</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo class="biggl" fence="true" is="true">(</mo><mrow is="true"><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">H</mi></mrow></mfrac><munder is="true"><mrow is="true"><mo linebreak="badbreak" is="true">∑</mo></mrow><mrow is="true"><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">s</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mo linebreak="badbreak" is="true">∈</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></mrow></munder><msubsup is="true"><mrow is="true"><mi is="true">g</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mo linebreak="badbreak" is="true">−</mo><mn is="true">1</mn></mrow></msubsup><mfenced open="(" close=")" is="true"><mrow is="true"><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">s</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mo is="true">)</mo></mrow></mrow></mfenced></mrow><mo class="biggr" fence="true" is="true">)</mo></mrow><mo is="true">.</mo></mrow></math>$$<math><mrow is="true"><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">g</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo class="biggl" fence="true" is="true">(</mo><mrow is="true"><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">|</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub><mo is="true">|</mo></mrow></mrow><mrow is="true"><mo linebreak="badbreak" is="true">−</mo><mn is="true">1</mn></mrow></msup><msub is="true"><mrow is="true"><mo linebreak="badbreak" is="true">∫</mo></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi><mo linebreak="badbreak" is="true">∈</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></mrow></msub><msubsup is="true"><mrow is="true"><mi is="true">g</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mo linebreak="badbreak" is="true">−</mo><mn is="true">1</mn></mrow></msubsup><mfenced open="(" close=")" is="true"><mrow is="true"><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold-italic" is="true">u</mi><mo is="true">)</mo></mrow></mrow></mfenced><mi is="true">d</mi><mi mathvariant="bold-italic" is="true">u</mi></mrow><mo class="biggr" fence="true" is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">≈</mo><msub is="true"><mrow is="true"><mi is="true">g</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mrow is="true"><mo class="biggl" fence="true" is="true">(</mo><mrow is="true"><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">H</mi></mrow></mfrac><munder is="true"><mrow is="true"><mo linebreak="badbreak" is="true">∑</mo></mrow><mrow is="true"><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">s</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mo linebreak="badbreak" is="true">∈</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></mrow></munder><msubsup is="true"><mrow is="true"><mi is="true">g</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mo linebreak="badbreak" is="true">−</mo><mn is="true">1</mn></mrow></msubsup><mfenced open="(" close=")" is="true"><mrow is="true"><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">s</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mo is="true">)</mo></mrow></mrow></mfenced></mrow><mo class="biggr" fence="true" is="true">)</mo></mrow><mo is="true">.</mo></mrow></math>$Under the identity link function $<math><mrow is="true"><mi is="true">g</mi><mrow is="true"><mo is="true">(</mo><mi is="true">x</mi><mo is="true">)</mo></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mi is="true">x</mi></mrow></math>$$<math><mrow is="true"><mi is="true">g</mi><mrow is="true"><mo is="true">(</mo><mi is="true">x</mi><mo is="true">)</mo></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mi is="true">x</mi></mrow></math>$, Eq.&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#fd3">(3)</a> is equivalent to Eq.&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#fd2">(2)</a>. For Poisson response with log link function $<math><mrow is="true"><mi is="true">g</mi><mrow is="true"><mo is="true">(</mo><mi is="true">x</mi><mo is="true">)</mo></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mo class="qopname" is="true">log</mo><mrow is="true"><mo is="true">(</mo><mi is="true">x</mi><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><mi is="true">g</mi><mrow is="true"><mo is="true">(</mo><mi is="true">x</mi><mo is="true">)</mo></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mo class="qopname" is="true">log</mo><mrow is="true"><mo is="true">(</mo><mi is="true">x</mi><mo is="true">)</mo></mrow></mrow></math>$, Eq.&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#fd3">(3)</a> becomes (4)$<math><mrow is="true"><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mo class="qopname" is="true">log</mo><mrow is="true"><mo class="biggl" fence="true" is="true">(</mo><mrow is="true"><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">|</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub><mo is="true">|</mo></mrow></mrow><mrow is="true"><mo linebreak="badbreak" is="true">−</mo><mn is="true">1</mn></mrow></msup><msub is="true"><mrow is="true"><mo class="qopname" linebreak="badbreak" is="true">∫</mo></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi><mo linebreak="badbreak" is="true">∈</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></mrow></msub><mo class="qopname" is="true">exp</mo><mfenced open="(" close=")" is="true"><mrow is="true"><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold-italic" is="true">u</mi><mo is="true">)</mo></mrow></mrow></mfenced><mi is="true">d</mi><mi mathvariant="bold-italic" is="true">u</mi></mrow><mo class="biggr" fence="true" is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">≈</mo><mo class="qopname" is="true">log</mo><mrow is="true"><mo class="biggl" fence="true" is="true">(</mo><mrow is="true"><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">H</mi></mrow></mfrac><munder is="true"><mrow is="true"><mo linebreak="badbreak" is="true">∑</mo></mrow><mrow is="true"><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">s</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mo linebreak="badbreak" is="true">∈</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></mrow></munder><mo class="qopname" is="true">exp</mo><mfenced open="(" close=")" is="true"><mrow is="true"><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">s</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mo is="true">)</mo></mrow></mrow></mfenced></mrow><mo class="biggr" fence="true" is="true">)</mo></mrow><mo is="true">.</mo></mrow></math>$$<math><mrow is="true"><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mo class="qopname" is="true">log</mo><mrow is="true"><mo class="biggl" fence="true" is="true">(</mo><mrow is="true"><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">|</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub><mo is="true">|</mo></mrow></mrow><mrow is="true"><mo linebreak="badbreak" is="true">−</mo><mn is="true">1</mn></mrow></msup><msub is="true"><mrow is="true"><mo class="qopname" linebreak="badbreak" is="true">∫</mo></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi><mo linebreak="badbreak" is="true">∈</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></mrow></msub><mo class="qopname" is="true">exp</mo><mfenced open="(" close=")" is="true"><mrow is="true"><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold-italic" is="true">u</mi><mo is="true">)</mo></mrow></mrow></mfenced><mi is="true">d</mi><mi mathvariant="bold-italic" is="true">u</mi></mrow><mo class="biggr" fence="true" is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">≈</mo><mo class="qopname" is="true">log</mo><mrow is="true"><mo class="biggl" fence="true" is="true">(</mo><mrow is="true"><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">H</mi></mrow></mfrac><munder is="true"><mrow is="true"><mo linebreak="badbreak" is="true">∑</mo></mrow><mrow is="true"><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">s</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mo linebreak="badbreak" is="true">∈</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">a</mi></mrow><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></mrow></munder><mo class="qopname" is="true">exp</mo><mfenced open="(" close=")" is="true"><mrow is="true"><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">s</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mo is="true">)</mo></mrow></mrow></mfenced></mrow><mo class="biggr" fence="true" is="true">)</mo></mrow><mo is="true">.</mo></mrow></math>$Typically, a small $<math><mi is="true">H</mi></math>$$<math><mi is="true">H</mi></math>$ between 5 to 10 is chosen to balance the trade-off between computational efficiency and model accuracy&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b13">Fuentes and Raftery, 2005</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b30">Liu et al., 2011</a>).</p><h3 id="2-2-Linking-to-existing-models"><a href="#2-2-Linking-to-existing-models" class="headerlink" title="2.2. Linking to existing models"></a>2.2. Linking to existing models</h3><p>Our proposed unifying framework utilizes elements from existing literature and combines them to create a flexible yet efficient spatial fusion model framework. As a result, there are some connections in terms of model structure between this framework and other established methods in spatial <a href="https://www.sciencedirect.com/topics/mathematics/statistics" title="Learn more about statistics from ScienceDirect's AI-generated Topic Pages">statistics</a>. At the same time, they share the same potential <a href="https://www.sciencedirect.com/topics/mathematics/identifiability" title="Learn more about identifiability from ScienceDirect's AI-generated Topic Pages">identifiability</a> issues.</p><p>In univariate settings, the unifying framework allows us to model each type of spatial data individually with a latent Gaussian process. When we have geostatistical data, the framework results in a geostatistical regression&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b7">Cressie, 1991</a>). With Poisson-distributed lattice data, we obtain a sampling-points approximation to the model used in&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b24">Kelsall and Wakefield (2002)</a>, which is an alternative modeling strategy to Besag–York–Mollé model&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b4">Besag et al., 1991</a>). With point-pattern data, we obtain a discretized LGCP&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b37">Møller et al., 1998</a>).</p><p>In multivariate geostatistical data settings, the design matrix $<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$$<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$ plays a <a href="https://www.sciencedirect.com/topics/engineering/pivotal-role" title="Learn more about pivotal role from ScienceDirect's AI-generated Topic Pages">pivotal role</a> in the identifiability of model parameters. When the number of independent Gaussian processes is less than the number of responses $<math><mrow is="true"><mi is="true">q</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">&lt;</mo><mi is="true">ℓ</mi></mrow></math>$$<math><mrow is="true"><mi is="true">q</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">&lt;</mo><mi is="true">ℓ</mi></mrow></math>$, we obtain a spatial factor model&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b52">Wang and Wall, 2003</a>). The latent spatial factors are assumed to have zero-mean unit-variance Gaussian processes, such that $<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$$<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$ controls the variance (partial sill) of latent processes. When $<math><mrow is="true"><mi is="true">q</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mi is="true">ℓ</mi></mrow></math>$$<math><mrow is="true"><mi is="true">q</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mi is="true">ℓ</mi></mrow></math>$, we obtain a general coregionalization framework&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b50">Wackernagel, 2003</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b46">Schmidt and Gelfand, 2003</a>). A similar LMC framework also exists for lattice data&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b33">MacNab, 2016</a>). Identifiability issues occur in the LMC since the number of latent values to be estimated in the latent processes is equal to the total number of observations in response variables. Additional spatial hyper-parameters and fixed-effect coefficients also need to be estimated. For this reason, <a href="https://www.sciencedirect.com/topics/mathematics/regularization" title="Learn more about regularization from ScienceDirect's AI-generated Topic Pages">regularization</a> is done via one of the following: (1) employing <a href="https://www.sciencedirect.com/topics/mathematics/empirical-bayes" title="Learn more about empirical Bayes from ScienceDirect's AI-generated Topic Pages">empirical Bayes</a> method by fixing some of the hyper-parameters; (2) choosing informative prior distributions in <a href="https://www.sciencedirect.com/topics/mathematics/bayesian-model" title="Learn more about Bayesian models from ScienceDirect's AI-generated Topic Pages">Bayesian models</a>; or (3) using a <a href="https://www.sciencedirect.com/topics/mathematics/lower-triangular-matrix" title="Learn more about lower triangular matrix from ScienceDirect's AI-generated Topic Pages">lower triangular matrix</a> for $<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$$<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b46">Schmidt and Gelfand, 2003</a>). In cases of $<math><mrow is="true"><mi is="true">q</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">&gt;</mo><mi is="true">ℓ</mi></mrow></math>$$<math><mrow is="true"><mi is="true">q</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">&gt;</mo><mi is="true">ℓ</mi></mrow></math>$, we acquire a similar model structure as shared component models&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b25">Knorr-Held and Best, 2001</a>) for Gaussian processes, where multiple outcomes have their own latent spatial components plus some shared spatial components. In this setting, the values in $<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$$<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$ need to be even further constrained to avoid identifiability issues&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b25">Knorr-Held and Best, 2001</a>).</p><p>Our framework is also linked to other process-based spatial data fusion models that combine geostatistical and lattice data types. When we let the response variables represent the same information with different data types, we obtain the model presented in&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b53">Wilson and Wakefield (2018)</a>, where an explicit relationship is used to link multiple response variables. If we further allow different information to be represented in the response variables, we reach the generalized spatial fusion model framework proposed in&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b51">Wang et al. (2018)</a>.</p><p>To the best of our knowledge, there is no existing approach or implementation that jointly models all three types of spatial data in a multivariate framework. With those links to the existing approaches, our framework extends upon them by combining different features and enhances the overall flexibility of spatial fusion models.</p><h2 id="3-Model-implementations"><a href="#3-Model-implementations" class="headerlink" title="3. Model implementations"></a>3. Model implementations</h2><p>It is well known that fitting full Gaussian processes in <a href="https://www.sciencedirect.com/topics/mathematics/bayesian-model" title="Learn more about Bayesian models from ScienceDirect's AI-generated Topic Pages">Bayesian models</a> is computationally expensive in both univariate and multivariate settings. Marginalized and conjugate Gaussian process models dramatically save computation time but they can only be used when geostatistical data with normally-distributed outcomes is fitted&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b1">Banerjee et al., 2014</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b54">Zhang et al., 2019</a>). There are several approaches to reduce the computational burden, such as low rank&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b8">Cressie and Johannesson, 2008</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b2">Banerjee et al., 2008</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b49">Stein, 2008</a>) and sparse&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b15">Furrer et al., 2006</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b44">Rue et al., 2009</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b9">Datta et al., 2016</a>) methods. Some of those approaches are utilized in existing spatial fusion models.&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b47">Shi and Kang (2017)</a> adapted the spatial basis function approach from fixed rank kriging&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b8">Cressie and Johannesson, 2008</a>).&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b38">Moraga et al. (2017)</a> used integrated nested Laplace <a href="https://www.sciencedirect.com/topics/computer-science/approximation-algorithm" title="Learn more about approximations from ScienceDirect's AI-generated Topic Pages">approximations</a> (INLA)&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b44">Rue et al., 2009</a>).&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b51">Wang et al. (2018)</a> exploited the nearest neighbor Gaussian process (NNGP)&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b9">Datta et al., 2016</a>). In this paper, we offer an efficient implementation strategy for the unifying spatial fusion model framework. The strategy follows&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b53">Wilson and Wakefield (2018)</a> to use the <a href="https://www.sciencedirect.com/topics/mathematics/stochastic-partial-differential-equation" title="Learn more about SPDE from ScienceDirect's AI-generated Topic Pages">SPDE</a> approach and INLA, with additional approximations for non-linear link functions.</p><p>Although the computational efficiency can be improved by using NNGPs instead of full Gaussian processes, it is still not feasible to fit multiple latent processes with more than thousands of locations in $<math><mi mathvariant="script" is="true">U</mi></math>$$<math><mi mathvariant="script" is="true">U</mi></math>$. Therefore, we choose the SPDE approach and INLA for the implementation of the fusion models.</p><p><a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b29">Lindgren et al. (2011)</a> established a connection between Matérn Gaussian process and GMRFs through a SPDE approach, where a Gaussian process over finite collection of locations can be approximated by triangulating the spatial domain and using a weighted sum of basis functions as (5)$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="script" is="true">U</mi></mrow></msub><mo linebreak="goodbreak" is="true">≈</mo><munderover is="true"><mrow is="true"><mo linebreak="badbreak" is="true">∑</mo></mrow><mrow is="true"><mi is="true">k</mi><mo linebreak="badbreak" is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi mathvariant="script" is="true">M</mi></mrow></munderover><msub is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub><mo is="true">,</mo></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="script" is="true">U</mi></mrow></msub><mo linebreak="goodbreak" is="true">≈</mo><munderover is="true"><mrow is="true"><mo linebreak="badbreak" is="true">∑</mo></mrow><mrow is="true"><mi is="true">k</mi><mo linebreak="badbreak" is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi mathvariant="script" is="true">M</mi></mrow></munderover><msub is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub><mo is="true">,</mo></mrow></math>$where $<math><mi mathvariant="script" is="true">M</mi></math>$$<math><mi mathvariant="script" is="true">M</mi></math>$ is the number of points in the triangulation, $<math><msub is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub></math>$ are Gaussian distributed weights and $<math><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub></math>$ are basis functions. The weights $<math><mrow is="true"><mi mathvariant="bold-italic" is="true">r</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mrow is="true"><mo is="true">[</mo><msub is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mi mathvariant="script" is="true">M</mi></mrow></msub><mo is="true">]</mo></mrow></mrow></math>$$<math><mrow is="true"><mi mathvariant="bold-italic" is="true">r</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mrow is="true"><mo is="true">[</mo><msub is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mi mathvariant="script" is="true">M</mi></mrow></msub><mo is="true">]</mo></mrow></mrow></math>$ forms a <a href="https://www.sciencedirect.com/topics/mathematics/markov-random-fields" title="Learn more about GMRF from ScienceDirect's AI-generated Topic Pages">GMRF</a> which follows a <a href="https://www.sciencedirect.com/topics/mathematics/multivariate-normal-distribution" title="Learn more about multivariate normal distribution from ScienceDirect's AI-generated Topic Pages">multivariate normal distribution</a> with a sparse <a href="https://www.sciencedirect.com/topics/computer-science/precision-matrix" title="Learn more about precision matrix from ScienceDirect's AI-generated Topic Pages">precision matrix</a>&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b43">Rue and Held, 2005</a>) that makes computation efficient. In this approximation approach, the <a href="https://www.sciencedirect.com/topics/mathematics/covariance-function" title="Learn more about covariance function from ScienceDirect's AI-generated Topic Pages">covariance function</a> of the Gaussian process must be a member of the Matérn family defined as (6)$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></mrow></msub><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></mrow><mrow is="true"><msup is="true"><mrow is="true"><mn is="true">2</mn></mrow><mrow is="true"><mi is="true">ν</mi><mo linebreak="badbreak" is="true">−</mo><mn is="true">1</mn></mrow></msup><mi is="true">Γ</mi><mrow is="true"><mo is="true">(</mo><mi is="true">ν</mi><mo is="true">)</mo></mrow></mrow></mfrac><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><msqrt is="true"><mrow is="true"><mn is="true">2</mn><mi is="true">ν</mi></mrow></msqrt><mo is="true">‖</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo linebreak="badbreak" is="true">−</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">‖</mo><mo is="true">∕</mo><mi is="true">ϕ</mi><mo is="true">)</mo></mrow></mrow><mrow is="true"><mi is="true">ν</mi></mrow></msup><msub is="true"><mrow is="true"><mi is="true">K</mi></mrow><mrow is="true"><mi is="true">ν</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msqrt is="true"><mrow is="true"><mn is="true">2</mn><mi is="true">ν</mi></mrow></msqrt><mo is="true">‖</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">−</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">‖</mo><mo is="true">∕</mo><mi is="true">ϕ</mi><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></mrow></msub><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></mrow><mrow is="true"><msup is="true"><mrow is="true"><mn is="true">2</mn></mrow><mrow is="true"><mi is="true">ν</mi><mo linebreak="badbreak" is="true">−</mo><mn is="true">1</mn></mrow></msup><mi is="true">Γ</mi><mrow is="true"><mo is="true">(</mo><mi is="true">ν</mi><mo is="true">)</mo></mrow></mrow></mfrac><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><msqrt is="true"><mrow is="true"><mn is="true">2</mn><mi is="true">ν</mi></mrow></msqrt><mo is="true">‖</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo linebreak="badbreak" is="true">−</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">‖</mo><mo is="true">∕</mo><mi is="true">ϕ</mi><mo is="true">)</mo></mrow></mrow><mrow is="true"><mi is="true">ν</mi></mrow></msup><msub is="true"><mrow is="true"><mi is="true">K</mi></mrow><mrow is="true"><mi is="true">ν</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msqrt is="true"><mrow is="true"><mn is="true">2</mn><mi is="true">ν</mi></mrow></msqrt><mo is="true">‖</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">−</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">‖</mo><mo is="true">∕</mo><mi is="true">ϕ</mi><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>$where $<math><mrow is="true"><mo is="true">‖</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">−</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">‖</mo></mrow></math>$$<math><mrow is="true"><mo is="true">‖</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">−</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">‖</mo></mrow></math>$ is the <a href="https://www.sciencedirect.com/topics/mathematics/euclidean-distance" title="Learn more about Euclidean distance from ScienceDirect's AI-generated Topic Pages">Euclidean distance</a> between $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></math>$ and $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></math>$, $<math><msub is="true"><mrow is="true"><mi is="true">K</mi></mrow><mrow is="true"><mi is="true">v</mi></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">K</mi></mrow><mrow is="true"><mi is="true">v</mi></mrow></msub></math>$ is the <a href="https://www.sciencedirect.com/topics/mathematics/modified-bessel-function" title="Learn more about modified Bessel function from ScienceDirect's AI-generated Topic Pages">modified Bessel function</a> of second kind with integer order $<math><mi is="true">ν</mi></math>$$<math><mi is="true">ν</mi></math>$, $<math><msup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></math>$$<math><msup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></math>$ is the partial sill, $<math><mi is="true">ϕ</mi></math>$$<math><mi is="true">ϕ</mi></math>$ relates to the spatial range and $<math><mi is="true">ν</mi></math>$$<math><mi is="true">ν</mi></math>$ is the smoothness parameter. The approximation in Eq.&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#fd5">(5)</a> can be written as $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="script" is="true">U</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">≈</mo><mi is="true">A</mi><mi mathvariant="bold-italic" is="true">r</mi></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="script" is="true">U</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">≈</mo><mi is="true">A</mi><mi mathvariant="bold-italic" is="true">r</mi></mrow></math>$, where $<math><mi is="true">A</mi></math>$$<math><mi is="true">A</mi></math>$ is a <a href="https://www.sciencedirect.com/topics/mathematics/projection-matrix" title="Learn more about projection matrix from ScienceDirect's AI-generated Topic Pages">projection matrix</a> that maps a GMRF defined on the triangulation <a href="https://www.sciencedirect.com/topics/engineering/meshes" title="Learn more about mesh from ScienceDirect's AI-generated Topic Pages">mesh</a> nodes to the observations’ locations.</p><p>INLA, which is suitable for a wide range of latent <a href="https://www.sciencedirect.com/topics/computer-science/gaussian-model" title="Learn more about Gaussian models from ScienceDirect's AI-generated Topic Pages">Gaussian models</a>, can be used to fit this approximation approach for $<math><mrow is="true"><mi is="true">ν</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">∈</mo><mrow is="true"><mo is="true">(</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">]</mo></mrow></mrow></math>$$<math><mrow is="true"><mi is="true">ν</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">∈</mo><mrow is="true"><mo is="true">(</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">]</mo></mrow></mrow></math>$&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b28">Lindgren and Rue, 2015</a>). The key to implementing the spatial fusion models in INLA lies within the projection matrix, with a different structure required for each data type&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b26">Krainski et al., 2018</a>).</p><ul><li><p>•</p><p>For geostatistical data, the $<math><mi is="true">i</mi></math>$$<math><mi is="true">i</mi></math>$th row of the projection matrix corresponds to the $<math><mi is="true">i</mi></math>$$<math><mi is="true">i</mi></math>$th location, it is filled with zeros except where (1) the location is on the $<math><mi is="true">j</mi></math>$$<math><mi is="true">j</mi></math>$th vertex, then the $<math><mi is="true">j</mi></math>$$<math><mi is="true">j</mi></math>$th column is ones or (2) the location is within a triangulation area, then three cells get values based on a mixture of barycentric based weights from three neighboring vertices of the triangulation.</p></li><li><p>•</p><p>For lattice data, we construct a projection matrix that links the $<math><mi is="true">i</mi></math>$$<math><mi is="true">i</mi></math>$th area with the mean value of GRF at mesh nodes which falls into the $<math><mi is="true">i</mi></math>$$<math><mi is="true">i</mi></math>$th area in analogous to Eq.&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#fd2">(2)</a>. If the link function is linear, increasing the mesh density will increase the number of mesh nodes that fall into each area, therefore, better approximates the average. However, it is sufficient to have a sparse mesh for non-linear link functions&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b12">Follestad and Rue, 2003</a>).</p></li><li><p>•</p><p>Finally, for the point-pattern data, we use an augmentation approach by&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b48">Simpson et al. (2016)</a>, which avoids discretizing the spatial domain into grid cells. The projection matrix is built as an <a href="https://www.sciencedirect.com/topics/mathematics/identitymatrix" title="Learn more about identity matrix from ScienceDirect's AI-generated Topic Pages">identity matrix</a> with a dimension equal to the total number of mesh nodes, row-binded with a projection matrix that is constructed on observed locations in the same way as the geostatistical case.</p></li></ul><p>The final model fitting is done by stacking the projection matrices corresponding to each response variable together using inla.stack() and assigning appropriate priors using the <strong>INLA</strong>&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b28">Lindgren and Rue, 2015</a>) package in R.</p><p>Recent advances in <strong>INLA</strong>&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b34">Martins et al., 2013</a>) such as allowing multiple likelihoods and ‘copy’ feature made this implementation possible. When the response variables follow different distributions, the family argument in inla() is used to assign them. The underlying latent process needs to be specified only once and then assigned to different response variables using the copy argument in f() when specifying a model formula. Example R code is available in the supplementary materials.</p><h2 id="4-Illustrations"><a href="#4-Illustrations" class="headerlink" title="4. Illustrations"></a>4. Illustrations</h2><p>In this section, we conduct two simulation studies and an analysis of epidemiological datasets to illustrate our proposed framework. We do not include comparisons with existing methods since they are either inflexible to be fitted with our model structure or computationally infeasible for the datasets. All results are obtained in R version 3.5.0&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b40">R Core Team, 2018</a>), on a Linux server with 256 GB of <a href="https://www.sciencedirect.com/topics/engineering/reliability-availability-and-maintainability-reliability-engineering" title="Learn more about RAM from ScienceDirect's AI-generated Topic Pages">RAM</a> and two Intel Xeon 6-core 2.5&nbsp;GHz processors. All R code used in the simulation studies is provided in the supplementary materials.</p><h3 id="4-1-Simulation-study-one"><a href="#4-1-Simulation-study-one" class="headerlink" title="4.1. Simulation study one"></a>4.1. Simulation study one</h3><p>We are interested in modeling a single latent spatial process within a $<math><mrow is="true"><mrow is="true"><mo is="true">[</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">10</mn><mo is="true">]</mo></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">×</mo><mrow is="true"><mo is="true">[</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">10</mn><mo is="true">]</mo></mrow></mrow></math>$$<math><mrow is="true"><mrow is="true"><mo is="true">[</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">10</mn><mo is="true">]</mo></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">×</mo><mrow is="true"><mo is="true">[</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">10</mn><mo is="true">]</mo></mrow></mrow></math>$ square, using three <a href="https://www.sciencedirect.com/topics/engineering/spatial-response" title="Learn more about spatial response from ScienceDirect's AI-generated Topic Pages">spatial response</a> variables with one response variable from each data type. First, we simulate a zero-mean GRF on densely uniformly distributed locations with a <a href="https://www.sciencedirect.com/topics/mathematics/covariance-matrix" title="Learn more about covariance matrix from ScienceDirect's AI-generated Topic Pages">covariance matrix</a> $<math><mrow is="true"><mi is="true">C</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mi is="true">⋅</mi><mo is="true">,</mo><mi is="true">⋅</mi><mo is="true">;</mo><msup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo is="true">,</mo><mi is="true">ϕ</mi></mrow></mfenced></mrow></math>$$<math><mrow is="true"><mi is="true">C</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mi is="true">⋅</mi><mo is="true">,</mo><mi is="true">⋅</mi><mo is="true">;</mo><msup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo is="true">,</mo><mi is="true">ϕ</mi></mrow></mfenced></mrow></math>$. We then sub-sample 200 locations to obtain the latent process at observed locations. For lattice observations, we divide the square into 100 Voronoi cells and compute aggregated GRF from all locations while accounting for ecological bias using Eq.&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#fd3">(3)</a>. In addition, we generate covariates $<math><msub is="true"><mrow is="true"><mi is="true">X</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">X</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$ and $<math><msub is="true"><mrow is="true"><mi is="true">X</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">X</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$ for geostatistical and lattice response by sampling independently from a <a href="https://www.sciencedirect.com/topics/mathematics/standard-normal-distribution" title="Learn more about standard normal distribution from ScienceDirect's AI-generated Topic Pages">standard normal distribution</a>. Afterwards, we generate a normally-distributed geostatistical response at the same sampled locations and a Poisson-distributed lattice response for each area. For point-pattern observations, we simulate from the same GRF on a coarse 20&nbsp;×&nbsp;20 grid, then exponentiate the values to obtain intensity at the grid cells. Afterwards, we generate <a href="https://www.sciencedirect.com/topics/mathematics/poisson-point-process" title="Learn more about Poisson point process from ScienceDirect's AI-generated Topic Pages">Poisson point process</a> using each intensity value multiplied by cell area and an offset term as the final intensity. In summary, the response variables are generated according to $<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo linebreak="goodbreak" is="true">∣</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">,</mo><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">,</mo><msubsup is="true"><mrow is="true"><mi is="true">τ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><mo id="mmlalignd1e2942" linebreak="goodbreak" is="true">∼</mo><mtext is="true">N</mtext><mfenced open="(" close=")" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><msubsup is="true"><mrow is="true"><mi is="true">τ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><mi mathvariant="bold-italic" is="true">I</mi></mrow></mfenced><mo is="true">,</mo></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo linebreak="goodbreak" is="true">∣</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">,</mo><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">,</mo><msubsup is="true"><mrow is="true"><mi is="true">τ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><mo id="mmlalignd1e2942" linebreak="goodbreak" is="true">∼</mo><mtext is="true">N</mtext><mfenced open="(" close=")" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><msubsup is="true"><mrow is="true"><mi is="true">τ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><mi mathvariant="bold-italic" is="true">I</mi></mrow></mfenced><mo is="true">,</mo></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo linebreak="goodbreak" is="true">∣</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">,</mo><mi mathvariant="bold-italic" is="true">w</mi><mo id="mmlalignd1e3020" linebreak="goodbreak" indentalign="id" indenttarget="mmlalignd1e2942" is="true">∼</mo><mtext is="true">Pois</mtext><mfenced open="(" close=")" is="true"><mrow is="true"><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow></mfenced><mo is="true">,</mo></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo linebreak="goodbreak" is="true">∣</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">,</mo><mi mathvariant="bold-italic" is="true">w</mi><mo id="mmlalignd1e3020" linebreak="goodbreak" indentalign="id" indenttarget="mmlalignd1e2942" is="true">∼</mo><mtext is="true">Pois</mtext><mfenced open="(" close=")" is="true"><mrow is="true"><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow></mfenced><mo is="true">,</mo></mrow></math>$(7)$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mo linebreak="goodbreak" is="true">∣</mo><mi mathvariant="bold-italic" is="true">w</mi><mo id="mmlalignd1e3084" linebreak="goodbreak" indentalign="id" indenttarget="mmlalignd1e2942" is="true">∼</mo><mtext is="true">Pois</mtext><mfenced open="(" close=")" is="true"><mrow is="true"><mi is="true">A</mi><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow></mfenced><mo is="true">.</mo></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mo linebreak="goodbreak" is="true">∣</mo><mi mathvariant="bold-italic" is="true">w</mi><mo id="mmlalignd1e3084" linebreak="goodbreak" indentalign="id" indenttarget="mmlalignd1e2942" is="true">∼</mo><mtext is="true">Pois</mtext><mfenced open="(" close=")" is="true"><mrow is="true"><mi is="true">A</mi><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow></mfenced><mo is="true">.</mo></mrow></math>$ In the simulation, we use an exponential covariance function ($<math><mrow is="true"><mi is="true">ν</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">5</mn></mrow></math>$$<math><mrow is="true"><mi is="true">ν</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">5</mn></mrow></math>$), i.e.&nbsp;$<math><mrow is="true"><mi is="true">C</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">;</mo><msup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo is="true">,</mo><mi is="true">ϕ</mi><mo is="true">)</mo></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mi mathvariant="normal" is="true">exp</mi><mrow is="true"><mo is="true">(</mo><mo is="true">−</mo><mo is="true">‖</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">−</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">‖</mo><mo is="true">∕</mo><mi is="true">ϕ</mi><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><mi is="true">C</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">;</mo><msup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo is="true">,</mo><mi is="true">ϕ</mi><mo is="true">)</mo></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mi mathvariant="normal" is="true">exp</mi><mrow is="true"><mo is="true">(</mo><mo is="true">−</mo><mo is="true">‖</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">−</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">‖</mo><mo is="true">∕</mo><mi is="true">ϕ</mi><mo is="true">)</mo></mrow></mrow></math>$, $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></math>$, $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></math>$ two <a href="https://www.sciencedirect.com/topics/computer-science/spatial-location" title="Learn more about spatial locations from ScienceDirect's AI-generated Topic Pages">spatial locations</a> in $<math><msup is="true"><mrow is="true"><mi mathvariant="script" is="true">R</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></math>$$<math><msup is="true"><mrow is="true"><mi mathvariant="script" is="true">R</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></math>$. The influence of different sample sizes and combination of spatial hyperparameters on <a href="https://www.sciencedirect.com/topics/mathematics/predictive-performance" title="Learn more about predictive performance from ScienceDirect's AI-generated Topic Pages">predictive performance</a> was investigated in a fusion model by&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b51">Wang et al. (2018)</a>, therefore, we only consider a single setup by setting $<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">5</mn></mrow></math>$$<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">5</mn></mrow></math>$ and $<math><mrow is="true"><mi is="true">ϕ</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">1</mn></mrow></math>$$<math><mrow is="true"><mi is="true">ϕ</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">1</mn></mrow></math>$. In addition, we set $<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">5</mn><mo is="true">)</mo></mrow></mrow><mrow is="true"><mo is="true">⊤</mo></mrow></msup><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">.</mo><mn is="true">5</mn><mo is="true">)</mo></mrow></mrow><mrow is="true"><mo is="true">⊤</mo></mrow></msup></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">5</mn><mo is="true">)</mo></mrow></mrow><mrow is="true"><mo is="true">⊤</mo></mrow></msup><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">.</mo><mn is="true">5</mn><mo is="true">)</mo></mrow></mrow><mrow is="true"><mo is="true">⊤</mo></mrow></msup></mrow></math>$ and $<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">τ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">1</mn></mrow></math>$$<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">τ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">1</mn></mrow></math>$. $<math><mi is="true">A</mi></math>$$<math><mi is="true">A</mi></math>$ is a constant term that controls the density of point process, and it is assigned as the product of grid cell area and an offset which takes value 0.25. $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$ and $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$ multiplies $<math><mi mathvariant="bold-italic" is="true">w</mi></math>$$<math><mi mathvariant="bold-italic" is="true">w</mi></math>$ with a matrix of zeros and ones to subset it to the locations of $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$ and gridded locations of $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub></math>$, while $<math><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$ applies Eq.&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#fd4">(4)</a> on the subset of sampling locations for $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$.</p><p>We consider seven different model specifications within our proposed framework: three univariate models using a single data type each, namely one of geostatistical, lattice and point-pattern data; three fusion models using different combinations of two data types; and a multivariate fusion model combining all three response variables. We use penalized complexity (PC) prior for Matérn GRF&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b14">Fuglstad et al., 2019</a>) with $<math><mi is="true">α</mi></math>$$<math><mi is="true">α</mi></math>$ fixed at 1.5, corresponding to the exponential covariance function. In addition, we choose the median practical spatial range to be 2 (corresponds to the median of $<math><mi is="true">ϕ</mi></math>$$<math><mi is="true">ϕ</mi></math>$ being 1) and the <a href="https://www.sciencedirect.com/topics/mathematics/probability-theory" title="Learn more about probability from ScienceDirect's AI-generated Topic Pages">probability</a> of $<math><mi is="true">σ</mi></math>$$<math><mi is="true">σ</mi></math>$ greater than 1.7 is 5%. The remaining priors are default options from R-INLA&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b28">Lindgren and Rue, 2015</a>), i.e.,&nbsp;a zero-mean normal distribution with precision 0.001 for the coefficients $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$ and $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$; and a <a href="https://www.sciencedirect.com/topics/engineering/gamma-distribution" title="Learn more about Gamma distribution from ScienceDirect's AI-generated Topic Pages">Gamma distribution</a> with shape being 1 and rate being 0.00001 for the precision $<math><msup is="true"><mrow is="true"><mi is="true">τ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></math>$$<math><msup is="true"><mrow is="true"><mi is="true">τ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></math>$. The simulation is repeated 100 times, each model runs just under one minute.</p><p>We choose an additional 1600 sites from a regular grid to evaluate the predictive performance of models on the latent process in terms of posterior standard deviation and root mean <a href="https://www.sciencedirect.com/topics/mathematics/squared-prediction-error" title="Learn more about squared prediction errors from ScienceDirect's AI-generated Topic Pages">squared prediction errors</a> (RMSPE) given by (8)$<math><mrow is="true"><mtext is="true">RMSPE</mtext><mo linebreak="goodbreak" is="true">=</mo><msup is="true"><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">1600</mn></mrow></mfrac><munderover is="true"><mrow is="true"><mo linebreak="badbreak" is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi><mo linebreak="badbreak" is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">1600</mn></mrow></munderover><msup is="true"><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">)</mo></mrow><mo linebreak="badbreak" is="true">−</mo><mover accent="true" is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mo is="true">ˆ</mo></mrow></mover><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">)</mo></mrow></mrow></mfenced></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></mrow></mfenced></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">∕</mo><mn is="true">2</mn></mrow></msup></mrow></math>$$<math><mrow is="true"><mtext is="true">RMSPE</mtext><mo linebreak="goodbreak" is="true">=</mo><msup is="true"><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">1600</mn></mrow></mfrac><munderover is="true"><mrow is="true"><mo linebreak="badbreak" is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi><mo linebreak="badbreak" is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">1600</mn></mrow></munderover><msup is="true"><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">)</mo></mrow><mo linebreak="badbreak" is="true">−</mo><mover accent="true" is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mo is="true">ˆ</mo></mrow></mover><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">)</mo></mrow></mrow></mfenced></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></mrow></mfenced></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">∕</mo><mn is="true">2</mn></mrow></msup></mrow></math>$under each scenario. In Eq.&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#fd8">(8)</a>, $<math><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mo is="true">ˆ</mo></mrow></mover><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mo is="true">ˆ</mo></mrow></mover><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">u</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">)</mo></mrow></mrow></math>$ denotes the posterior median. Note that the latent process is kept the same under each simulation for a fair comparison. The prediction sites are located at the centers of a 40&nbsp;×&nbsp;40 grid that uniformly covers the sampling domain. Their predictive performance is shown in <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#fig1">Fig.&nbsp;1</a>. The first <a href="https://www.sciencedirect.com/topics/mathematics/venn-diagram" title="Learn more about Venn diagram from ScienceDirect's AI-generated Topic Pages">Venn diagram</a> shows the average posterior standard deviation over 100 simulations under different models. The second Venn diagram shows average RMSPEs. The point-process data only model has the lowest posterior standard deviations, however, its RMSPE is the largest. Overall, the RMSPEs are smaller in multivariate fusion models compared to univariate process-based models. The <a href="https://www.sciencedirect.com/topics/engineering/joints-structural-components" title="Learn more about joint from ScienceDirect's AI-generated Topic Pages">joint</a> modeling of all three types of spatial data has the lowest RMSPE on the prediction of the latent process at unobserved locations, demonstrating the benefits of spatial fusion models.</p><p><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0167947321000748-gr1.jpg"></p><ol><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167947321000748-gr1_lrg.jpg" title="Download high-res image (170KB)">Download : Download high-res image (170KB)</a></li><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167947321000748-gr1.jpg" title="Download full-size image">Download : Download full-size image</a></li></ol><p>Fig. 1. <a href="https://www.sciencedirect.com/topics/mathematics/venn-diagram" title="Learn more about Venn diagram from ScienceDirect's AI-generated Topic Pages">Venn diagram</a> of mean posterior standard deviation and root mean <a href="https://www.sciencedirect.com/topics/mathematics/squared-prediction-error" title="Learn more about squared prediction error from ScienceDirect's AI-generated Topic Pages">squared prediction error</a> for the unifying fusion framework fitted to each data type (and combinations thereof). Values in overlapping areas indicate results from models with multiple data types.</p><h3 id="4-2-Simulation-study-two"><a href="#4-2-Simulation-study-two" class="headerlink" title="4.2. Simulation study two"></a>4.2. Simulation study two</h3><p>In the second simulation study, we simulate a new scenario with three response variables of different types and two latent processes to evaluate the parameter estimates of a multivariate fusion model. We firstly simulate two independent zero-mean unit-variance GRFs $<math><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">)</mo></mrow></mrow><mrow is="true"><mo is="true">⊤</mo></mrow></msup></mrow></math>$$<math><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">)</mo></mrow></mrow><mrow is="true"><mo is="true">⊤</mo></mrow></msup></mrow></math>$ with exponential covariance function distributed on the spatial domain of $<math><mrow is="true"><mrow is="true"><mo is="true">[</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">100</mn><mo is="true">]</mo></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">×</mo><mrow is="true"><mo is="true">[</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">100</mn><mo is="true">]</mo></mrow></mrow></math>$$<math><mrow is="true"><mrow is="true"><mo is="true">[</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">100</mn><mo is="true">]</mo></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">×</mo><mrow is="true"><mo is="true">[</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">100</mn><mo is="true">]</mo></mrow></mrow></math>$ square, then compute the sub-sampled and aggregated GRF for each response variable in the same way as in simulation one.</p><p>Each response depends on the latent processes via the <a href="https://www.sciencedirect.com/topics/mathematics/design-matrix" title="Learn more about design matrix from ScienceDirect's AI-generated Topic Pages">design matrix</a> (9)$<math><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi><mo linebreak="goodbreak" is="true">=</mo><mfenced open="[" close="]" is="true"><mrow is="true"><mtable align="axis" equalrows="false" columnlines="none none none none none none none none none" equalcolumns="false" class="bmatrix" is="true"><mtr is="true"><mtd class="array" columnalign="center" is="true"><mn is="true">1</mn><mo is="true">.</mo><mn is="true">2</mn></mtd><mtd class="array" columnalign="center" is="true"><mn is="true">0</mn></mtd></mtr><mtr is="true"><mtd class="array" columnalign="center" is="true"><mn is="true">0</mn><mo is="true">.</mo><mn is="true">5</mn></mtd><mtd class="array" columnalign="center" is="true"><mn is="true">1</mn><mo is="true">.</mo><mn is="true">2</mn></mtd></mtr><mtr is="true"><mtd class="array" columnalign="center" is="true"><mn is="true">0</mn></mtd><mtd class="array" columnalign="center" is="true"><mn is="true">1</mn></mtd></mtr></mtable></mrow></mfenced><mo is="true">.</mo></mrow></math>$$<math><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi><mo linebreak="goodbreak" is="true">=</mo><mfenced open="[" close="]" is="true"><mrow is="true"><mtable align="axis" equalrows="false" columnlines="none none none none none none none none none" equalcolumns="false" class="bmatrix" is="true"><mtr is="true"><mtd class="array" columnalign="center" is="true"><mn is="true">1</mn><mo is="true">.</mo><mn is="true">2</mn></mtd><mtd class="array" columnalign="center" is="true"><mn is="true">0</mn></mtd></mtr><mtr is="true"><mtd class="array" columnalign="center" is="true"><mn is="true">0</mn><mo is="true">.</mo><mn is="true">5</mn></mtd><mtd class="array" columnalign="center" is="true"><mn is="true">1</mn><mo is="true">.</mo><mn is="true">2</mn></mtd></mtr><mtr is="true"><mtd class="array" columnalign="center" is="true"><mn is="true">0</mn></mtd><mtd class="array" columnalign="center" is="true"><mn is="true">1</mn></mtd></mtr></mtable></mrow></mfenced><mo is="true">.</mo></mrow></math>$The first geostatistical response variable only depends on the first latent process, the second lattice response variable depends on both latent processes, while the third point pattern response variable depends only on the second latent process. The response variables are generated as follows, $<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo linebreak="goodbreak" is="true">∣</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">,</mo><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">,</mo><msubsup is="true"><mrow is="true"><mi is="true">τ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><mo id="mmlalignd1e3808" linebreak="goodbreak" is="true">∼</mo><mtext is="true">N</mtext><mfenced open="(" close=")" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><msubsup is="true"><mrow is="true"><mi is="true">τ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><mi mathvariant="bold-italic" is="true">I</mi></mrow></mfenced><mo is="true">,</mo></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo linebreak="goodbreak" is="true">∣</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">,</mo><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">,</mo><msubsup is="true"><mrow is="true"><mi is="true">τ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><mo id="mmlalignd1e3808" linebreak="goodbreak" is="true">∼</mo><mtext is="true">N</mtext><mfenced open="(" close=")" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><msubsup is="true"><mrow is="true"><mi is="true">τ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><mi mathvariant="bold-italic" is="true">I</mi></mrow></mfenced><mo is="true">,</mo></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo linebreak="goodbreak" is="true">∣</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">,</mo><mi mathvariant="bold-italic" is="true">w</mi><mo id="mmlalignd1e3893" linebreak="goodbreak" indentalign="id" indenttarget="mmlalignd1e3808" is="true">∼</mo><mtext is="true">Pois</mtext><mfenced open="(" close=")" is="true"><mrow is="true"><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow></mfenced><mo is="true">,</mo></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo linebreak="goodbreak" is="true">∣</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">,</mo><mi mathvariant="bold-italic" is="true">w</mi><mo id="mmlalignd1e3893" linebreak="goodbreak" indentalign="id" indenttarget="mmlalignd1e3808" is="true">∼</mo><mtext is="true">Pois</mtext><mfenced open="(" close=")" is="true"><mrow is="true"><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow></mfenced><mo is="true">,</mo></mrow></math>$(10)$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mo linebreak="goodbreak" is="true">∣</mo><mi mathvariant="bold-italic" is="true">w</mi><mo id="mmlalignd1e3964" linebreak="goodbreak" indentalign="id" indenttarget="mmlalignd1e3808" is="true">∼</mo><mtext is="true">Pois</mtext><mfenced open="(" close=")" is="true"><mrow is="true"><mi is="true">A</mi><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow></mfenced><mo is="true">,</mo></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mo linebreak="goodbreak" is="true">∣</mo><mi mathvariant="bold-italic" is="true">w</mi><mo id="mmlalignd1e3964" linebreak="goodbreak" indentalign="id" indenttarget="mmlalignd1e3808" is="true">∼</mo><mtext is="true">Pois</mtext><mfenced open="(" close=")" is="true"><mrow is="true"><mi is="true">A</mi><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Z</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mi mathvariant="bold-italic" is="true">w</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow></mfenced><mo is="true">,</mo></mrow></math>$ where $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$ consists of 500 geostatistical observations, $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$ has 100 lattice observations and $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub></math>$ represents the number of events observed at each of 400 cells on a 20&nbsp;×&nbsp;20 grid. In addition, we set $<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><mn is="true">3</mn><mo is="true">,</mo><mn is="true">5</mn><mo is="true">)</mo></mrow></mrow><mrow is="true"><mo is="true">⊤</mo></mrow></msup><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">5</mn><mo is="true">,</mo><mn is="true">2</mn><mo is="true">)</mo></mrow></mrow><mrow is="true"><mo is="true">⊤</mo></mrow></msup><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">5</mn></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><mn is="true">3</mn><mo is="true">,</mo><mn is="true">5</mn><mo is="true">)</mo></mrow></mrow><mrow is="true"><mo is="true">⊤</mo></mrow></msup><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">β</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">5</mn><mo is="true">,</mo><mn is="true">2</mn><mo is="true">)</mo></mrow></mrow><mrow is="true"><mo is="true">⊤</mo></mrow></msup><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">5</mn></mrow></math>$, $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">25</mn></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">25</mn></mrow></math>$ and $<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">τ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">5</mn></mrow></math>$$<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">τ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">5</mn></mrow></math>$. $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$ and $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">⋅</mi><mo is="true">)</mo></mrow></mrow></math>$ are defined similarly as in simulation one. Since we have two latent processes in the simulation, using any of the univariate model or fusion model with two response variables can lead to identifiability problem. Therefore, we estimate the parameters using the unifying spatial fusion model with three responses only. The model and their prior specifications are the same as in simulation one, except for the spatial range parameter and the design matrix. The prior for both $<math><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$ and $<math><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$ is a PC prior with median practical spatial range $<math><mrow is="true"><mn is="true">20</mn></mrow></math>$$<math><mrow is="true"><mn is="true">20</mn></mrow></math>$ (corresponds to the median of $<math><mi is="true">ϕ</mi></math>$$<math><mi is="true">ϕ</mi></math>$ being 10). For implementation purposes, the design matrix components are parameterized differently in INLA. $<math><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">11</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">11</mn></mrow></msub></math>$ and $<math><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">32</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">32</mn></mrow></msub></math>$ are treated as $<math><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$ and $<math><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$ of each latent process, while $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">21</mn></mrow></msub><mo is="true">∕</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">11</mn></mrow></msub></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">21</mn></mrow></msub><mo is="true">∕</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">11</mn></mrow></msub></mrow></math>$ and $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">22</mn></mrow></msub><mo is="true">∕</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">32</mn></mrow></msub></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">22</mn></mrow></msub><mo is="true">∕</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">32</mn></mrow></msub></mrow></math>$ are coefficients for the latent processes with variance $<math><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup></math>$$<math><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup></math>$ and $<math><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup></math>$$<math><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup></math>$. The former has the same prior as in simulation one, the latter has a normal prior with a mean of 1 and a standard deviation of 10. The simulation is run 100 times.</p><p>The parameter estimates based on posterior medians are displayed in <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#fig2">Fig.&nbsp;2</a>. The PC prior in R-INLA penalizes complex structure in GRF hence tends to have a slightly over-estimated range and underestimated coefficients in $<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$$<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b14">Fuglstad et al., 2019</a>). One example of the posterior median of fitted latent processes at locations with geostatistical observations is shown in <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#fig3">Fig.&nbsp;3</a>. The figure shows both $<math><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$ and $<math><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$ are fitted well and the plot of fitted $<math><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$ and $<math><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$ shows their independence structure assumed in the model. The <a href="https://www.sciencedirect.com/topics/engineering/root-mean-squared-error" title="Learn more about root mean squared errors from ScienceDirect's AI-generated Topic Pages">root mean squared errors</a> are 0.54 and 0.48 for the first and second latent processes. The computation time for the INLA implementation of the fusion model is 11&nbsp;min.</p><p><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0167947321000748-gr2.jpg"></p><ol><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167947321000748-gr2_lrg.jpg" title="Download high-res image (450KB)">Download : Download high-res image (450KB)</a></li><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167947321000748-gr2.jpg" title="Download full-size image">Download : Download full-size image</a></li></ol><p>Fig. 2. Distribution of posterior <a href="https://www.sciencedirect.com/topics/engineering/median-estimate" title="Learn more about median estimates from ScienceDirect's AI-generated Topic Pages">median estimates</a> from the unifying spatial fusion model in simulation two. The black vertical line indicates the true parameter value, dashed curves indicate prior density.</p><p><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0167947321000748-gr3.jpg"></p><ol><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167947321000748-gr3_lrg.jpg" title="Download high-res image (263KB)">Download : Download high-res image (263KB)</a></li><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167947321000748-gr3.jpg" title="Download full-size image">Download : Download full-size image</a></li></ol><p>Fig. 3. One example of true versus fitted latent process at locations with geostatistical observation in simulation two, for $<math><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$, $<math><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$ and fitted $<math><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$ against $<math><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$ respectively. Pearson’s correlation coefficients $<math><mi is="true">ρ</mi></math>$$<math><mi is="true">ρ</mi></math>$ are displayed.</p><h3 id="4-3-Application-to-LuftiBus-SNC-dataset"><a href="#4-3-Application-to-LuftiBus-SNC-dataset" class="headerlink" title="4.3. Application to LuftiBus-SNC dataset"></a>4.3. Application to LuftiBus-SNC dataset</h3><p>In spatial epidemiology, joint analysis of multiple diseases with similar etiology allows us to separate underlying risk factors into shared and disease-specific components. In this analysis, we examine the disease-specific spatial risk surface of lung cancer and shared spatial risk components between lung cancer and respiratory disease while taking PM<sub>10</sub> (particulate matter with diameter $<math><mrow is="true"><mo is="true">&lt;</mo><mn is="true">10</mn><mspace width="1em" class="nbsp" is="true"></mspace><mi mathvariant="normal" is="true">μ</mi><mi mathvariant="normal" is="true">m</mi></mrow></math>$$<math><mrow is="true"><mo is="true">&lt;</mo><mn is="true">10</mn><mspace width="1em" class="nbsp" is="true"></mspace><mi mathvariant="normal" is="true">μ</mi><mi mathvariant="normal" is="true">m</mi></mrow></math>$) pollution surface into consideration. Previously, a similar analysis was done in <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b51">Wang et al. (2018)</a> considering combined lung cancer and respiratory disease risk only.</p><p>Chronic lung disease contributes substantially to morbidity and mortality worldwide, with chronic obstructive pulmonary disease (COPD) being the third leading cause of death&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b31">Lozano et al., 2012</a>). Forced expiratory volume in one second (FEV1) is a measure of the amount of air a person can exhale during a pulmonary test and it can be used to diagnose disease and predict respiratory-related mortality&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b36">Menezes et al., 2014</a>). While respiratory disease and lung cancer share many common risk factors such as smoking and exposure to air pollution, it is of interest to examine the lung cancer-specific spatial risk component. It may provide insights into identifying risk factors that are solely associated with lung cancer.</p><p>Initiated as a health promotion campaign by <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b32">Lunge&nbsp;Zürich (2017)</a> in Switzerland, the ‘LuftiBus’ project collected lung function measurements including FEV1 and demographic information from local residents. Data from LuftiBus observed between 2003 and 2012 were deterministically linked with the census-based Swiss National Cohort (SNC) (<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b5">Bopp et al., 2008</a>). The purpose was to obtain 44,071 individuals with demographic, health and environmental variables in Switzerland. More importantly, the linkage provides us with the residential location of individual participants.</p><p>For lattice data, we use the computed expected cause-specific (respiratory and lung cancer, respectively) mortalities in each municipality, adjusted by 5-year age-group and gender based on the SNC data ($<math><mi is="true">n</mi></math>$$<math><mi is="true">n</mi></math>$ $<math><mo is="true">=</mo></math>$$<math><mo is="true">=</mo></math>$ 572,993). For point process data, we sample from a PM<sub>10</sub> pollution map in 2010&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b20">Geoinformation Kanton&nbsp;Zurich, 2015</a>) to obtain proxy pollution sources. The probability of sampling each location is proportional to standardized PM<sub>10</sub> annual mean value. We obtain 253 locations which are mainly distributed along highway roads.</p><p>We assume three latent spatial risk surfaces that are associated with respiratory disease and lung cancer. The first risk surface, $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$, is shared between FEV1, respiratory mortality and lung cancer mortality, while the second risk surface $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$ is lung cancer-specific risk surface and the third risk surface $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub></math>$ is shared between PM<sub>10</sub> and the other three disease-related variables. Typically with lattice data, multivariate conditional <a href="https://www.sciencedirect.com/topics/mathematics/autoregressive-model" title="Learn more about autoregressive models from ScienceDirect's AI-generated Topic Pages">autoregressive models</a> allow us to jointly analyze multiple responses and identify different latent components&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b23">Jin et al., 2005</a>). Because municipal boundaries are artificial, we argue that a continuous spatial surface is a more natural modeling assumption. Therefore, we use our process-based unifying framework to conduct the analysis. Another advantage is that it allows us to incorporate the rich FEV1 data from Luftibus as well as additional pollution data.</p><p>The fusion model is structured as $<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mtext is="true">FEV1</mtext></mrow></msub><mo id="mmlalignd1e4555" linebreak="goodbreak" is="true">∼</mo><mtext is="true">N</mtext><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mn is="true">0</mn></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mn is="true">1</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mrow is="true"><mtext is="true">age</mtext></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mn is="true">2</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mrow is="true"><mtext is="true">gender</mtext></mrow></msub><mo is="true">−</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">11</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">)</mo></mrow><mo is="true">−</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">13</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mo is="true">)</mo></mrow><mo is="true">,</mo><msup is="true"><mrow is="true"><mi is="true">τ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mi mathvariant="bold-italic" is="true">I</mi><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mtext is="true">FEV1</mtext></mrow></msub><mo id="mmlalignd1e4555" linebreak="goodbreak" is="true">∼</mo><mtext is="true">N</mtext><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mn is="true">0</mn></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mn is="true">1</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mrow is="true"><mtext is="true">age</mtext></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mn is="true">2</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mrow is="true"><mtext is="true">gender</mtext></mrow></msub><mo is="true">−</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">11</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">)</mo></mrow><mo is="true">−</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">13</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mo is="true">)</mo></mrow><mo is="true">,</mo><msup is="true"><mrow is="true"><mi is="true">τ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mi mathvariant="bold-italic" is="true">I</mi><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mtext is="true">resp</mtext></mrow></msub><mo id="mmlalignd1e4705" linebreak="goodbreak" indentalign="id" indenttarget="mmlalignd1e4555" is="true">∼</mo><mtext is="true">Pois</mtext><mrow is="true"><mo class="bigl" fence="true" is="true">(</mo><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">E</mi></mrow><mrow is="true"><mtext is="true">resp</mtext></mrow></msub><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">2</mn><mo is="true">,</mo><mn is="true">0</mn></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">21</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">)</mo></mrow><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">23</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow><mo class="bigr" fence="true" is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mtext is="true">resp</mtext></mrow></msub><mo id="mmlalignd1e4705" linebreak="goodbreak" indentalign="id" indenttarget="mmlalignd1e4555" is="true">∼</mo><mtext is="true">Pois</mtext><mrow is="true"><mo class="bigl" fence="true" is="true">(</mo><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">E</mi></mrow><mrow is="true"><mtext is="true">resp</mtext></mrow></msub><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">2</mn><mo is="true">,</mo><mn is="true">0</mn></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">21</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">)</mo></mrow><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">23</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow><mo class="bigr" fence="true" is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mtext is="true">cancer</mtext></mrow></msub><mo id="mmlalignd1e4818" linebreak="goodbreak" indentalign="id" indenttarget="mmlalignd1e4555" is="true">∼</mo><mtext is="true">Pois</mtext><mrow is="true"><mo class="bigl" fence="true" is="true">(</mo><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">E</mi></mrow><mrow is="true"><mtext is="true">cancer</mtext></mrow></msub><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">3</mn><mo is="true">,</mo><mn is="true">0</mn></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">31</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">)</mo></mrow><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">32</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">)</mo></mrow><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">33</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow><mo class="bigr" fence="true" is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mtext is="true">cancer</mtext></mrow></msub><mo id="mmlalignd1e4818" linebreak="goodbreak" indentalign="id" indenttarget="mmlalignd1e4555" is="true">∼</mo><mtext is="true">Pois</mtext><mrow is="true"><mo class="bigl" fence="true" is="true">(</mo><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">E</mi></mrow><mrow is="true"><mtext is="true">cancer</mtext></mrow></msub><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">3</mn><mo is="true">,</mo><mn is="true">0</mn></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">31</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">)</mo></mrow><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">32</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">)</mo></mrow><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">33</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow><mo class="bigr" fence="true" is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>$(11)$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mtext is="true">pm</mtext></mrow></msub><mo id="mmlalignd1e4966" linebreak="goodbreak" indentalign="id" indenttarget="mmlalignd1e4555" is="true">∼</mo><mtext is="true">Pois</mtext><mrow is="true"><mo class="bigl" fence="true" is="true">(</mo><mrow is="true"><mi is="true">A</mi><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">4</mn><mo is="true">,</mo><mn is="true">0</mn></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">43</mn></mrow></msub><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow><mo class="bigr" fence="true" is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mtext is="true">pm</mtext></mrow></msub><mo id="mmlalignd1e4966" linebreak="goodbreak" indentalign="id" indenttarget="mmlalignd1e4555" is="true">∼</mo><mtext is="true">Pois</mtext><mrow is="true"><mo class="bigl" fence="true" is="true">(</mo><mrow is="true"><mi is="true">A</mi><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">4</mn><mo is="true">,</mo><mn is="true">0</mn></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">43</mn></mrow></msub><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow><mo class="bigr" fence="true" is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>$ where $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">E</mi></mrow><mrow is="true"><mtext is="true">resp</mtext></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">E</mi></mrow><mrow is="true"><mtext is="true">resp</mtext></mrow></msub></math>$ and $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">E</mi></mrow><mrow is="true"><mtext is="true">cancer</mtext></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">E</mi></mrow><mrow is="true"><mtext is="true">cancer</mtext></mrow></msub></math>$ are the expected cause-specific mortalities. We set some coefficients of the design matrix $<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$$<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$ to zero and directly depict the association between each response variable and the latent processes using the resulting product of $<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$$<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$ and $<math><mi mathvariant="bold-italic" is="true">w</mi></math>$$<math><mi mathvariant="bold-italic" is="true">w</mi></math>$. The terms $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">11</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">11</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">)</mo></mrow></mrow></math>$ and $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">13</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">Z</mi></mrow><mrow is="true"><mn is="true">13</mn></mrow></msub><mspace width="0.2777em" is="true"></mspace><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mo is="true">)</mo></mrow></mrow></math>$ are set to be negative since FEV1 is assumed to decrease as the latent risks increase. In addition, we add an additional intercept for $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mtext is="true">pm</mtext></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mtext is="true">pm</mtext></mrow></msub></math>$ to indicate constant background pollution.</p><p>More than 60% of the FEV1 measurements in the linked dataset are located in the Canton of Zurich, therefore, we restrict our analysis to the Canton of Zurich. In addition, we focus the analysis on people who are 40&nbsp;years or older, which results in 16,160 geostatistical observations. We use PC prior for the latent components with $<math><mrow is="true"><mi is="true">α</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">1</mn><mo is="true">.</mo><mn is="true">5</mn></mrow></math>$$<math><mrow is="true"><mi is="true">α</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">1</mn><mo is="true">.</mo><mn is="true">5</mn></mrow></math>$ corresponding to exponential covariance function, median practical range of 1&nbsp;km and median $<math><mi is="true">σ</mi></math>$$<math><mi is="true">σ</mi></math>$ of 1. <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#fig4">Fig.&nbsp;4</a> shows the locations of geostatistical observation, standardized mortality ratio for respiratory disease and lung cancer and point process locations for pollution source proxy. <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#tbl1">Table&nbsp;1</a> shows parameter estimates and <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#fig5">Fig.&nbsp;5</a> shows the transformed posterior estimates of the latent processes representing relative risk surfaces in the Canton of Zurich. The shared risk components between FEV1, respiratory mortality, and lung cancer mortality is the highest in urban areas, with an effective range of 1.1 km (95% CI: 0.5, 2.1) based on the exponential covariance function. The estimated relative risk is computed by exponentiating the latent process, which varies between 0.91 and 1.13. Meanwhile, high-risk areas of lung cancer-specific components are scattered around the Canton of Zurich, mainly in the north and west regions with an effective range of 0.9 km (95% CI: 0.3, 3.1). The variability is larger than the shared component with values between 0.90 and 1.20. The lung cancer-specific risk component is modeled via lattice data $<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mtext is="true">cancer</mtext></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Y</mi></mrow><mrow is="true"><mtext is="true">cancer</mtext></mrow></msub></math>$ only, hence it appears to have some block-wise structures compared to the shared component and have larger variability in the range estimation. Such results complement classical lattice data-based disease mapping approaches to identify potential disease hotpots at a finer resolution. The pollution surface is shared between all four response variables and has the longest effective range with 5.9 km (95% CI: 3.3, 9.4). It is the strongest around city centers.</p><p><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0167947321000748-gr4.jpg"></p><ol><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167947321000748-gr4_lrg.jpg" title="Download high-res image (888KB)">Download : Download high-res image (888KB)</a></li><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167947321000748-gr4.jpg" title="Download full-size image">Download : Download full-size image</a></li></ol><p>Fig. 4. Data used in the fusion model. Top left: locations of FEV1 observations. Top right: respiratory standardized mortality ratio. Bottom left: lung cancer standardized mortality ratio. Bottom right: sampled locations with density proportional to PM<sub>10</sub> annual mean.</p><p>Table 1. Parameter estimates and their 95% posterior credible intervals (95% CI) for the LuftiBus-SNC dataset. $<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></mrow></math>$$<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></mrow></math>$ and $<math><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub></math>$ are in meters.</p><table><thead><tr><th>Parameter</th><th>Median</th><th>95% CI</th><th>Parameter</th><th>Median</th><th>95% CI</th></tr></thead><tbody><tr><td>$<math><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mn is="true">0</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mn is="true">0</mn></mrow></msub></math>$</td><td>4.74</td><td>(4.70, 4.79)</td><td>$<math><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mn is="true">11</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mn is="true">11</mn></mrow></msub></math>$</td><td>0.0809</td><td>(0.0539, 0.122)</td></tr><tr><td>$<math><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mn is="true">1</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mn is="true">1</mn></mrow></msub></math>$</td><td>0.907</td><td>(0.891, 0.923)</td><td>$<math><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mn is="true">13</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mn is="true">13</mn></mrow></msub></math>$</td><td>0.0566</td><td>(0.0324, 0.0796)</td></tr><tr><td>$<math><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mn is="true">2</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mn is="true">2</mn></mrow></msub></math>$</td><td>$<math><mo is="true">−</mo></math>$$<math><mo is="true">−</mo></math>$0.0375</td><td>($<math><mo is="true">−</mo></math>$$<math><mo is="true">−</mo></math>$0.0382, $<math><mo is="true">−</mo></math>$$<math><mo is="true">−</mo></math>$0.0368)</td><td>$<math><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mn is="true">21</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mn is="true">21</mn></mrow></msub></math>$</td><td>0.0855</td><td>(0.0537, 0.137)</td></tr><tr><td>$<math><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">2</mn><mo is="true">,</mo><mn is="true">0</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">2</mn><mo is="true">,</mo><mn is="true">0</mn></mrow></msub></math>$</td><td>$<math><mo is="true">−</mo></math>$$<math><mo is="true">−</mo></math>$0.0643</td><td>($<math><mo is="true">−</mo></math>$$<math><mo is="true">−</mo></math>$0.130, $<math><mo is="true">−</mo></math>$$<math><mo is="true">−</mo></math>$0.00387)</td><td>$<math><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mn is="true">23</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mn is="true">23</mn></mrow></msub></math>$</td><td>0.141</td><td>(0.0868, 0.228)</td></tr><tr><td>$<math><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">3</mn><mo is="true">,</mo><mn is="true">0</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">3</mn><mo is="true">,</mo><mn is="true">0</mn></mrow></msub></math>$</td><td>$<math><mo is="true">−</mo></math>$$<math><mo is="true">−</mo></math>$0.119</td><td>($<math><mo is="true">−</mo></math>$$<math><mo is="true">−</mo></math>$0.191, $<math><mo is="true">−</mo></math>$$<math><mo is="true">−</mo></math>$0.0502)</td><td>$<math><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mn is="true">31</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mn is="true">31</mn></mrow></msub></math>$</td><td>0.0902</td><td>(0.0386, 0.172)</td></tr><tr><td>$<math><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">4</mn><mo is="true">,</mo><mn is="true">0</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">β</mi></mrow><mrow is="true"><mn is="true">4</mn><mo is="true">,</mo><mn is="true">0</mn></mrow></msub></math>$</td><td>$<math><mo is="true">−</mo></math>$$<math><mo is="true">−</mo></math>$0.0501</td><td>($<math><mo is="true">−</mo></math>$$<math><mo is="true">−</mo></math>$0.178, $<math><mo is="true">−</mo></math>$$<math><mo is="true">−</mo></math>$0.0833)</td><td>$<math><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mn is="true">32</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mn is="true">32</mn></mrow></msub></math>$</td><td>0.500</td><td>(0.208, 1.170)</td></tr><tr><td>$<math><msup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></math>$$<math><msup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></math>$</td><td>0.268</td><td>(0.263, 0.274)</td><td>$<math><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mn is="true">33</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mn is="true">33</mn></mrow></msub></math>$</td><td>0.159</td><td>(0.0931, 0.251)</td></tr><tr><td>$<math><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$</td><td>367</td><td>(173, 690)</td><td>$<math><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mn is="true">43</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mn is="true">43</mn></mrow></msub></math>$</td><td>0.123</td><td>(0.0824, 0.168)</td></tr><tr><td>$<math><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></math>$</td><td>307</td><td>(98, 1030)</td><td></td><td></td><td></td></tr><tr><td>$<math><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub></math>$$<math><msub is="true"><mrow is="true"><mi is="true">ϕ</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub></math>$</td><td>1970</td><td>(1110, 3130)</td><td></td><td></td><td></td></tr></tbody></table><p><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0167947321000748-gr5.jpg"></p><ol><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167947321000748-gr5_lrg.jpg" title="Download high-res image (993KB)">Download : Download high-res image (993KB)</a></li><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167947321000748-gr5.jpg" title="Download full-size image">Download : Download full-size image</a></li></ol><p>Fig. 5. Estimated spatial relative risk surfaces. Left: shared component between FEV1, respiratory mortality and lung cancer mortality, $<math><mrow is="true"><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">)</mo></mrow></mrow></math>$. Middle: lung cancer mortality-specific component, $<math><mrow is="true"><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">)</mo></mrow></mrow></math>$. Right: shared component between PM<sub>10</sub> and all three disease-related response variables, $<math><mrow is="true"><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><mo class="qopname" is="true">exp</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">w</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msub><mo is="true">)</mo></mrow></mrow></math>$.</p><h2 id="5-Summary-and-discussions"><a href="#5-Summary-and-discussions" class="headerlink" title="5. Summary and discussions"></a>5. Summary and discussions</h2><p>We have proposed a unifying process-based statistical framework to handle spatial data fusion. The framework allows simultaneously modeling all three types of spatial data, namely geostatistical, lattice and point-pattern data, to be easily incorporated into a single multivariate spatial model. This framework contains theoretical and computational elements from several existing literatures: the implementation uses the SPDE approach&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b29">Lindgren et al., 2011</a>) and INLA&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b44">Rue et al., 2009</a>), the sampling point approximation approach for modeling lattice data is adopted from&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b13">Fuentes and Raftery (2005)</a> and data augmentation&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b48">Simpson et al., 2016</a>) is used in INLA. We have combined all of the individual elements and constructed this unifying framework. The framework extends upon existing flexible spatial fusion models&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b51">Wang et al., 2018</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b53">Wilson and Wakefield, 2018</a>) by making point-pattern data also compatible, hence it completes all three spatial data types. We have shown in the first simulation study that it is advantageous to conduct <a href="https://www.sciencedirect.com/topics/mathematics/multivariate-analysis" title="Learn more about multivariate analysis from ScienceDirect's AI-generated Topic Pages">multivariate analysis</a> using multiple spatial datasets if they are available.</p><p>Identifiability issues arise when there is more than one latent spatial process in the fusion model. Similar concern has been brought up in other multivariate spatial models&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b42">Ren and Banerjee, 2013</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b25">Knorr-Held and Best, 2001</a>). Since the model becomes invariant under certain <a href="https://www.sciencedirect.com/topics/mathematics/orthogonal-transformation" title="Learn more about orthogonal transformations from ScienceDirect's AI-generated Topic Pages">orthogonal transformations</a>, the design matrix $<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$$<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$ is not identifiable.&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b25">Knorr-Held and Best (2001)</a> proposed a specific constraint on the relationship among the individual elements of the design matrix.&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b42">Ren and Banerjee (2013)</a> proposed to constrain one element of each row in the design matrix to be strictly positive and to have an ordered spatial range parameter. The same constraints allow identifiable parameters in our implementation. A distinction between our proposed framework and existing multivariate models is that we can potentially have only one observation at any of the <a href="https://www.sciencedirect.com/topics/computer-science/spatial-location" title="Learn more about spatial locations from ScienceDirect's AI-generated Topic Pages">spatial locations</a> even when we have three response variables in our model. This makes it problematic to identify more than one latent process at each location. Our implementation using the SPDE approach and INLA avoids this problem since it does not directly model the latent variable parameters at the set of locations $<math><mi mathvariant="script" is="true">U</mi></math>$$<math><mi mathvariant="script" is="true">U</mi></math>$, but on the <a href="https://www.sciencedirect.com/topics/engineering/meshes" title="Learn more about mesh from ScienceDirect's AI-generated Topic Pages">mesh</a> vertices. When a model involves Matérn covariance function with a smoothness parameter greater than 1, the models using the SPDE approach can be used as an approximation while the model likelihood can be directly used with a <a href="https://www.sciencedirect.com/topics/mathematics/bayesian" title="Learn more about Bayesian from ScienceDirect's AI-generated Topic Pages">Bayesian</a> hierarchical approach.</p><p>The usage of our proposed framework is multifaceted. The interest sometimes lies within latent spatial processes when spatial data are analyzed, which represent residual <a href="https://www.sciencedirect.com/topics/mathematics/spatial-correlation" title="Learn more about spatial correlation from ScienceDirect's AI-generated Topic Pages">spatial correlation</a> in the response variables after considering existing covariates. The result can be used for detecting spatial clusters of unexplained risk or shared scientific drivers for response variables, which warrant further investigation in identifying those unknown drivers. When the interest is in predicting a response variable for a newly observed spatial unit, the fusion model improves the prediction of latent processes which in turn improves response variable prediction. Furthermore, the framework can be modified to use a one-dimensional Gaussian process in the latent components such that it applies beyond spatial data. For example, it can be used in <a href="https://www.sciencedirect.com/topics/mathematics/time-series-modeling" title="Learn more about time series modeling from ScienceDirect's AI-generated Topic Pages">time series modeling</a> where all the observations are in $<math><mi mathvariant="script" is="true">R</mi></math>$$<math><mi mathvariant="script" is="true">R</mi></math>$ and as well as in <a href="https://www.sciencedirect.com/topics/computer-science/machine-learning" title="Learn more about machine learning from ScienceDirect's AI-generated Topic Pages">machine learning</a> applications&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b41">Rasmussen and Williams, 2005</a>).</p><p>Although our current framework assumes independent latent Gaussian processes, it can be viewed as modeling multivariate Gaussian process via the LMC approach&nbsp;(<a href="https://www.sciencedirect.com/science/article/pii/S0167947321000748#b19">Genton and Kleiber, 2015</a>). One drawback is that the number of parameters in the design matrix $<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$$<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$ increases with the number of latent Gaussian processes, which is difficult to estimate and may lead to convergence issues. In those cases, it may be advantageous to directly model the spatial structure with a cross-covariance function. In addition, the framework requires the number of latent processes $<math><mi is="true">q</mi></math>$$<math><mi is="true">q</mi></math>$ to be specified a priori.</p><p>As with any other <a href="https://www.sciencedirect.com/topics/mathematics/statistical-modeling" title="Learn more about statistical modeling from ScienceDirect's AI-generated Topic Pages">statistical modeling</a>, <a href="https://www.sciencedirect.com/topics/mathematics/model-misspecification" title="Learn more about model misspecification from ScienceDirect's AI-generated Topic Pages">model misspecification</a> has an impact on the inference and should be assessed via sensitivity analysis. Further research might include checking the compatibility of different data sources for spatial fusion modeling, i.e.&nbsp;if overlapping information exists between different spatial datasets. Such information helps to inform the model structure, especially the design matrix $<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$$<math><mi mathvariant="bold-italic" is="true">Z</mi></math>$.</p><h2 id="Supporting-information"><a href="#Supporting-information" class="headerlink" title="Supporting information"></a>Supporting information</h2><p>Supplementary material for this article is available online at the author’s website <a href="http://www.math.uzh.ch/furrer/download/unifying2020.zip">www.math.uzh.ch/furrer/download/unifying2020.zip</a>, including all the R code used for the simulation studies and additional details on the Stan implementation of the framework using Bayesian hierarchical models.</p>    <style>    #refplus, #refplus li{         padding:0;        margin:0;        list-style:none;    }；    </style>    <script src="https://unpkg.com/@popperjs/core@2"></script>    <script src="https://unpkg.com/tippy.js@6"></script>    <script>    document.querySelectorAll(".refplus-num").forEach((ref) => {        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');        let refel = document.querySelector(refid);        let refnum = refel.dataset.num;        let ref_content = refel.innerText.replace(`[${refnum}]`,'');        tippy(ref, {            content: ref_content,        });    });    </script>    ]]></content>
      
      
      <categories>
          
          <category> 高斯过程 </category>
          
          <category> 概览 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 非参数模型 </tag>
            
            <tag> 高斯过程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>🔥  贝叶斯方法索引帖</title>
      <link href="/vll-pages/posts/7a2e65c2.html"/>
      <url>/vll-pages/posts/7a2e65c2.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h2 id="一、知识要点"><a href="#一、知识要点" class="headerlink" title="一、知识要点"></a>一、知识要点</h2><h3 id="1-1-贝叶斯思维与工作流"><a href="#1-1-贝叶斯思维与工作流" class="headerlink" title="1.1 贝叶斯思维与工作流"></a>1.1 贝叶斯思维与工作流</h3><p>推荐的几本基础入门书籍：</p><ul><li>Martin 2015 年的 <a href="https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html">《Bayesian Analysis with Python》</a> </li><li>Martin 2022 年的 <a href="https://xishansnow.github.io/BayesianModelingandComputationInPython/index.html">《Bayesian Modeling and Computation in Python》</a></li><li>Kruschke 2015 年的 <a href="https://sites.google.com/site/doingbayesiandataanalysis/">《Doing Bayesian Data Analysis》</a></li></ul><h3 id="1-2-主要的贝叶斯推断方法"><a href="#1-2-主要的贝叶斯推断方法" class="headerlink" title="1.2 主要的贝叶斯推断方法"></a>1.2 主要的贝叶斯推断方法</h3><p><strong>（1） 关于基础的推断方法</strong> </p><p><a href="https://engineering.purdue.edu/kak/Tutorials/Trinity.pdf">普渡大学机器人视觉实验室的自编教程</a>： 普渡大学的一篇入门教程，清晰地从贝叶斯定理出发，阐明了最大似然估计、最大后验估计、贝叶斯估计三者之间的关系，值得仔细品读。</p><p><strong>（2）关于似然函数</strong> </p><p>Reid 等 2010 年  <a href="db36c3ec.html">《似然与基于似然的推断》</a> ： 全面地对似然函数以及基于似然的推断方法进行了综述。由于似然函数时贝叶斯方法中的重要组成部分，因此掌握这方面的知识是必要的。文中提到了似然函数及其派生量、最大似然估计及其渐进性质、剖面最大似然估计、<a href="2c19b889.html">受限最大似然估计</a>、贝叶斯估计等方法，并给出了偏似然、伪似然、组合似然、准似然、经验似然等似然函数的常用变体。另外可参考  <a href="8f2c5e85.html">《似然及其在参数估计和模型比较中的引用》</a>。</p><p>Martin 等 2022 年的 <a href="226fd4ce.html">《近似贝叶斯计算简明教程》</a> ：当似然函数无法解析建模时，只能以某种方式对似然进行近似。近似贝叶斯计算就是解决此类问题的一类方法，通过设计一个可参数化的函数来近似复杂的真实似然，进而使贝叶斯推断可以继续进行。本文节选自 Martin《Python 中的贝叶斯建模和计算》一书的第八章。 </p><p><strong>（3）关于先验</strong> </p><p>涉及共轭先验、无信息先验等内容，待整理。</p><p><strong>（4）后验推断</strong> </p><p>精确推断，待整理。</p><p>Blei 的 <a href="7e8d23a9.html">《主要的贝叶斯近似推断方法》</a> ：根据贝叶斯领域大师 Blei 关于贝叶斯推断方法的讲座整理，主要涉及蒙特卡罗方法和变分推断方法，是一篇入门贝叶斯推断方法的好资料。 </p><p>各种蒙特卡罗方法的具体介绍参见  <a href="d7dd5b59.html">《蒙特卡洛推断方法索引贴》</a>：涉及基础采样、MCMC、HMC、NUTS、SMC、SGMCMC 等重要方法。</p><p>各种变分推断方法的具体介绍参见  <a href="5f5b1f29.html">《变分推断方法索引贴》</a>： 涉及平均场变分推断、随机变分推断、黑盒变分推断、自动变分推断等里程碑方法，另外 Zhang 2018 年的 <a href="46ae35f1.html">Advances in Variational Inference</a> 介绍了变分推断的核心思想，并概述了迄今为止最主要的变分推断方法，是不可多得的好综述。</p><h3 id="1-3-基于概率图的表示、推断与学习"><a href="#1-3-基于概率图的表示、推断与学习" class="headerlink" title="1.3 基于概率图的表示、推断与学习"></a>1.3 基于概率图的表示、推断与学习</h3><p>概率图模型是利用图形化方式表达、学习和推断概率模型的优雅手段，是掌握贝叶斯方法的基本技能。</p><p>对于概率图模型比较陌生的同学，可以阅读人门帖  <a href="70f04f5e.html">《概率图模型概览》</a></p><p>进一步学习，可参考 <a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a> 和 <a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a> 课程。</p><p>主要参考书籍包括:</p><ul><li>Koller, Daphne. Probabilistic Graphical Models : Principles and Techniques. Cambridge, Massachusetts: The MIT Press, 2009.</li><li>Jordan, Michael Irwin, ed. Learning in Graphical Models. Adaptive Computation and Machine Learning. Cambridge, Mass: MIT Press, 1999.</li></ul><h3 id="1-4-常见模型的贝叶斯建模与推断"><a href="#1-4-常见模型的贝叶斯建模与推断" class="headerlink" title="1.4 常见模型的贝叶斯建模与推断"></a>1.4 常见模型的贝叶斯建模与推断</h3><p>常见模型的概率图表示、推断及学习，内容较多待整理。</p><h3 id="1-5-贝叶斯优化问题"><a href="#1-5-贝叶斯优化问题" class="headerlink" title="1.5 贝叶斯优化问题"></a>1.5 贝叶斯优化问题</h3><p>为各类机器学习模型 <em>寻找最佳超参数</em> 本身是一种优化问题，与传统优化针对目标函数做出一些假设不同，在超参数调整过程中训练模型的代价可能非常高，而且如果某些超参数是离散型的，也不存在传统优化中的梯度概念，更困难的是，从超参数到性能的映射可能高度复杂且多峰的，局部的优化可能并不会产生可接受的结果。从目前来看，解决此类问题的主要方法是贝叶斯优化方法。</p><p>暂时对此主题探讨不够深入，请参考 《贝叶斯优化》 一书的 <a href="5d350677.html">《引言》章节</a>。</p><h3 id="1-6-概率模型与神经网络的结合"><a href="#1-6-概率模型与神经网络的结合" class="headerlink" title="1.6 概率模型与神经网络的结合"></a>1.6 概率模型与神经网络的结合</h3><p><strong>（1）神经网络的不确定性量化</strong> </p><p>回归与分类任务中中的不确定性量化，参见 Gawlikowski 等 2021 年的 <a href="926f8964.html">《深度神经网络中的不确定性综述文章》</a> ： 该文全面概述了神经网络中的不确定性估计，回顾了该领域的最新进展。论文首先对不确定性来源这一关键因素进行了全面介绍，并将其分为（可还原的） <strong>模型不确定性</strong> 和（不可还原的） <strong>数据不确定性</strong> 。介绍了基于<code>单一确定性神经网络</code>、<code>贝叶斯神经网络</code>、<code>神经网络集成</code>、<code>测试时数据增强</code> 四种不确定性的建模方法，讨论了这些领域的不同分支及最新发展。在实际应用方面，我们讨论了各种不确定性的度量和校准方法，并评述了现有基线和可用成果。</p><p><strong>（2） 神经网络实现高斯过程</strong> </p><ul><li><p>Garnelo2018 年的 <a href="22316bf9.html">《条件神经过程》</a>。首次提出了条件神经过程和神经过程的概念，采用元学习实现了深度学习灵活性和概率模型不确定性的结合，算是用神经网络实现随机过程的最早尝试。该方法的问题在于无法为相同的背景点生成不同的函数样本，即缺少不确定性建模能力。</p></li><li><p>Garnelo2018 年的 <a href="650d46e1.html">《神经过程》</a>，另参见 Kaspar 2018 年的一个<a href="c49f015e.html">博文</a>。为了提升不确定性建模能力，在条件神经过程基础上增加了一个类似于 VAE 瓶颈的隐变量 $z$，$z$ 的每一个随机样本都对应于随机过程的一个具体实现，这样就可以通过多个样本在解码器网络中的前向传递，生成目标处的预测分布。作者将整个模型命名为神经过程。该方法的问题在于单个预测输出虽然包含了不确定性（即测试点处的边缘分布），但不同点处的输出之间相互独立，无法对输出的相关性建模，这从某种程度上来说，失去了随机过程的优势。</p></li><li><p>Kim 等 2019 年提出的 <a href="">《注意力神经过程》</a> : 为了实现对输出相关性建模，在神经过程中引入注意力机制。</p></li><li><p>Bruinsma 等 2021 年的 <a href="6c68a4b9.html">《高斯神经过程》</a> : 采用函数 $KL$ 散度作为训练的代价函数，同时为了解决输出相关性建模问题，引入了一个用于学习核函数的神经网络，并将其与神经过程网络的结合体称为高斯神经过程。</p></li><li><p>Markou 等 2021 年的 <a href="e85cc444.html">《高效的高斯神经过程回归》</a>： 认为 Bruinsma 的高斯神经过程方法采用的 CNN 神经网络（ 本文作者称为为 FullConvGP）会限制输入的维度（$D = 1$ ），因此提出了对原始高斯神经过程方法的改进，并将新模型称为卷积高斯神经过程（ConvGP）。</p></li><li><p>Dutordoir 等 2022 年的 <a href="c0d702a8.html">《神经扩散过程》</a>：将扩散模型引入神经过程，</p></li><li><p>Nguyen 等 2022 年的  <a href="a099fc2c.html">《transformer 神经过程》</a>: transformer 神经过程。 </p></li><li><p>Bruinsma 等 2023 年的 <a href="5f9a5d71.html">《自回归条件神经过程》</a> : 还是为了提升相关性预测能力，但自回归条件神经过程并不对模型或训练过程进行任何修改，而是像 MCDropout、神经自回归密度估计器 (NADE) 等一样，改变了 CNP 在测试阶段的部署方式，使用概率链式法则来自回归地定义联合预测分布，而不是对每个目标点独立进行预测。</p></li></ul><p><strong>（3） 高斯过程模拟和解释神经网络</strong> </p><ul><li><p><strong>Neal</strong> 等 1994 年《无线宽神经网络的先验》: 单隐层无限宽神经网络等效于高斯过程。</p></li><li><p><strong>Williams</strong> 等 1997 年  [《Computing with infinite networks》]: 计算出了单隐层神经网络的解析高斯过程核，并给出了使用高斯过程先验进行回归的精确贝叶斯推断方法。</p></li><li><p><strong>Hazan</strong> 等 2015 年的《Steps toward deep kernel methods from infinite neural networks》：讨论了无限宽深度神经网络的等效核构建问题，但只限于两个非线性隐藏层。</p></li><li><p><strong>Daniely</strong> 等 2016 年的《Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity》： 将组合核方法扩展到神经网络，利用有向无环图构造了神经网络的 “具有相同非线性全连接拓扑的组合核”。</p></li><li><p><strong>Lee</strong> 等 2017 年的 <a href="9515d5ad.html">《神经网络高斯过程》</a>： 论证分析了深度的无线宽神经网络等效于高斯过程。</p></li><li><p><strong>Matthews</strong> 等 2018 年的 <a href="bddb7fac.html">《宽深度神经网络的高斯过程表现》</a> :</p></li><li><p><strong>Jacot</strong> 等 2018 年的 <a href="92799764.html">《神经切线核》</a>：剖析了神经网络训练期间的动态特性，并认为其训练动力学可以被视为一种神经正切核机制， 入门参见 Rajatvd 2019 年的 <a href="473bc1cc.html">《神经正切核入门》</a>， Novak 2019 年的 <a href="c1f3dd64.html">《神经切线核之 Python 实现》</a></p></li><li><p><strong>Domingos</strong> 等 2020 年的  <a href="80deb1b2.html">《梯度下降学得的模型都近似于一个核机》</a>：在神经正切核基础上，提出了路径核的概念，并认为所有通过梯度下降学得的模型，都可以被视为一种核机器。</p></li><li><p><strong>Li</strong>  等 2022 年的  <a href="">《神经网络的高斯过程代理模型》</a> 。将深度学习网络视为为内部过程不透明的复杂系统，用易于解释的高斯过程取代（或模仿）复杂神经网络系统的行为，这种高斯过程代理模型能够从神经网络的自然行为中凭经验学习高斯过程的核，这与 Lee 、Matthews Domingos 等从神经网络的极限情况下推导核具有显著不同。</p></li></ul><p><strong>（4）生成式神经网络</strong> </p><p>所有的生成模型几乎都与学习数据分布以及采样有关，也是概率模型与神经网络产出最多的领域。</p><ul><li><p>受限玻尔兹曼机：Hinton 等提出的 <a href="3f3fecac.html">受限玻尔兹曼机及深度置信网络</a></p></li><li><p>变分自编码器：Kingma 等 2014 年提出的变分自编码器，入门可以先阅读<a href="65612c13.html">《初始变分自编码器》</a>，进一步可以阅读原作者 2019 年撰写的 <a href="da72f251.html">《权威综述》</a>。 </p></li><li><p>自回归神经网络：参看 Murphy 2023 年的  <a href="d097769d.html">《Probabilistic Machine Learning: Advanced Topics》 第 22 章</a>。</p></li><li><p>归一化流：参见 Papamakarios 等人 2021 年的综述文章 <a href="1e755394.html"> 《Normalizing Flows for Probabilistic Modeling and Inference》 </a></p></li><li><p>基于能量的模型：参看 Murphy 2023 年的  <a href="d8f42e3d.html">《Probabilistic Machine Learning: Advanced Topics》 第 24 章</a></p></li><li><p>生成式对抗网络： 待整理</p></li><li><p>扩散模型：参见 Yang 等 2022 年的综述 <a href="c0fb1f85.html">《Diffusion Models: A Comprehensive Survey of Methods and Applications》</a> 以及 <a href="3b7358a6.html">扩散模型概览</a></p></li></ul><h2 id="二、知识体系的构建"><a href="#二、知识体系的构建" class="headerlink" title="二、知识体系的构建"></a>二、知识体系的构建</h2><p>贝叶斯统计方法以贝叶斯规则指导，已经基本形成了以概率图为形式化工具的一套相对完整的知识体系。贝叶斯新手建议由浅入深得学习。个人建议分为三个层次：</p><h3 id="入门层次"><a href="#入门层次" class="headerlink" title="入门层次"></a>入门层次</h3><ul><li>愿 景：掌握基础概念和入门级别的工作能力。</li><li>目 的：<ul><li>理解贝叶斯思维</li><li>学会基础的贝叶斯建模流程和工作方法</li><li>学会简单的概率编程</li><li>感性认识 MCMC、变分推断等统计推断方法</li><li>掌握线性回归、多元线性回归、广义线性回归等基础回归模型</li><li>理解高斯混合模型、狄利克雷过程、连续混合模型等隐变量模型</li><li>理解高斯过程等非参数贝叶斯方法</li></ul></li><li>书 籍：<ul><li>Osvaldo Martin, [Bayesian Analysis with Python(2nd)](<a href="https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html">https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html</a></li><li>Osvaldo Martin,<a href="https://xishansnow.github.io/BayesianModelingandComputationInPython/index.html">Bayesian Modeling and Computation in Python</a></li><li>McElreath, R. . [Statistical Rethinking (2nd )]. <a href="https://doi.org/10.1201/9781315372495">https://doi.org/10.1201/9781315372495</a></li><li>Kruschke, <a href="https://sites.google.com/site/doingbayesiandataanalysis/">Doing Bayesian Data Analysis</a></li><li>Andrew Gelman et al., <a href="https://avehtari.github.io/BDA_course_Aalto/index.html">Bayesian Data Analysis(3rd)</a></li></ul></li><li>教 程：<ul><li>Herbert Lee, <a href="https://www.coursera.org/learn/bayesian-statistics?specialization=bayesian-statistics">Bayesian Statistics: From Concept to Data Analysis</a></li><li>Matthew Helner, <a href="https://www.coursera.org/learn/mcmc-bayesian-statistics?specialization=bayesian-statistics">Bayesian Statistics: Techniques and Models</a></li><li>McElreath R. et al., [Statistical Rethinking 2022] <a href="https://github.com/rmcelreath/stat_rethinking_2022">https://github.com/rmcelreath/stat_rethinking_2022</a></li><li>Andrew Gelman et al., <a href="https://avehtari.github.io/BDA_course_Aalto/index.html">Bayesian Data Analysis Course</a></li><li>Padhraic Smyth et al., <a href="https://www.ics.uci.edu/~smyth/courses/cs274/">California Unvi., CS274A-Probabilistic Learning:Theory and Algorithms</a></li></ul></li></ul><hr><h3 id="中级层次"><a href="#中级层次" class="headerlink" title="中级层次"></a>中级层次</h3><ul><li>愿 景：熟练掌握概率图模型，并利用概率图模型进行建模、学习和推断。</li><li>目 标：<ul><li>理解什么是概率图模型</li><li>掌握贝叶斯网络、马尔可夫随机场两种表示方法</li><li>掌握变量消除、消息传递等概率图推断的传统方法</li><li>掌握 MCMC、变分推断等近似推断基本原理和方法</li><li>掌握完全可观测模型、部分可观测模型的学习原理和方法</li><li>掌握高斯过程、狄利克雷过程等非参数模型的概率图方法</li><li>掌握因子分析、主组分分析、隐马尔可夫、状态空间等常用的概率图模型</li></ul></li><li>书 籍：<ul><li>Koller, D. (2009). Probabilistic graphical models: Principles and techniques. The MIT Press.</li><li>Michael I. Jordan, An Introduction to Probabilistic Graphical Models</li></ul></li><li>教 程：<ul><li>Stefano ERmon et al., <a href="https://ermongroup.github.io/cs228/">Standford Univ., CS228- Probabilistic Graphical Models</a></li><li>Daphne Koller, Standford Univ., <a href="https://www.coursera.org/specializations/probabilistic-graphical-models">Probabilistic Graphical Models: Master a new way of reasoning and learning in complex domains</a></li></ul></li><li>Erik Sudderth et al. <a href="https://canvas.eee.uci.edu/courses/35909">California Univ. CS274B-Learning in Graphical Models</a></li><li>Eric P.Xing et al. <a href="http://www.cs.cmu.edu/~epxing/Class/10708-20/index.html">CMU. 10-708-Probabilistic Graphical Models</a> ， 课程的 Lecture 和 Notes 都非常全，其中高级主题部分可以纳入下一个层次</li></ul><hr><h3 id="高级层级"><a href="#高级层级" class="headerlink" title="高级层级"></a>高级层级</h3><ul><li>愿 景：掌握概率图和神经网络的结合和应用方法。</li><li>目 标：<ul><li>熟练使用概率图和计算图建立概率神经网络模型</li></ul></li></ul>    <style>    #refplus, #refplus li{         padding:0;        margin:0;        list-style:none;    }；    </style>    <script src="https://unpkg.com/@popperjs/core@2"></script>    <script src="https://unpkg.com/tippy.js@6"></script>    <script>    document.querySelectorAll(".refplus-num").forEach((ref) => {        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');        let refel = document.querySelector(refid);        let refnum = refel.dataset.num;        let ref_content = refel.innerText.replace(`[${refnum}]`,'');        tippy(ref, {            content: ref_content,        });    });    </script>    ]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 综述概览 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 统计建模 </tag>
            
            <tag> 统计学习 </tag>
            
            <tag> 统计推断 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>🔥  贝叶斯方法索引帖</title>
      <link href="/vll-pages/posts/7a2e65c2.html"/>
      <url>/vll-pages/posts/7a2e65c2.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h2 id="一、知识要点"><a href="#一、知识要点" class="headerlink" title="一、知识要点"></a>一、知识要点</h2><h3 id="1-1-贝叶斯思维与工作流"><a href="#1-1-贝叶斯思维与工作流" class="headerlink" title="1.1 贝叶斯思维与工作流"></a>1.1 贝叶斯思维与工作流</h3><p>推荐的几本基础入门书籍：</p><ul><li>Martin 2015 年的 <a href="https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html">《Bayesian Analysis with Python》</a> </li><li>Martin 2022 年的 <a href="https://xishansnow.github.io/BayesianModelingandComputationInPython/index.html">《Bayesian Modeling and Computation in Python》</a></li><li>Kruschke 2015 年的 <a href="https://sites.google.com/site/doingbayesiandataanalysis/">《Doing Bayesian Data Analysis》</a></li></ul><h3 id="1-2-主要的贝叶斯推断方法"><a href="#1-2-主要的贝叶斯推断方法" class="headerlink" title="1.2 主要的贝叶斯推断方法"></a>1.2 主要的贝叶斯推断方法</h3><p><strong>（1） 关于基础的推断方法</strong> </p><p><a href="https://engineering.purdue.edu/kak/Tutorials/Trinity.pdf">普渡大学机器人视觉实验室的自编教程</a>： 普渡大学的一篇入门教程，清晰地从贝叶斯定理出发，阐明了最大似然估计、最大后验估计、贝叶斯估计三者之间的关系，值得仔细品读。</p><p><strong>（2）关于似然函数</strong> </p><p>Reid 等 2010 年  <a href="db36c3ec.html">《似然与基于似然的推断》</a> ： 全面地对似然函数以及基于似然的推断方法进行了综述。由于似然函数时贝叶斯方法中的重要组成部分，因此掌握这方面的知识是必要的。文中提到了似然函数及其派生量、最大似然估计及其渐进性质、剖面最大似然估计、<a href="2c19b889.html">受限最大似然估计</a>、贝叶斯估计等方法，并给出了偏似然、伪似然、组合似然、准似然、经验似然等似然函数的常用变体。另外可参考  <a href="8f2c5e85.html">《似然及其在参数估计和模型比较中的引用》</a>。</p><p>Martin 等 2022 年的 <a href="226fd4ce.html">《近似贝叶斯计算简明教程》</a> ：当似然函数无法解析建模时，只能以某种方式对似然进行近似。近似贝叶斯计算就是解决此类问题的一类方法，通过设计一个可参数化的函数来近似复杂的真实似然，进而使贝叶斯推断可以继续进行。本文节选自 Martin《Python 中的贝叶斯建模和计算》一书的第八章。 </p><p><strong>（3）关于先验</strong> </p><p>涉及共轭先验、无信息先验等内容，待整理。</p><p><strong>（4）后验推断</strong> </p><p>精确推断，待整理。</p><p>Blei 的 <a href="7e8d23a9.html">《主要的贝叶斯近似推断方法》</a> ：根据贝叶斯领域大师 Blei 关于贝叶斯推断方法的讲座整理，主要涉及蒙特卡罗方法和变分推断方法，是一篇入门贝叶斯推断方法的好资料。 </p><p>各种蒙特卡罗方法的具体介绍参见  <a href="d7dd5b59.html">《蒙特卡洛推断方法索引贴》</a>：涉及基础采样、MCMC、HMC、NUTS、SMC、SGMCMC 等重要方法。</p><p>各种变分推断方法的具体介绍参见  <a href="5f5b1f29.html">《变分推断方法索引贴》</a>： 涉及平均场变分推断、随机变分推断、黑盒变分推断、自动变分推断等里程碑方法，另外 Zhang 2018 年的 <a href="46ae35f1.html">Advances in Variational Inference</a> 介绍了变分推断的核心思想，并概述了迄今为止最主要的变分推断方法，是不可多得的好综述。</p><h3 id="1-3-基于概率图的表示、推断与学习"><a href="#1-3-基于概率图的表示、推断与学习" class="headerlink" title="1.3 基于概率图的表示、推断与学习"></a>1.3 基于概率图的表示、推断与学习</h3><p>概率图模型是利用图形化方式表达、学习和推断概率模型的优雅手段，是掌握贝叶斯方法的基本技能。</p><p>对于概率图模型比较陌生的同学，可以阅读人门帖  <a href="70f04f5e.html">《概率图模型概览》</a></p><p>进一步学习，可参考 <a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a> 和 <a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a> 课程。</p><p>主要参考书籍包括:</p><ul><li>Koller, Daphne. Probabilistic Graphical Models : Principles and Techniques. Cambridge, Massachusetts: The MIT Press, 2009.</li><li>Jordan, Michael Irwin, ed. Learning in Graphical Models. Adaptive Computation and Machine Learning. Cambridge, Mass: MIT Press, 1999.</li></ul><h3 id="1-4-常见模型的贝叶斯建模与推断"><a href="#1-4-常见模型的贝叶斯建模与推断" class="headerlink" title="1.4 常见模型的贝叶斯建模与推断"></a>1.4 常见模型的贝叶斯建模与推断</h3><p>常见模型的概率图表示、推断及学习，内容较多待整理。</p><h3 id="1-5-贝叶斯优化问题"><a href="#1-5-贝叶斯优化问题" class="headerlink" title="1.5 贝叶斯优化问题"></a>1.5 贝叶斯优化问题</h3><p>为各类机器学习模型 <em>寻找最佳超参数</em> 本身是一种优化问题，与传统优化针对目标函数做出一些假设不同，在超参数调整过程中训练模型的代价可能非常高，而且如果某些超参数是离散型的，也不存在传统优化中的梯度概念，更困难的是，从超参数到性能的映射可能高度复杂且多峰的，局部的优化可能并不会产生可接受的结果。从目前来看，解决此类问题的主要方法是贝叶斯优化方法。</p><p>暂时对此主题探讨不够深入，请参考 《贝叶斯优化》 一书的 <a href="5d350677.html">《引言》章节</a>。</p><h3 id="1-6-概率模型与神经网络的结合"><a href="#1-6-概率模型与神经网络的结合" class="headerlink" title="1.6 概率模型与神经网络的结合"></a>1.6 概率模型与神经网络的结合</h3><p><strong>（1）神经网络的不确定性量化</strong> </p><p>回归与分类任务中中的不确定性量化，参见 Gawlikowski 等 2021 年的 <a href="926f8964.html">《深度神经网络中的不确定性综述文章》</a> ： 该文全面概述了神经网络中的不确定性估计，回顾了该领域的最新进展。论文首先对不确定性来源这一关键因素进行了全面介绍，并将其分为（可还原的） <strong>模型不确定性</strong> 和（不可还原的） <strong>数据不确定性</strong> 。介绍了基于<code>单一确定性神经网络</code>、<code>贝叶斯神经网络</code>、<code>神经网络集成</code>、<code>测试时数据增强</code> 四种不确定性的建模方法，讨论了这些领域的不同分支及最新发展。在实际应用方面，我们讨论了各种不确定性的度量和校准方法，并评述了现有基线和可用成果。</p><p><strong>（2） 神经网络实现高斯过程</strong> </p><ul><li><p>Garnelo2018 年的 <a href="22316bf9.html">《条件神经过程》</a>。首次提出了条件神经过程和神经过程的概念，采用元学习实现了深度学习灵活性和概率模型不确定性的结合，算是用神经网络实现随机过程的最早尝试。该方法的问题在于无法为相同的背景点生成不同的函数样本，即缺少不确定性建模能力。</p></li><li><p>Garnelo2018 年的 <a href="650d46e1.html">《神经过程》</a>，另参见 Kaspar 2018 年的一个<a href="c49f015e.html">博文</a>。为了提升不确定性建模能力，在条件神经过程基础上增加了一个类似于 VAE 瓶颈的隐变量 $z$，$z$ 的每一个随机样本都对应于随机过程的一个具体实现，这样就可以通过多个样本在解码器网络中的前向传递，生成目标处的预测分布。作者将整个模型命名为神经过程。该方法的问题在于单个预测输出虽然包含了不确定性（即测试点处的边缘分布），但不同点处的输出之间相互独立，无法对输出的相关性建模，这从某种程度上来说，失去了随机过程的优势。</p></li><li><p>Kim 等 2019 年提出的 <a href="">《注意力神经过程》</a> : 为了实现对输出相关性建模，在神经过程中引入注意力机制。</p></li><li><p>Bruinsma 等 2021 年的 <a href="6c68a4b9.html">《高斯神经过程》</a> : 采用函数 $KL$ 散度作为训练的代价函数，同时为了解决输出相关性建模问题，引入了一个用于学习核函数的神经网络，并将其与神经过程网络的结合体称为高斯神经过程。</p></li><li><p>Markou 等 2021 年的 <a href="e85cc444.html">《高效的高斯神经过程回归》</a>： 认为 Bruinsma 的高斯神经过程方法采用的 CNN 神经网络（ 本文作者称为为 FullConvGP）会限制输入的维度（$D = 1$ ），因此提出了对原始高斯神经过程方法的改进，并将新模型称为卷积高斯神经过程（ConvGP）。</p></li><li><p>Dutordoir 等 2022 年的 <a href="c0d702a8.html">《神经扩散过程》</a>：将扩散模型引入神经过程，</p></li><li><p>Nguyen 等 2022 年的  <a href="a099fc2c.html">《transformer 神经过程》</a>: transformer 神经过程。 </p></li><li><p>Bruinsma 等 2023 年的 <a href="5f9a5d71.html">《自回归条件神经过程》</a> : 还是为了提升相关性预测能力，但自回归条件神经过程并不对模型或训练过程进行任何修改，而是像 MCDropout、神经自回归密度估计器 (NADE) 等一样，改变了 CNP 在测试阶段的部署方式，使用概率链式法则来自回归地定义联合预测分布，而不是对每个目标点独立进行预测。</p></li></ul><p><strong>（3） 高斯过程模拟和解释神经网络</strong> </p><ul><li><p><strong>Neal</strong> 等 1994 年《无线宽神经网络的先验》: 单隐层无限宽神经网络等效于高斯过程。</p></li><li><p><strong>Williams</strong> 等 1997 年  [《Computing with infinite networks》]: 计算出了单隐层神经网络的解析高斯过程核，并给出了使用高斯过程先验进行回归的精确贝叶斯推断方法。</p></li><li><p><strong>Hazan</strong> 等 2015 年的《Steps toward deep kernel methods from infinite neural networks》：讨论了无限宽深度神经网络的等效核构建问题，但只限于两个非线性隐藏层。</p></li><li><p><strong>Daniely</strong> 等 2016 年的《Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity》： 将组合核方法扩展到神经网络，利用有向无环图构造了神经网络的 “具有相同非线性全连接拓扑的组合核”。</p></li><li><p><strong>Lee</strong> 等 2017 年的 <a href="9515d5ad.html">《神经网络高斯过程》</a>： 论证分析了深度的无线宽神经网络等效于高斯过程。</p></li><li><p><strong>Matthews</strong> 等 2018 年的 <a href="bddb7fac.html">《宽深度神经网络的高斯过程表现》</a> :</p></li><li><p><strong>Jacot</strong> 等 2018 年的 <a href="92799764.html">《神经切线核》</a>：剖析了神经网络训练期间的动态特性，并认为其训练动力学可以被视为一种神经正切核机制， 入门参见 Rajatvd 2019 年的 <a href="473bc1cc.html">《神经正切核入门》</a>， Novak 2019 年的 <a href="c1f3dd64.html">《神经切线核之 Python 实现》</a></p></li><li><p><strong>Domingos</strong> 等 2020 年的  <a href="80deb1b2.html">《梯度下降学得的模型都近似于一个核机》</a>：在神经正切核基础上，提出了路径核的概念，并认为所有通过梯度下降学得的模型，都可以被视为一种核机器。</p></li><li><p><strong>Li</strong>  等 2022 年的  <a href="">《神经网络的高斯过程代理模型》</a> 。将深度学习网络视为为内部过程不透明的复杂系统，用易于解释的高斯过程取代（或模仿）复杂神经网络系统的行为，这种高斯过程代理模型能够从神经网络的自然行为中凭经验学习高斯过程的核，这与 Lee 、Matthews Domingos 等从神经网络的极限情况下推导核具有显著不同。</p></li></ul><p><strong>（4）生成式神经网络</strong> </p><p>所有的生成模型几乎都与学习数据分布以及采样有关，也是概率模型与神经网络产出最多的领域。</p><ul><li><p>受限玻尔兹曼机：Hinton 等提出的 <a href="3f3fecac.html">受限玻尔兹曼机及深度置信网络</a></p></li><li><p>变分自编码器：Kingma 等 2014 年提出的变分自编码器，入门可以先阅读<a href="65612c13.html">《初始变分自编码器》</a>，进一步可以阅读原作者 2019 年撰写的 <a href="da72f251.html">《权威综述》</a>。 </p></li><li><p>自回归神经网络：参看 Murphy 2023 年的  <a href="d097769d.html">《Probabilistic Machine Learning: Advanced Topics》 第 22 章</a>。</p></li><li><p>归一化流：参见 Papamakarios 等人 2021 年的综述文章 <a href="1e755394.html"> 《Normalizing Flows for Probabilistic Modeling and Inference》 </a></p></li><li><p>基于能量的模型：参看 Murphy 2023 年的  <a href="d8f42e3d.html">《Probabilistic Machine Learning: Advanced Topics》 第 24 章</a></p></li><li><p>生成式对抗网络： 待整理</p></li><li><p>扩散模型：参见 Yang 等 2022 年的综述 <a href="c0fb1f85.html">《Diffusion Models: A Comprehensive Survey of Methods and Applications》</a> 以及 <a href="3b7358a6.html">扩散模型概览</a></p></li></ul><h2 id="二、知识体系的构建"><a href="#二、知识体系的构建" class="headerlink" title="二、知识体系的构建"></a>二、知识体系的构建</h2><p>贝叶斯统计方法以贝叶斯规则指导，已经基本形成了以概率图为形式化工具的一套相对完整的知识体系。贝叶斯新手建议由浅入深得学习。个人建议分为三个层次：</p><h3 id="入门层次"><a href="#入门层次" class="headerlink" title="入门层次"></a>入门层次</h3><ul><li>愿 景：掌握基础概念和入门级别的工作能力。</li><li>目 的：<ul><li>理解贝叶斯思维</li><li>学会基础的贝叶斯建模流程和工作方法</li><li>学会简单的概率编程</li><li>感性认识 MCMC、变分推断等统计推断方法</li><li>掌握线性回归、多元线性回归、广义线性回归等基础回归模型</li><li>理解高斯混合模型、狄利克雷过程、连续混合模型等隐变量模型</li><li>理解高斯过程等非参数贝叶斯方法</li></ul></li><li>书 籍：<ul><li>Osvaldo Martin, [Bayesian Analysis with Python(2nd)](<a href="https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html">https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html</a></li><li>Osvaldo Martin,<a href="https://xishansnow.github.io/BayesianModelingandComputationInPython/index.html">Bayesian Modeling and Computation in Python</a></li><li>McElreath, R. . [Statistical Rethinking (2nd )]. <a href="https://doi.org/10.1201/9781315372495">https://doi.org/10.1201/9781315372495</a></li><li>Kruschke, <a href="https://sites.google.com/site/doingbayesiandataanalysis/">Doing Bayesian Data Analysis</a></li><li>Andrew Gelman et al., <a href="https://avehtari.github.io/BDA_course_Aalto/index.html">Bayesian Data Analysis(3rd)</a></li></ul></li><li>教 程：<ul><li>Herbert Lee, <a href="https://www.coursera.org/learn/bayesian-statistics?specialization=bayesian-statistics">Bayesian Statistics: From Concept to Data Analysis</a></li><li>Matthew Helner, <a href="https://www.coursera.org/learn/mcmc-bayesian-statistics?specialization=bayesian-statistics">Bayesian Statistics: Techniques and Models</a></li><li>McElreath R. et al., [Statistical Rethinking 2022] <a href="https://github.com/rmcelreath/stat_rethinking_2022">https://github.com/rmcelreath/stat_rethinking_2022</a></li><li>Andrew Gelman et al., <a href="https://avehtari.github.io/BDA_course_Aalto/index.html">Bayesian Data Analysis Course</a></li><li>Padhraic Smyth et al., <a href="https://www.ics.uci.edu/~smyth/courses/cs274/">California Unvi., CS274A-Probabilistic Learning:Theory and Algorithms</a></li></ul></li></ul><hr><h3 id="中级层次"><a href="#中级层次" class="headerlink" title="中级层次"></a>中级层次</h3><ul><li>愿 景：熟练掌握概率图模型，并利用概率图模型进行建模、学习和推断。</li><li>目 标：<ul><li>理解什么是概率图模型</li><li>掌握贝叶斯网络、马尔可夫随机场两种表示方法</li><li>掌握变量消除、消息传递等概率图推断的传统方法</li><li>掌握 MCMC、变分推断等近似推断基本原理和方法</li><li>掌握完全可观测模型、部分可观测模型的学习原理和方法</li><li>掌握高斯过程、狄利克雷过程等非参数模型的概率图方法</li><li>掌握因子分析、主组分分析、隐马尔可夫、状态空间等常用的概率图模型</li></ul></li><li>书 籍：<ul><li>Koller, D. (2009). Probabilistic graphical models: Principles and techniques. The MIT Press.</li><li>Michael I. Jordan, An Introduction to Probabilistic Graphical Models</li></ul></li><li>教 程：<ul><li>Stefano ERmon et al., <a href="https://ermongroup.github.io/cs228/">Standford Univ., CS228- Probabilistic Graphical Models</a></li><li>Daphne Koller, Standford Univ., <a href="https://www.coursera.org/specializations/probabilistic-graphical-models">Probabilistic Graphical Models: Master a new way of reasoning and learning in complex domains</a></li></ul></li><li>Erik Sudderth et al. <a href="https://canvas.eee.uci.edu/courses/35909">California Univ. CS274B-Learning in Graphical Models</a></li><li>Eric P.Xing et al. <a href="http://www.cs.cmu.edu/~epxing/Class/10708-20/index.html">CMU. 10-708-Probabilistic Graphical Models</a> ， 课程的 Lecture 和 Notes 都非常全，其中高级主题部分可以纳入下一个层次</li></ul><hr><h3 id="高级层级"><a href="#高级层级" class="headerlink" title="高级层级"></a>高级层级</h3><ul><li>愿 景：掌握概率图和神经网络的结合和应用方法。</li><li>目 标：<ul><li>熟练使用概率图和计算图建立概率神经网络模型</li></ul></li></ul>    <style>    #refplus, #refplus li{         padding:0;        margin:0;        list-style:none;    }；    </style>    <script src="https://unpkg.com/@popperjs/core@2"></script>    <script src="https://unpkg.com/tippy.js@6"></script>    <script>    document.querySelectorAll(".refplus-num").forEach((ref) => {        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');        let refel = document.querySelector(refid);        let refnum = refel.dataset.num;        let ref_content = refel.innerText.replace(`[${refnum}]`,'');        tippy(ref, {            content: ref_content,        });    });    </script>    ]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 综述概览 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 统计建模 </tag>
            
            <tag> 统计学习 </tag>
            
            <tag> 统计推断 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>🔥  贝叶斯方法索引帖</title>
      <link href="/vll-pages/posts/7a2e65c2.html"/>
      <url>/vll-pages/posts/7a2e65c2.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h2 id="一、知识要点"><a href="#一、知识要点" class="headerlink" title="一、知识要点"></a>一、知识要点</h2><h3 id="1-1-贝叶斯思维与工作流"><a href="#1-1-贝叶斯思维与工作流" class="headerlink" title="1.1 贝叶斯思维与工作流"></a>1.1 贝叶斯思维与工作流</h3><p>推荐的几本基础入门书籍：</p><ul><li>Martin 2015 年的 <a href="https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html">《Bayesian Analysis with Python》</a> </li><li>Martin 2022 年的 <a href="https://xishansnow.github.io/BayesianModelingandComputationInPython/index.html">《Bayesian Modeling and Computation in Python》</a></li><li>Kruschke 2015 年的 <a href="https://sites.google.com/site/doingbayesiandataanalysis/">《Doing Bayesian Data Analysis》</a></li></ul><h3 id="1-2-主要的贝叶斯推断方法"><a href="#1-2-主要的贝叶斯推断方法" class="headerlink" title="1.2 主要的贝叶斯推断方法"></a>1.2 主要的贝叶斯推断方法</h3><p><strong>（1） 关于基础的推断方法</strong> </p><p><a href="https://engineering.purdue.edu/kak/Tutorials/Trinity.pdf">普渡大学机器人视觉实验室的自编教程</a>： 普渡大学的一篇入门教程，清晰地从贝叶斯定理出发，阐明了最大似然估计、最大后验估计、贝叶斯估计三者之间的关系，值得仔细品读。</p><p><strong>（2）关于似然函数</strong> </p><p>Reid 等 2010 年  <a href="db36c3ec.html">《似然与基于似然的推断》</a> ： 全面地对似然函数以及基于似然的推断方法进行了综述。由于似然函数时贝叶斯方法中的重要组成部分，因此掌握这方面的知识是必要的。文中提到了似然函数及其派生量、最大似然估计及其渐进性质、剖面最大似然估计、<a href="2c19b889.html">受限最大似然估计</a>、贝叶斯估计等方法，并给出了偏似然、伪似然、组合似然、准似然、经验似然等似然函数的常用变体。另外可参考  <a href="8f2c5e85.html">《似然及其在参数估计和模型比较中的引用》</a>。</p><p>Martin 等 2022 年的 <a href="226fd4ce.html">《近似贝叶斯计算简明教程》</a> ：当似然函数无法解析建模时，只能以某种方式对似然进行近似。近似贝叶斯计算就是解决此类问题的一类方法，通过设计一个可参数化的函数来近似复杂的真实似然，进而使贝叶斯推断可以继续进行。本文节选自 Martin《Python 中的贝叶斯建模和计算》一书的第八章。 </p><p><strong>（3）关于先验</strong> </p><p>涉及共轭先验、无信息先验等内容，待整理。</p><p><strong>（4）后验推断</strong> </p><p>精确推断，待整理。</p><p>Blei 的 <a href="7e8d23a9.html">《主要的贝叶斯近似推断方法》</a> ：根据贝叶斯领域大师 Blei 关于贝叶斯推断方法的讲座整理，主要涉及蒙特卡罗方法和变分推断方法，是一篇入门贝叶斯推断方法的好资料。 </p><p>各种蒙特卡罗方法的具体介绍参见  <a href="d7dd5b59.html">《蒙特卡洛推断方法索引贴》</a>：涉及基础采样、MCMC、HMC、NUTS、SMC、SGMCMC 等重要方法。</p><p>各种变分推断方法的具体介绍参见  <a href="5f5b1f29.html">《变分推断方法索引贴》</a>： 涉及平均场变分推断、随机变分推断、黑盒变分推断、自动变分推断等里程碑方法，另外 Zhang 2018 年的 <a href="46ae35f1.html">Advances in Variational Inference</a> 介绍了变分推断的核心思想，并概述了迄今为止最主要的变分推断方法，是不可多得的好综述。</p><h3 id="1-3-基于概率图的表示、推断与学习"><a href="#1-3-基于概率图的表示、推断与学习" class="headerlink" title="1.3 基于概率图的表示、推断与学习"></a>1.3 基于概率图的表示、推断与学习</h3><p>概率图模型是利用图形化方式表达、学习和推断概率模型的优雅手段，是掌握贝叶斯方法的基本技能。</p><p>对于概率图模型比较陌生的同学，可以阅读人门帖  <a href="70f04f5e.html">《概率图模型概览》</a></p><p>进一步学习，可参考 <a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a> 和 <a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a> 课程。</p><p>主要参考书籍包括:</p><ul><li>Koller, Daphne. Probabilistic Graphical Models : Principles and Techniques. Cambridge, Massachusetts: The MIT Press, 2009.</li><li>Jordan, Michael Irwin, ed. Learning in Graphical Models. Adaptive Computation and Machine Learning. Cambridge, Mass: MIT Press, 1999.</li></ul><h3 id="1-4-常见模型的贝叶斯建模与推断"><a href="#1-4-常见模型的贝叶斯建模与推断" class="headerlink" title="1.4 常见模型的贝叶斯建模与推断"></a>1.4 常见模型的贝叶斯建模与推断</h3><p>常见模型的概率图表示、推断及学习，内容较多待整理。</p><h3 id="1-5-贝叶斯优化问题"><a href="#1-5-贝叶斯优化问题" class="headerlink" title="1.5 贝叶斯优化问题"></a>1.5 贝叶斯优化问题</h3><p>为各类机器学习模型 <em>寻找最佳超参数</em> 本身是一种优化问题，与传统优化针对目标函数做出一些假设不同，在超参数调整过程中训练模型的代价可能非常高，而且如果某些超参数是离散型的，也不存在传统优化中的梯度概念，更困难的是，从超参数到性能的映射可能高度复杂且多峰的，局部的优化可能并不会产生可接受的结果。从目前来看，解决此类问题的主要方法是贝叶斯优化方法。</p><p>暂时对此主题探讨不够深入，请参考 《贝叶斯优化》 一书的 <a href="5d350677.html">《引言》章节</a>。</p><h3 id="1-6-概率模型与神经网络的结合"><a href="#1-6-概率模型与神经网络的结合" class="headerlink" title="1.6 概率模型与神经网络的结合"></a>1.6 概率模型与神经网络的结合</h3><p><strong>（1）神经网络的不确定性量化</strong> </p><p>回归与分类任务中中的不确定性量化，参见 Gawlikowski 等 2021 年的 <a href="926f8964.html">《深度神经网络中的不确定性综述文章》</a> ： 该文全面概述了神经网络中的不确定性估计，回顾了该领域的最新进展。论文首先对不确定性来源这一关键因素进行了全面介绍，并将其分为（可还原的） <strong>模型不确定性</strong> 和（不可还原的） <strong>数据不确定性</strong> 。介绍了基于<code>单一确定性神经网络</code>、<code>贝叶斯神经网络</code>、<code>神经网络集成</code>、<code>测试时数据增强</code> 四种不确定性的建模方法，讨论了这些领域的不同分支及最新发展。在实际应用方面，我们讨论了各种不确定性的度量和校准方法，并评述了现有基线和可用成果。</p><p><strong>（2） 神经网络实现高斯过程</strong> </p><ul><li><p>Garnelo2018 年的 <a href="22316bf9.html">《条件神经过程》</a>。首次提出了条件神经过程和神经过程的概念，采用元学习实现了深度学习灵活性和概率模型不确定性的结合，算是用神经网络实现随机过程的最早尝试。该方法的问题在于无法为相同的背景点生成不同的函数样本，即缺少不确定性建模能力。</p></li><li><p>Garnelo2018 年的 <a href="650d46e1.html">《神经过程》</a>，另参见 Kaspar 2018 年的一个<a href="c49f015e.html">博文</a>。为了提升不确定性建模能力，在条件神经过程基础上增加了一个类似于 VAE 瓶颈的隐变量 $z$，$z$ 的每一个随机样本都对应于随机过程的一个具体实现，这样就可以通过多个样本在解码器网络中的前向传递，生成目标处的预测分布。作者将整个模型命名为神经过程。该方法的问题在于单个预测输出虽然包含了不确定性（即测试点处的边缘分布），但不同点处的输出之间相互独立，无法对输出的相关性建模，这从某种程度上来说，失去了随机过程的优势。</p></li><li><p>Kim 等 2019 年提出的 <a href="">《注意力神经过程》</a> : 为了实现对输出相关性建模，在神经过程中引入注意力机制。</p></li><li><p>Bruinsma 等 2021 年的 <a href="6c68a4b9.html">《高斯神经过程》</a> : 采用函数 $KL$ 散度作为训练的代价函数，同时为了解决输出相关性建模问题，引入了一个用于学习核函数的神经网络，并将其与神经过程网络的结合体称为高斯神经过程。</p></li><li><p>Markou 等 2021 年的 <a href="e85cc444.html">《高效的高斯神经过程回归》</a>： 认为 Bruinsma 的高斯神经过程方法采用的 CNN 神经网络（ 本文作者称为为 FullConvGP）会限制输入的维度（$D = 1$ ），因此提出了对原始高斯神经过程方法的改进，并将新模型称为卷积高斯神经过程（ConvGP）。</p></li><li><p>Dutordoir 等 2022 年的 <a href="c0d702a8.html">《神经扩散过程》</a>：将扩散模型引入神经过程，</p></li><li><p>Nguyen 等 2022 年的  <a href="a099fc2c.html">《transformer 神经过程》</a>: transformer 神经过程。 </p></li><li><p>Bruinsma 等 2023 年的 <a href="5f9a5d71.html">《自回归条件神经过程》</a> : 还是为了提升相关性预测能力，但自回归条件神经过程并不对模型或训练过程进行任何修改，而是像 MCDropout、神经自回归密度估计器 (NADE) 等一样，改变了 CNP 在测试阶段的部署方式，使用概率链式法则来自回归地定义联合预测分布，而不是对每个目标点独立进行预测。</p></li></ul><p><strong>（3） 高斯过程模拟和解释神经网络</strong> </p><ul><li><p><strong>Neal</strong> 等 1994 年《无线宽神经网络的先验》: 单隐层无限宽神经网络等效于高斯过程。</p></li><li><p><strong>Williams</strong> 等 1997 年  [《Computing with infinite networks》]: 计算出了单隐层神经网络的解析高斯过程核，并给出了使用高斯过程先验进行回归的精确贝叶斯推断方法。</p></li><li><p><strong>Hazan</strong> 等 2015 年的《Steps toward deep kernel methods from infinite neural networks》：讨论了无限宽深度神经网络的等效核构建问题，但只限于两个非线性隐藏层。</p></li><li><p><strong>Daniely</strong> 等 2016 年的《Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity》： 将组合核方法扩展到神经网络，利用有向无环图构造了神经网络的 “具有相同非线性全连接拓扑的组合核”。</p></li><li><p><strong>Lee</strong> 等 2017 年的 <a href="9515d5ad.html">《神经网络高斯过程》</a>： 论证分析了深度的无线宽神经网络等效于高斯过程。</p></li><li><p><strong>Matthews</strong> 等 2018 年的 <a href="bddb7fac.html">《宽深度神经网络的高斯过程表现》</a> :</p></li><li><p><strong>Jacot</strong> 等 2018 年的 <a href="92799764.html">《神经切线核》</a>：剖析了神经网络训练期间的动态特性，并认为其训练动力学可以被视为一种神经正切核机制， 入门参见 Rajatvd 2019 年的 <a href="473bc1cc.html">《神经正切核入门》</a>， Novak 2019 年的 <a href="c1f3dd64.html">《神经切线核之 Python 实现》</a></p></li><li><p><strong>Domingos</strong> 等 2020 年的  <a href="80deb1b2.html">《梯度下降学得的模型都近似于一个核机》</a>：在神经正切核基础上，提出了路径核的概念，并认为所有通过梯度下降学得的模型，都可以被视为一种核机器。</p></li><li><p><strong>Li</strong>  等 2022 年的  <a href="">《神经网络的高斯过程代理模型》</a> 。将深度学习网络视为为内部过程不透明的复杂系统，用易于解释的高斯过程取代（或模仿）复杂神经网络系统的行为，这种高斯过程代理模型能够从神经网络的自然行为中凭经验学习高斯过程的核，这与 Lee 、Matthews Domingos 等从神经网络的极限情况下推导核具有显著不同。</p></li></ul><p><strong>（4）生成式神经网络</strong> </p><p>所有的生成模型几乎都与学习数据分布以及采样有关，也是概率模型与神经网络产出最多的领域。</p><ul><li><p>受限玻尔兹曼机：Hinton 等提出的 <a href="3f3fecac.html">受限玻尔兹曼机及深度置信网络</a></p></li><li><p>变分自编码器：Kingma 等 2014 年提出的变分自编码器，入门可以先阅读<a href="65612c13.html">《初始变分自编码器》</a>，进一步可以阅读原作者 2019 年撰写的 <a href="da72f251.html">《权威综述》</a>。 </p></li><li><p>自回归神经网络：参看 Murphy 2023 年的  <a href="d097769d.html">《Probabilistic Machine Learning: Advanced Topics》 第 22 章</a>。</p></li><li><p>归一化流：参见 Papamakarios 等人 2021 年的综述文章 <a href="1e755394.html"> 《Normalizing Flows for Probabilistic Modeling and Inference》 </a></p></li><li><p>基于能量的模型：参看 Murphy 2023 年的  <a href="d8f42e3d.html">《Probabilistic Machine Learning: Advanced Topics》 第 24 章</a></p></li><li><p>生成式对抗网络： 待整理</p></li><li><p>扩散模型：参见 Yang 等 2022 年的综述 <a href="c0fb1f85.html">《Diffusion Models: A Comprehensive Survey of Methods and Applications》</a> 以及 <a href="3b7358a6.html">扩散模型概览</a></p></li></ul><h2 id="二、知识体系的构建"><a href="#二、知识体系的构建" class="headerlink" title="二、知识体系的构建"></a>二、知识体系的构建</h2><p>贝叶斯统计方法以贝叶斯规则指导，已经基本形成了以概率图为形式化工具的一套相对完整的知识体系。贝叶斯新手建议由浅入深得学习。个人建议分为三个层次：</p><h3 id="入门层次"><a href="#入门层次" class="headerlink" title="入门层次"></a>入门层次</h3><ul><li>愿 景：掌握基础概念和入门级别的工作能力。</li><li>目 的：<ul><li>理解贝叶斯思维</li><li>学会基础的贝叶斯建模流程和工作方法</li><li>学会简单的概率编程</li><li>感性认识 MCMC、变分推断等统计推断方法</li><li>掌握线性回归、多元线性回归、广义线性回归等基础回归模型</li><li>理解高斯混合模型、狄利克雷过程、连续混合模型等隐变量模型</li><li>理解高斯过程等非参数贝叶斯方法</li></ul></li><li>书 籍：<ul><li>Osvaldo Martin, [Bayesian Analysis with Python(2nd)](<a href="https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html">https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html</a></li><li>Osvaldo Martin,<a href="https://xishansnow.github.io/BayesianModelingandComputationInPython/index.html">Bayesian Modeling and Computation in Python</a></li><li>McElreath, R. . [Statistical Rethinking (2nd )]. <a href="https://doi.org/10.1201/9781315372495">https://doi.org/10.1201/9781315372495</a></li><li>Kruschke, <a href="https://sites.google.com/site/doingbayesiandataanalysis/">Doing Bayesian Data Analysis</a></li><li>Andrew Gelman et al., <a href="https://avehtari.github.io/BDA_course_Aalto/index.html">Bayesian Data Analysis(3rd)</a></li></ul></li><li>教 程：<ul><li>Herbert Lee, <a href="https://www.coursera.org/learn/bayesian-statistics?specialization=bayesian-statistics">Bayesian Statistics: From Concept to Data Analysis</a></li><li>Matthew Helner, <a href="https://www.coursera.org/learn/mcmc-bayesian-statistics?specialization=bayesian-statistics">Bayesian Statistics: Techniques and Models</a></li><li>McElreath R. et al., [Statistical Rethinking 2022] <a href="https://github.com/rmcelreath/stat_rethinking_2022">https://github.com/rmcelreath/stat_rethinking_2022</a></li><li>Andrew Gelman et al., <a href="https://avehtari.github.io/BDA_course_Aalto/index.html">Bayesian Data Analysis Course</a></li><li>Padhraic Smyth et al., <a href="https://www.ics.uci.edu/~smyth/courses/cs274/">California Unvi., CS274A-Probabilistic Learning:Theory and Algorithms</a></li></ul></li></ul><hr><h3 id="中级层次"><a href="#中级层次" class="headerlink" title="中级层次"></a>中级层次</h3><ul><li>愿 景：熟练掌握概率图模型，并利用概率图模型进行建模、学习和推断。</li><li>目 标：<ul><li>理解什么是概率图模型</li><li>掌握贝叶斯网络、马尔可夫随机场两种表示方法</li><li>掌握变量消除、消息传递等概率图推断的传统方法</li><li>掌握 MCMC、变分推断等近似推断基本原理和方法</li><li>掌握完全可观测模型、部分可观测模型的学习原理和方法</li><li>掌握高斯过程、狄利克雷过程等非参数模型的概率图方法</li><li>掌握因子分析、主组分分析、隐马尔可夫、状态空间等常用的概率图模型</li></ul></li><li>书 籍：<ul><li>Koller, D. (2009). Probabilistic graphical models: Principles and techniques. The MIT Press.</li><li>Michael I. Jordan, An Introduction to Probabilistic Graphical Models</li></ul></li><li>教 程：<ul><li>Stefano ERmon et al., <a href="https://ermongroup.github.io/cs228/">Standford Univ., CS228- Probabilistic Graphical Models</a></li><li>Daphne Koller, Standford Univ., <a href="https://www.coursera.org/specializations/probabilistic-graphical-models">Probabilistic Graphical Models: Master a new way of reasoning and learning in complex domains</a></li></ul></li><li>Erik Sudderth et al. <a href="https://canvas.eee.uci.edu/courses/35909">California Univ. CS274B-Learning in Graphical Models</a></li><li>Eric P.Xing et al. <a href="http://www.cs.cmu.edu/~epxing/Class/10708-20/index.html">CMU. 10-708-Probabilistic Graphical Models</a> ， 课程的 Lecture 和 Notes 都非常全，其中高级主题部分可以纳入下一个层次</li></ul><hr><h3 id="高级层级"><a href="#高级层级" class="headerlink" title="高级层级"></a>高级层级</h3><ul><li>愿 景：掌握概率图和神经网络的结合和应用方法。</li><li>目 标：<ul><li>熟练使用概率图和计算图建立概率神经网络模型</li></ul></li></ul>    <style>    #refplus, #refplus li{         padding:0;        margin:0;        list-style:none;    }；    </style>    <script src="https://unpkg.com/@popperjs/core@2"></script>    <script src="https://unpkg.com/tippy.js@6"></script>    <script>    document.querySelectorAll(".refplus-num").forEach((ref) => {        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');        let refel = document.querySelector(refid);        let refnum = refel.dataset.num;        let ref_content = refel.innerText.replace(`[${refnum}]`,'');        tippy(ref, {            content: ref_content,        });    });    </script>    ]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 综述概览 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 统计建模 </tag>
            
            <tag> 统计学习 </tag>
            
            <tag> 统计推断 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>🔥  贝叶斯方法索引帖</title>
      <link href="/vll-pages/posts/7a2e65c2.html"/>
      <url>/vll-pages/posts/7a2e65c2.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h2 id="一、知识要点"><a href="#一、知识要点" class="headerlink" title="一、知识要点"></a>一、知识要点</h2><h3 id="1-1-贝叶斯思维与工作流"><a href="#1-1-贝叶斯思维与工作流" class="headerlink" title="1.1 贝叶斯思维与工作流"></a>1.1 贝叶斯思维与工作流</h3><p>推荐的几本基础入门书籍：</p><ul><li>Martin 2015 年的 <a href="https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html">《Bayesian Analysis with Python》</a> </li><li>Martin 2022 年的 <a href="https://xishansnow.github.io/BayesianModelingandComputationInPython/index.html">《Bayesian Modeling and Computation in Python》</a></li><li>Kruschke 2015 年的 <a href="https://sites.google.com/site/doingbayesiandataanalysis/">《Doing Bayesian Data Analysis》</a></li></ul><h3 id="1-2-主要的贝叶斯推断方法"><a href="#1-2-主要的贝叶斯推断方法" class="headerlink" title="1.2 主要的贝叶斯推断方法"></a>1.2 主要的贝叶斯推断方法</h3><p><strong>（1） 关于基础的推断方法</strong> </p><p><a href="https://engineering.purdue.edu/kak/Tutorials/Trinity.pdf">普渡大学机器人视觉实验室的自编教程</a>： 普渡大学的一篇入门教程，清晰地从贝叶斯定理出发，阐明了最大似然估计、最大后验估计、贝叶斯估计三者之间的关系，值得仔细品读。</p><p><strong>（2）关于似然函数</strong> </p><p>Reid 等 2010 年  <a href="db36c3ec.html">《似然与基于似然的推断》</a> ： 全面地对似然函数以及基于似然的推断方法进行了综述。由于似然函数时贝叶斯方法中的重要组成部分，因此掌握这方面的知识是必要的。文中提到了似然函数及其派生量、最大似然估计及其渐进性质、剖面最大似然估计、<a href="2c19b889.html">受限最大似然估计</a>、贝叶斯估计等方法，并给出了偏似然、伪似然、组合似然、准似然、经验似然等似然函数的常用变体。另外可参考  <a href="8f2c5e85.html">《似然及其在参数估计和模型比较中的引用》</a>。</p><p>Martin 等 2022 年的 <a href="226fd4ce.html">《近似贝叶斯计算简明教程》</a> ：当似然函数无法解析建模时，只能以某种方式对似然进行近似。近似贝叶斯计算就是解决此类问题的一类方法，通过设计一个可参数化的函数来近似复杂的真实似然，进而使贝叶斯推断可以继续进行。本文节选自 Martin《Python 中的贝叶斯建模和计算》一书的第八章。 </p><p><strong>（3）关于先验</strong> </p><p>涉及共轭先验、无信息先验等内容，待整理。</p><p><strong>（4）后验推断</strong> </p><p>精确推断，待整理。</p><p>Blei 的 <a href="7e8d23a9.html">《主要的贝叶斯近似推断方法》</a> ：根据贝叶斯领域大师 Blei 关于贝叶斯推断方法的讲座整理，主要涉及蒙特卡罗方法和变分推断方法，是一篇入门贝叶斯推断方法的好资料。 </p><p>各种蒙特卡罗方法的具体介绍参见  <a href="d7dd5b59.html">《蒙特卡洛推断方法索引贴》</a>：涉及基础采样、MCMC、HMC、NUTS、SMC、SGMCMC 等重要方法。</p><p>各种变分推断方法的具体介绍参见  <a href="5f5b1f29.html">《变分推断方法索引贴》</a>： 涉及平均场变分推断、随机变分推断、黑盒变分推断、自动变分推断等里程碑方法，另外 Zhang 2018 年的 <a href="46ae35f1.html">Advances in Variational Inference</a> 介绍了变分推断的核心思想，并概述了迄今为止最主要的变分推断方法，是不可多得的好综述。</p><h3 id="1-3-基于概率图的表示、推断与学习"><a href="#1-3-基于概率图的表示、推断与学习" class="headerlink" title="1.3 基于概率图的表示、推断与学习"></a>1.3 基于概率图的表示、推断与学习</h3><p>概率图模型是利用图形化方式表达、学习和推断概率模型的优雅手段，是掌握贝叶斯方法的基本技能。</p><p>对于概率图模型比较陌生的同学，可以阅读人门帖  <a href="70f04f5e.html">《概率图模型概览》</a></p><p>进一步学习，可参考 <a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a> 和 <a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a> 课程。</p><p>主要参考书籍包括:</p><ul><li>Koller, Daphne. Probabilistic Graphical Models : Principles and Techniques. Cambridge, Massachusetts: The MIT Press, 2009.</li><li>Jordan, Michael Irwin, ed. Learning in Graphical Models. Adaptive Computation and Machine Learning. Cambridge, Mass: MIT Press, 1999.</li></ul><h3 id="1-4-常见模型的贝叶斯建模与推断"><a href="#1-4-常见模型的贝叶斯建模与推断" class="headerlink" title="1.4 常见模型的贝叶斯建模与推断"></a>1.4 常见模型的贝叶斯建模与推断</h3><p>常见模型的概率图表示、推断及学习，内容较多待整理。</p><h3 id="1-5-贝叶斯优化问题"><a href="#1-5-贝叶斯优化问题" class="headerlink" title="1.5 贝叶斯优化问题"></a>1.5 贝叶斯优化问题</h3><p>为各类机器学习模型 <em>寻找最佳超参数</em> 本身是一种优化问题，与传统优化针对目标函数做出一些假设不同，在超参数调整过程中训练模型的代价可能非常高，而且如果某些超参数是离散型的，也不存在传统优化中的梯度概念，更困难的是，从超参数到性能的映射可能高度复杂且多峰的，局部的优化可能并不会产生可接受的结果。从目前来看，解决此类问题的主要方法是贝叶斯优化方法。</p><p>暂时对此主题探讨不够深入，请参考 《贝叶斯优化》 一书的 <a href="5d350677.html">《引言》章节</a>。</p><h3 id="1-6-概率模型与神经网络的结合"><a href="#1-6-概率模型与神经网络的结合" class="headerlink" title="1.6 概率模型与神经网络的结合"></a>1.6 概率模型与神经网络的结合</h3><p><strong>（1）神经网络的不确定性量化</strong> </p><p>回归与分类任务中中的不确定性量化，参见 Gawlikowski 等 2021 年的 <a href="926f8964.html">《深度神经网络中的不确定性综述文章》</a> ： 该文全面概述了神经网络中的不确定性估计，回顾了该领域的最新进展。论文首先对不确定性来源这一关键因素进行了全面介绍，并将其分为（可还原的） <strong>模型不确定性</strong> 和（不可还原的） <strong>数据不确定性</strong> 。介绍了基于<code>单一确定性神经网络</code>、<code>贝叶斯神经网络</code>、<code>神经网络集成</code>、<code>测试时数据增强</code> 四种不确定性的建模方法，讨论了这些领域的不同分支及最新发展。在实际应用方面，我们讨论了各种不确定性的度量和校准方法，并评述了现有基线和可用成果。</p><p><strong>（2） 神经网络实现高斯过程</strong> </p><ul><li><p>Garnelo2018 年的 <a href="22316bf9.html">《条件神经过程》</a>。首次提出了条件神经过程和神经过程的概念，采用元学习实现了深度学习灵活性和概率模型不确定性的结合，算是用神经网络实现随机过程的最早尝试。该方法的问题在于无法为相同的背景点生成不同的函数样本，即缺少不确定性建模能力。</p></li><li><p>Garnelo2018 年的 <a href="650d46e1.html">《神经过程》</a>，另参见 Kaspar 2018 年的一个<a href="c49f015e.html">博文</a>。为了提升不确定性建模能力，在条件神经过程基础上增加了一个类似于 VAE 瓶颈的隐变量 $z$，$z$ 的每一个随机样本都对应于随机过程的一个具体实现，这样就可以通过多个样本在解码器网络中的前向传递，生成目标处的预测分布。作者将整个模型命名为神经过程。该方法的问题在于单个预测输出虽然包含了不确定性（即测试点处的边缘分布），但不同点处的输出之间相互独立，无法对输出的相关性建模，这从某种程度上来说，失去了随机过程的优势。</p></li><li><p>Kim 等 2019 年提出的 <a href="">《注意力神经过程》</a> : 为了实现对输出相关性建模，在神经过程中引入注意力机制。</p></li><li><p>Bruinsma 等 2021 年的 <a href="6c68a4b9.html">《高斯神经过程》</a> : 采用函数 $KL$ 散度作为训练的代价函数，同时为了解决输出相关性建模问题，引入了一个用于学习核函数的神经网络，并将其与神经过程网络的结合体称为高斯神经过程。</p></li><li><p>Markou 等 2021 年的 <a href="e85cc444.html">《高效的高斯神经过程回归》</a>： 认为 Bruinsma 的高斯神经过程方法采用的 CNN 神经网络（ 本文作者称为为 FullConvGP）会限制输入的维度（$D = 1$ ），因此提出了对原始高斯神经过程方法的改进，并将新模型称为卷积高斯神经过程（ConvGP）。</p></li><li><p>Dutordoir 等 2022 年的 <a href="c0d702a8.html">《神经扩散过程》</a>：将扩散模型引入神经过程，</p></li><li><p>Nguyen 等 2022 年的  <a href="a099fc2c.html">《transformer 神经过程》</a>: transformer 神经过程。 </p></li><li><p>Bruinsma 等 2023 年的 <a href="5f9a5d71.html">《自回归条件神经过程》</a> : 还是为了提升相关性预测能力，但自回归条件神经过程并不对模型或训练过程进行任何修改，而是像 MCDropout、神经自回归密度估计器 (NADE) 等一样，改变了 CNP 在测试阶段的部署方式，使用概率链式法则来自回归地定义联合预测分布，而不是对每个目标点独立进行预测。</p></li></ul><p><strong>（3） 高斯过程模拟和解释神经网络</strong> </p><ul><li><p><strong>Neal</strong> 等 1994 年《无线宽神经网络的先验》: 单隐层无限宽神经网络等效于高斯过程。</p></li><li><p><strong>Williams</strong> 等 1997 年  [《Computing with infinite networks》]: 计算出了单隐层神经网络的解析高斯过程核，并给出了使用高斯过程先验进行回归的精确贝叶斯推断方法。</p></li><li><p><strong>Hazan</strong> 等 2015 年的《Steps toward deep kernel methods from infinite neural networks》：讨论了无限宽深度神经网络的等效核构建问题，但只限于两个非线性隐藏层。</p></li><li><p><strong>Daniely</strong> 等 2016 年的《Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity》： 将组合核方法扩展到神经网络，利用有向无环图构造了神经网络的 “具有相同非线性全连接拓扑的组合核”。</p></li><li><p><strong>Lee</strong> 等 2017 年的 <a href="9515d5ad.html">《神经网络高斯过程》</a>： 论证分析了深度的无线宽神经网络等效于高斯过程。</p></li><li><p><strong>Matthews</strong> 等 2018 年的 <a href="bddb7fac.html">《宽深度神经网络的高斯过程表现》</a> :</p></li><li><p><strong>Jacot</strong> 等 2018 年的 <a href="92799764.html">《神经切线核》</a>：剖析了神经网络训练期间的动态特性，并认为其训练动力学可以被视为一种神经正切核机制， 入门参见 Rajatvd 2019 年的 <a href="473bc1cc.html">《神经正切核入门》</a>， Novak 2019 年的 <a href="c1f3dd64.html">《神经切线核之 Python 实现》</a></p></li><li><p><strong>Domingos</strong> 等 2020 年的  <a href="80deb1b2.html">《梯度下降学得的模型都近似于一个核机》</a>：在神经正切核基础上，提出了路径核的概念，并认为所有通过梯度下降学得的模型，都可以被视为一种核机器。</p></li><li><p><strong>Li</strong>  等 2022 年的  <a href="">《神经网络的高斯过程代理模型》</a> 。将深度学习网络视为为内部过程不透明的复杂系统，用易于解释的高斯过程取代（或模仿）复杂神经网络系统的行为，这种高斯过程代理模型能够从神经网络的自然行为中凭经验学习高斯过程的核，这与 Lee 、Matthews Domingos 等从神经网络的极限情况下推导核具有显著不同。</p></li></ul><p><strong>（4）生成式神经网络</strong> </p><p>所有的生成模型几乎都与学习数据分布以及采样有关，也是概率模型与神经网络产出最多的领域。</p><ul><li><p>受限玻尔兹曼机：Hinton 等提出的 <a href="3f3fecac.html">受限玻尔兹曼机及深度置信网络</a></p></li><li><p>变分自编码器：Kingma 等 2014 年提出的变分自编码器，入门可以先阅读<a href="65612c13.html">《初始变分自编码器》</a>，进一步可以阅读原作者 2019 年撰写的 <a href="da72f251.html">《权威综述》</a>。 </p></li><li><p>自回归神经网络：参看 Murphy 2023 年的  <a href="d097769d.html">《Probabilistic Machine Learning: Advanced Topics》 第 22 章</a>。</p></li><li><p>归一化流：参见 Papamakarios 等人 2021 年的综述文章 <a href="1e755394.html"> 《Normalizing Flows for Probabilistic Modeling and Inference》 </a></p></li><li><p>基于能量的模型：参看 Murphy 2023 年的  <a href="d8f42e3d.html">《Probabilistic Machine Learning: Advanced Topics》 第 24 章</a></p></li><li><p>生成式对抗网络： 待整理</p></li><li><p>扩散模型：参见 Yang 等 2022 年的综述 <a href="c0fb1f85.html">《Diffusion Models: A Comprehensive Survey of Methods and Applications》</a> 以及 <a href="3b7358a6.html">扩散模型概览</a></p></li></ul><h2 id="二、知识体系的构建"><a href="#二、知识体系的构建" class="headerlink" title="二、知识体系的构建"></a>二、知识体系的构建</h2><p>贝叶斯统计方法以贝叶斯规则指导，已经基本形成了以概率图为形式化工具的一套相对完整的知识体系。贝叶斯新手建议由浅入深得学习。个人建议分为三个层次：</p><h3 id="入门层次"><a href="#入门层次" class="headerlink" title="入门层次"></a>入门层次</h3><ul><li>愿 景：掌握基础概念和入门级别的工作能力。</li><li>目 的：<ul><li>理解贝叶斯思维</li><li>学会基础的贝叶斯建模流程和工作方法</li><li>学会简单的概率编程</li><li>感性认识 MCMC、变分推断等统计推断方法</li><li>掌握线性回归、多元线性回归、广义线性回归等基础回归模型</li><li>理解高斯混合模型、狄利克雷过程、连续混合模型等隐变量模型</li><li>理解高斯过程等非参数贝叶斯方法</li></ul></li><li>书 籍：<ul><li>Osvaldo Martin, [Bayesian Analysis with Python(2nd)](<a href="https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html">https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html</a></li><li>Osvaldo Martin,<a href="https://xishansnow.github.io/BayesianModelingandComputationInPython/index.html">Bayesian Modeling and Computation in Python</a></li><li>McElreath, R. . [Statistical Rethinking (2nd )]. <a href="https://doi.org/10.1201/9781315372495">https://doi.org/10.1201/9781315372495</a></li><li>Kruschke, <a href="https://sites.google.com/site/doingbayesiandataanalysis/">Doing Bayesian Data Analysis</a></li><li>Andrew Gelman et al., <a href="https://avehtari.github.io/BDA_course_Aalto/index.html">Bayesian Data Analysis(3rd)</a></li></ul></li><li>教 程：<ul><li>Herbert Lee, <a href="https://www.coursera.org/learn/bayesian-statistics?specialization=bayesian-statistics">Bayesian Statistics: From Concept to Data Analysis</a></li><li>Matthew Helner, <a href="https://www.coursera.org/learn/mcmc-bayesian-statistics?specialization=bayesian-statistics">Bayesian Statistics: Techniques and Models</a></li><li>McElreath R. et al., [Statistical Rethinking 2022] <a href="https://github.com/rmcelreath/stat_rethinking_2022">https://github.com/rmcelreath/stat_rethinking_2022</a></li><li>Andrew Gelman et al., <a href="https://avehtari.github.io/BDA_course_Aalto/index.html">Bayesian Data Analysis Course</a></li><li>Padhraic Smyth et al., <a href="https://www.ics.uci.edu/~smyth/courses/cs274/">California Unvi., CS274A-Probabilistic Learning:Theory and Algorithms</a></li></ul></li></ul><hr><h3 id="中级层次"><a href="#中级层次" class="headerlink" title="中级层次"></a>中级层次</h3><ul><li>愿 景：熟练掌握概率图模型，并利用概率图模型进行建模、学习和推断。</li><li>目 标：<ul><li>理解什么是概率图模型</li><li>掌握贝叶斯网络、马尔可夫随机场两种表示方法</li><li>掌握变量消除、消息传递等概率图推断的传统方法</li><li>掌握 MCMC、变分推断等近似推断基本原理和方法</li><li>掌握完全可观测模型、部分可观测模型的学习原理和方法</li><li>掌握高斯过程、狄利克雷过程等非参数模型的概率图方法</li><li>掌握因子分析、主组分分析、隐马尔可夫、状态空间等常用的概率图模型</li></ul></li><li>书 籍：<ul><li>Koller, D. (2009). Probabilistic graphical models: Principles and techniques. The MIT Press.</li><li>Michael I. Jordan, An Introduction to Probabilistic Graphical Models</li></ul></li><li>教 程：<ul><li>Stefano ERmon et al., <a href="https://ermongroup.github.io/cs228/">Standford Univ., CS228- Probabilistic Graphical Models</a></li><li>Daphne Koller, Standford Univ., <a href="https://www.coursera.org/specializations/probabilistic-graphical-models">Probabilistic Graphical Models: Master a new way of reasoning and learning in complex domains</a></li></ul></li><li>Erik Sudderth et al. <a href="https://canvas.eee.uci.edu/courses/35909">California Univ. CS274B-Learning in Graphical Models</a></li><li>Eric P.Xing et al. <a href="http://www.cs.cmu.edu/~epxing/Class/10708-20/index.html">CMU. 10-708-Probabilistic Graphical Models</a> ， 课程的 Lecture 和 Notes 都非常全，其中高级主题部分可以纳入下一个层次</li></ul><hr><h3 id="高级层级"><a href="#高级层级" class="headerlink" title="高级层级"></a>高级层级</h3><ul><li>愿 景：掌握概率图和神经网络的结合和应用方法。</li><li>目 标：<ul><li>熟练使用概率图和计算图建立概率神经网络模型</li></ul></li></ul>    <style>    #refplus, #refplus li{         padding:0;        margin:0;        list-style:none;    }；    </style>    <script src="https://unpkg.com/@popperjs/core@2"></script>    <script src="https://unpkg.com/tippy.js@6"></script>    <script>    document.querySelectorAll(".refplus-num").forEach((ref) => {        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');        let refel = document.querySelector(refid);        let refnum = refel.dataset.num;        let ref_content = refel.innerText.replace(`[${refnum}]`,'');        tippy(ref, {            content: ref_content,        });    });    </script>    ]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 综述概览 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 统计建模 </tag>
            
            <tag> 统计学习 </tag>
            
            <tag> 统计推断 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>🔥  贝叶斯方法索引帖</title>
      <link href="/vll-pages/posts/7a2e65c2.html"/>
      <url>/vll-pages/posts/7a2e65c2.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h2 id="一、知识要点"><a href="#一、知识要点" class="headerlink" title="一、知识要点"></a>一、知识要点</h2><h3 id="1-1-贝叶斯思维与工作流"><a href="#1-1-贝叶斯思维与工作流" class="headerlink" title="1.1 贝叶斯思维与工作流"></a>1.1 贝叶斯思维与工作流</h3><p>推荐的几本基础入门书籍：</p><ul><li>Martin 2015 年的 <a href="https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html">《Bayesian Analysis with Python》</a> </li><li>Martin 2022 年的 <a href="https://xishansnow.github.io/BayesianModelingandComputationInPython/index.html">《Bayesian Modeling and Computation in Python》</a></li><li>Kruschke 2015 年的 <a href="https://sites.google.com/site/doingbayesiandataanalysis/">《Doing Bayesian Data Analysis》</a></li></ul><h3 id="1-2-主要的贝叶斯推断方法"><a href="#1-2-主要的贝叶斯推断方法" class="headerlink" title="1.2 主要的贝叶斯推断方法"></a>1.2 主要的贝叶斯推断方法</h3><p><strong>（1） 关于基础的推断方法</strong> </p><p><a href="https://engineering.purdue.edu/kak/Tutorials/Trinity.pdf">普渡大学机器人视觉实验室的自编教程</a>： 普渡大学的一篇入门教程，清晰地从贝叶斯定理出发，阐明了最大似然估计、最大后验估计、贝叶斯估计三者之间的关系，值得仔细品读。</p><p><strong>（2）关于似然函数</strong> </p><p>Reid 等 2010 年  <a href="db36c3ec.html">《似然与基于似然的推断》</a> ： 全面地对似然函数以及基于似然的推断方法进行了综述。由于似然函数时贝叶斯方法中的重要组成部分，因此掌握这方面的知识是必要的。文中提到了似然函数及其派生量、最大似然估计及其渐进性质、剖面最大似然估计、<a href="2c19b889.html">受限最大似然估计</a>、贝叶斯估计等方法，并给出了偏似然、伪似然、组合似然、准似然、经验似然等似然函数的常用变体。另外可参考  <a href="8f2c5e85.html">《似然及其在参数估计和模型比较中的引用》</a>。</p><p>Martin 等 2022 年的 <a href="226fd4ce.html">《近似贝叶斯计算简明教程》</a> ：当似然函数无法解析建模时，只能以某种方式对似然进行近似。近似贝叶斯计算就是解决此类问题的一类方法，通过设计一个可参数化的函数来近似复杂的真实似然，进而使贝叶斯推断可以继续进行。本文节选自 Martin《Python 中的贝叶斯建模和计算》一书的第八章。 </p><p><strong>（3）关于先验</strong> </p><p>涉及共轭先验、无信息先验等内容，待整理。</p><p><strong>（4）后验推断</strong> </p><p>精确推断，待整理。</p><p>Blei 的 <a href="7e8d23a9.html">《主要的贝叶斯近似推断方法》</a> ：根据贝叶斯领域大师 Blei 关于贝叶斯推断方法的讲座整理，主要涉及蒙特卡罗方法和变分推断方法，是一篇入门贝叶斯推断方法的好资料。 </p><p>各种蒙特卡罗方法的具体介绍参见  <a href="d7dd5b59.html">《蒙特卡洛推断方法索引贴》</a>：涉及基础采样、MCMC、HMC、NUTS、SMC、SGMCMC 等重要方法。</p><p>各种变分推断方法的具体介绍参见  <a href="5f5b1f29.html">《变分推断方法索引贴》</a>： 涉及平均场变分推断、随机变分推断、黑盒变分推断、自动变分推断等里程碑方法，另外 Zhang 2018 年的 <a href="46ae35f1.html">Advances in Variational Inference</a> 介绍了变分推断的核心思想，并概述了迄今为止最主要的变分推断方法，是不可多得的好综述。</p><h3 id="1-3-基于概率图的表示、推断与学习"><a href="#1-3-基于概率图的表示、推断与学习" class="headerlink" title="1.3 基于概率图的表示、推断与学习"></a>1.3 基于概率图的表示、推断与学习</h3><p>概率图模型是利用图形化方式表达、学习和推断概率模型的优雅手段，是掌握贝叶斯方法的基本技能。</p><p>对于概率图模型比较陌生的同学，可以阅读人门帖  <a href="70f04f5e.html">《概率图模型概览》</a></p><p>进一步学习，可参考 <a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a> 和 <a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a> 课程。</p><p>主要参考书籍包括:</p><ul><li>Koller, Daphne. Probabilistic Graphical Models : Principles and Techniques. Cambridge, Massachusetts: The MIT Press, 2009.</li><li>Jordan, Michael Irwin, ed. Learning in Graphical Models. Adaptive Computation and Machine Learning. Cambridge, Mass: MIT Press, 1999.</li></ul><h3 id="1-4-常见模型的贝叶斯建模与推断"><a href="#1-4-常见模型的贝叶斯建模与推断" class="headerlink" title="1.4 常见模型的贝叶斯建模与推断"></a>1.4 常见模型的贝叶斯建模与推断</h3><p>常见模型的概率图表示、推断及学习，内容较多待整理。</p><h3 id="1-5-贝叶斯优化问题"><a href="#1-5-贝叶斯优化问题" class="headerlink" title="1.5 贝叶斯优化问题"></a>1.5 贝叶斯优化问题</h3><p>为各类机器学习模型 <em>寻找最佳超参数</em> 本身是一种优化问题，与传统优化针对目标函数做出一些假设不同，在超参数调整过程中训练模型的代价可能非常高，而且如果某些超参数是离散型的，也不存在传统优化中的梯度概念，更困难的是，从超参数到性能的映射可能高度复杂且多峰的，局部的优化可能并不会产生可接受的结果。从目前来看，解决此类问题的主要方法是贝叶斯优化方法。</p><p>暂时对此主题探讨不够深入，请参考 《贝叶斯优化》 一书的 <a href="5d350677.html">《引言》章节</a>。</p><h3 id="1-6-概率模型与神经网络的结合"><a href="#1-6-概率模型与神经网络的结合" class="headerlink" title="1.6 概率模型与神经网络的结合"></a>1.6 概率模型与神经网络的结合</h3><p><strong>（1）神经网络的不确定性量化</strong> </p><p>回归与分类任务中中的不确定性量化，参见 Gawlikowski 等 2021 年的 <a href="926f8964.html">《深度神经网络中的不确定性综述文章》</a> ： 该文全面概述了神经网络中的不确定性估计，回顾了该领域的最新进展。论文首先对不确定性来源这一关键因素进行了全面介绍，并将其分为（可还原的） <strong>模型不确定性</strong> 和（不可还原的） <strong>数据不确定性</strong> 。介绍了基于<code>单一确定性神经网络</code>、<code>贝叶斯神经网络</code>、<code>神经网络集成</code>、<code>测试时数据增强</code> 四种不确定性的建模方法，讨论了这些领域的不同分支及最新发展。在实际应用方面，我们讨论了各种不确定性的度量和校准方法，并评述了现有基线和可用成果。</p><p><strong>（2） 神经网络实现高斯过程</strong> </p><ul><li><p>Garnelo2018 年的 <a href="22316bf9.html">《条件神经过程》</a>。首次提出了条件神经过程和神经过程的概念，采用元学习实现了深度学习灵活性和概率模型不确定性的结合，算是用神经网络实现随机过程的最早尝试。该方法的问题在于无法为相同的背景点生成不同的函数样本，即缺少不确定性建模能力。</p></li><li><p>Garnelo2018 年的 <a href="650d46e1.html">《神经过程》</a>，另参见 Kaspar 2018 年的一个<a href="c49f015e.html">博文</a>。为了提升不确定性建模能力，在条件神经过程基础上增加了一个类似于 VAE 瓶颈的隐变量 $z$，$z$ 的每一个随机样本都对应于随机过程的一个具体实现，这样就可以通过多个样本在解码器网络中的前向传递，生成目标处的预测分布。作者将整个模型命名为神经过程。该方法的问题在于单个预测输出虽然包含了不确定性（即测试点处的边缘分布），但不同点处的输出之间相互独立，无法对输出的相关性建模，这从某种程度上来说，失去了随机过程的优势。</p></li><li><p>Kim 等 2019 年提出的 <a href="">《注意力神经过程》</a> : 为了实现对输出相关性建模，在神经过程中引入注意力机制。</p></li><li><p>Bruinsma 等 2021 年的 <a href="6c68a4b9.html">《高斯神经过程》</a> : 采用函数 $KL$ 散度作为训练的代价函数，同时为了解决输出相关性建模问题，引入了一个用于学习核函数的神经网络，并将其与神经过程网络的结合体称为高斯神经过程。</p></li><li><p>Markou 等 2021 年的 <a href="e85cc444.html">《高效的高斯神经过程回归》</a>： 认为 Bruinsma 的高斯神经过程方法采用的 CNN 神经网络（ 本文作者称为为 FullConvGP）会限制输入的维度（$D = 1$ ），因此提出了对原始高斯神经过程方法的改进，并将新模型称为卷积高斯神经过程（ConvGP）。</p></li><li><p>Dutordoir 等 2022 年的 <a href="c0d702a8.html">《神经扩散过程》</a>：将扩散模型引入神经过程，</p></li><li><p>Nguyen 等 2022 年的  <a href="a099fc2c.html">《transformer 神经过程》</a>: transformer 神经过程。 </p></li><li><p>Bruinsma 等 2023 年的 <a href="5f9a5d71.html">《自回归条件神经过程》</a> : 还是为了提升相关性预测能力，但自回归条件神经过程并不对模型或训练过程进行任何修改，而是像 MCDropout、神经自回归密度估计器 (NADE) 等一样，改变了 CNP 在测试阶段的部署方式，使用概率链式法则来自回归地定义联合预测分布，而不是对每个目标点独立进行预测。</p></li></ul><p><strong>（3） 高斯过程模拟和解释神经网络</strong> </p><ul><li><p><strong>Neal</strong> 等 1994 年《无线宽神经网络的先验》: 单隐层无限宽神经网络等效于高斯过程。</p></li><li><p><strong>Williams</strong> 等 1997 年  [《Computing with infinite networks》]: 计算出了单隐层神经网络的解析高斯过程核，并给出了使用高斯过程先验进行回归的精确贝叶斯推断方法。</p></li><li><p><strong>Hazan</strong> 等 2015 年的《Steps toward deep kernel methods from infinite neural networks》：讨论了无限宽深度神经网络的等效核构建问题，但只限于两个非线性隐藏层。</p></li><li><p><strong>Daniely</strong> 等 2016 年的《Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity》： 将组合核方法扩展到神经网络，利用有向无环图构造了神经网络的 “具有相同非线性全连接拓扑的组合核”。</p></li><li><p><strong>Lee</strong> 等 2017 年的 <a href="9515d5ad.html">《神经网络高斯过程》</a>： 论证分析了深度的无线宽神经网络等效于高斯过程。</p></li><li><p><strong>Matthews</strong> 等 2018 年的 <a href="bddb7fac.html">《宽深度神经网络的高斯过程表现》</a> :</p></li><li><p><strong>Jacot</strong> 等 2018 年的 <a href="92799764.html">《神经切线核》</a>：剖析了神经网络训练期间的动态特性，并认为其训练动力学可以被视为一种神经正切核机制， 入门参见 Rajatvd 2019 年的 <a href="473bc1cc.html">《神经正切核入门》</a>， Novak 2019 年的 <a href="c1f3dd64.html">《神经切线核之 Python 实现》</a></p></li><li><p><strong>Domingos</strong> 等 2020 年的  <a href="80deb1b2.html">《梯度下降学得的模型都近似于一个核机》</a>：在神经正切核基础上，提出了路径核的概念，并认为所有通过梯度下降学得的模型，都可以被视为一种核机器。</p></li><li><p><strong>Li</strong>  等 2022 年的  <a href="">《神经网络的高斯过程代理模型》</a> 。将深度学习网络视为为内部过程不透明的复杂系统，用易于解释的高斯过程取代（或模仿）复杂神经网络系统的行为，这种高斯过程代理模型能够从神经网络的自然行为中凭经验学习高斯过程的核，这与 Lee 、Matthews Domingos 等从神经网络的极限情况下推导核具有显著不同。</p></li></ul><p><strong>（4）生成式神经网络</strong> </p><p>所有的生成模型几乎都与学习数据分布以及采样有关，也是概率模型与神经网络产出最多的领域。</p><ul><li><p>受限玻尔兹曼机：Hinton 等提出的 <a href="3f3fecac.html">受限玻尔兹曼机及深度置信网络</a></p></li><li><p>变分自编码器：Kingma 等 2014 年提出的变分自编码器，入门可以先阅读<a href="65612c13.html">《初始变分自编码器》</a>，进一步可以阅读原作者 2019 年撰写的 <a href="da72f251.html">《权威综述》</a>。 </p></li><li><p>自回归神经网络：参看 Murphy 2023 年的  <a href="d097769d.html">《Probabilistic Machine Learning: Advanced Topics》 第 22 章</a>。</p></li><li><p>归一化流：参见 Papamakarios 等人 2021 年的综述文章 <a href="1e755394.html"> 《Normalizing Flows for Probabilistic Modeling and Inference》 </a></p></li><li><p>基于能量的模型：参看 Murphy 2023 年的  <a href="d8f42e3d.html">《Probabilistic Machine Learning: Advanced Topics》 第 24 章</a></p></li><li><p>生成式对抗网络： 待整理</p></li><li><p>扩散模型：参见 Yang 等 2022 年的综述 <a href="c0fb1f85.html">《Diffusion Models: A Comprehensive Survey of Methods and Applications》</a> 以及 <a href="3b7358a6.html">扩散模型概览</a></p></li></ul><h2 id="二、知识体系的构建"><a href="#二、知识体系的构建" class="headerlink" title="二、知识体系的构建"></a>二、知识体系的构建</h2><p>贝叶斯统计方法以贝叶斯规则指导，已经基本形成了以概率图为形式化工具的一套相对完整的知识体系。贝叶斯新手建议由浅入深得学习。个人建议分为三个层次：</p><h3 id="入门层次"><a href="#入门层次" class="headerlink" title="入门层次"></a>入门层次</h3><ul><li>愿 景：掌握基础概念和入门级别的工作能力。</li><li>目 的：<ul><li>理解贝叶斯思维</li><li>学会基础的贝叶斯建模流程和工作方法</li><li>学会简单的概率编程</li><li>感性认识 MCMC、变分推断等统计推断方法</li><li>掌握线性回归、多元线性回归、广义线性回归等基础回归模型</li><li>理解高斯混合模型、狄利克雷过程、连续混合模型等隐变量模型</li><li>理解高斯过程等非参数贝叶斯方法</li></ul></li><li>书 籍：<ul><li>Osvaldo Martin, [Bayesian Analysis with Python(2nd)](<a href="https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html">https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html</a></li><li>Osvaldo Martin,<a href="https://xishansnow.github.io/BayesianModelingandComputationInPython/index.html">Bayesian Modeling and Computation in Python</a></li><li>McElreath, R. . [Statistical Rethinking (2nd )]. <a href="https://doi.org/10.1201/9781315372495">https://doi.org/10.1201/9781315372495</a></li><li>Kruschke, <a href="https://sites.google.com/site/doingbayesiandataanalysis/">Doing Bayesian Data Analysis</a></li><li>Andrew Gelman et al., <a href="https://avehtari.github.io/BDA_course_Aalto/index.html">Bayesian Data Analysis(3rd)</a></li></ul></li><li>教 程：<ul><li>Herbert Lee, <a href="https://www.coursera.org/learn/bayesian-statistics?specialization=bayesian-statistics">Bayesian Statistics: From Concept to Data Analysis</a></li><li>Matthew Helner, <a href="https://www.coursera.org/learn/mcmc-bayesian-statistics?specialization=bayesian-statistics">Bayesian Statistics: Techniques and Models</a></li><li>McElreath R. et al., [Statistical Rethinking 2022] <a href="https://github.com/rmcelreath/stat_rethinking_2022">https://github.com/rmcelreath/stat_rethinking_2022</a></li><li>Andrew Gelman et al., <a href="https://avehtari.github.io/BDA_course_Aalto/index.html">Bayesian Data Analysis Course</a></li><li>Padhraic Smyth et al., <a href="https://www.ics.uci.edu/~smyth/courses/cs274/">California Unvi., CS274A-Probabilistic Learning:Theory and Algorithms</a></li></ul></li></ul><hr><h3 id="中级层次"><a href="#中级层次" class="headerlink" title="中级层次"></a>中级层次</h3><ul><li>愿 景：熟练掌握概率图模型，并利用概率图模型进行建模、学习和推断。</li><li>目 标：<ul><li>理解什么是概率图模型</li><li>掌握贝叶斯网络、马尔可夫随机场两种表示方法</li><li>掌握变量消除、消息传递等概率图推断的传统方法</li><li>掌握 MCMC、变分推断等近似推断基本原理和方法</li><li>掌握完全可观测模型、部分可观测模型的学习原理和方法</li><li>掌握高斯过程、狄利克雷过程等非参数模型的概率图方法</li><li>掌握因子分析、主组分分析、隐马尔可夫、状态空间等常用的概率图模型</li></ul></li><li>书 籍：<ul><li>Koller, D. (2009). Probabilistic graphical models: Principles and techniques. The MIT Press.</li><li>Michael I. Jordan, An Introduction to Probabilistic Graphical Models</li></ul></li><li>教 程：<ul><li>Stefano ERmon et al., <a href="https://ermongroup.github.io/cs228/">Standford Univ., CS228- Probabilistic Graphical Models</a></li><li>Daphne Koller, Standford Univ., <a href="https://www.coursera.org/specializations/probabilistic-graphical-models">Probabilistic Graphical Models: Master a new way of reasoning and learning in complex domains</a></li></ul></li><li>Erik Sudderth et al. <a href="https://canvas.eee.uci.edu/courses/35909">California Univ. CS274B-Learning in Graphical Models</a></li><li>Eric P.Xing et al. <a href="http://www.cs.cmu.edu/~epxing/Class/10708-20/index.html">CMU. 10-708-Probabilistic Graphical Models</a> ， 课程的 Lecture 和 Notes 都非常全，其中高级主题部分可以纳入下一个层次</li></ul><hr><h3 id="高级层级"><a href="#高级层级" class="headerlink" title="高级层级"></a>高级层级</h3><ul><li>愿 景：掌握概率图和神经网络的结合和应用方法。</li><li>目 标：<ul><li>熟练使用概率图和计算图建立概率神经网络模型</li></ul></li></ul>    <style>    #refplus, #refplus li{         padding:0;        margin:0;        list-style:none;    }；    </style>    <script src="https://unpkg.com/@popperjs/core@2"></script>    <script src="https://unpkg.com/tippy.js@6"></script>    <script>    document.querySelectorAll(".refplus-num").forEach((ref) => {        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');        let refel = document.querySelector(refid);        let refnum = refel.dataset.num;        let ref_content = refel.innerText.replace(`[${refnum}]`,'');        tippy(ref, {            content: ref_content,        });    });    </script>    ]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 综述概览 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 统计建模 </tag>
            
            <tag> 统计学习 </tag>
            
            <tag> 统计推断 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>🔥  贝叶斯方法索引帖</title>
      <link href="/vll-pages/posts/7a2e65c2.html"/>
      <url>/vll-pages/posts/7a2e65c2.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h2 id="一、知识要点"><a href="#一、知识要点" class="headerlink" title="一、知识要点"></a>一、知识要点</h2><h3 id="1-1-贝叶斯思维与工作流"><a href="#1-1-贝叶斯思维与工作流" class="headerlink" title="1.1 贝叶斯思维与工作流"></a>1.1 贝叶斯思维与工作流</h3><p>推荐的几本基础入门书籍：</p><ul><li>Martin 2015 年的 <a href="https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html">《Bayesian Analysis with Python》</a> </li><li>Martin 2022 年的 <a href="https://xishansnow.github.io/BayesianModelingandComputationInPython/index.html">《Bayesian Modeling and Computation in Python》</a></li><li>Kruschke 2015 年的 <a href="https://sites.google.com/site/doingbayesiandataanalysis/">《Doing Bayesian Data Analysis》</a></li></ul><h3 id="1-2-主要的贝叶斯推断方法"><a href="#1-2-主要的贝叶斯推断方法" class="headerlink" title="1.2 主要的贝叶斯推断方法"></a>1.2 主要的贝叶斯推断方法</h3><p><strong>（1） 关于基础的推断方法</strong> </p><p><a href="https://engineering.purdue.edu/kak/Tutorials/Trinity.pdf">普渡大学机器人视觉实验室的自编教程</a>： 普渡大学的一篇入门教程，清晰地从贝叶斯定理出发，阐明了最大似然估计、最大后验估计、贝叶斯估计三者之间的关系，值得仔细品读。</p><p><strong>（2）关于似然函数</strong> </p><p>Reid 等 2010 年  <a href="db36c3ec.html">《似然与基于似然的推断》</a> ： 全面地对似然函数以及基于似然的推断方法进行了综述。由于似然函数时贝叶斯方法中的重要组成部分，因此掌握这方面的知识是必要的。文中提到了似然函数及其派生量、最大似然估计及其渐进性质、剖面最大似然估计、<a href="2c19b889.html">受限最大似然估计</a>、贝叶斯估计等方法，并给出了偏似然、伪似然、组合似然、准似然、经验似然等似然函数的常用变体。另外可参考  <a href="8f2c5e85.html">《似然及其在参数估计和模型比较中的引用》</a>。</p><p>Martin 等 2022 年的 <a href="226fd4ce.html">《近似贝叶斯计算简明教程》</a> ：当似然函数无法解析建模时，只能以某种方式对似然进行近似。近似贝叶斯计算就是解决此类问题的一类方法，通过设计一个可参数化的函数来近似复杂的真实似然，进而使贝叶斯推断可以继续进行。本文节选自 Martin《Python 中的贝叶斯建模和计算》一书的第八章。 </p><p><strong>（3）关于先验</strong> </p><p>涉及共轭先验、无信息先验等内容，待整理。</p><p><strong>（4）后验推断</strong> </p><p>精确推断，待整理。</p><p>Blei 的 <a href="7e8d23a9.html">《主要的贝叶斯近似推断方法》</a> ：根据贝叶斯领域大师 Blei 关于贝叶斯推断方法的讲座整理，主要涉及蒙特卡罗方法和变分推断方法，是一篇入门贝叶斯推断方法的好资料。 </p><p>各种蒙特卡罗方法的具体介绍参见  <a href="d7dd5b59.html">《蒙特卡洛推断方法索引贴》</a>：涉及基础采样、MCMC、HMC、NUTS、SMC、SGMCMC 等重要方法。</p><p>各种变分推断方法的具体介绍参见  <a href="5f5b1f29.html">《变分推断方法索引贴》</a>： 涉及平均场变分推断、随机变分推断、黑盒变分推断、自动变分推断等里程碑方法，另外 Zhang 2018 年的 <a href="46ae35f1.html">Advances in Variational Inference</a> 介绍了变分推断的核心思想，并概述了迄今为止最主要的变分推断方法，是不可多得的好综述。</p><h3 id="1-3-基于概率图的表示、推断与学习"><a href="#1-3-基于概率图的表示、推断与学习" class="headerlink" title="1.3 基于概率图的表示、推断与学习"></a>1.3 基于概率图的表示、推断与学习</h3><p>概率图模型是利用图形化方式表达、学习和推断概率模型的优雅手段，是掌握贝叶斯方法的基本技能。</p><p>对于概率图模型比较陌生的同学，可以阅读人门帖  <a href="70f04f5e.html">《概率图模型概览》</a></p><p>进一步学习，可参考 <a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a> 和 <a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a> 课程。</p><p>主要参考书籍包括:</p><ul><li>Koller, Daphne. Probabilistic Graphical Models : Principles and Techniques. Cambridge, Massachusetts: The MIT Press, 2009.</li><li>Jordan, Michael Irwin, ed. Learning in Graphical Models. Adaptive Computation and Machine Learning. Cambridge, Mass: MIT Press, 1999.</li></ul><h3 id="1-4-常见模型的贝叶斯建模与推断"><a href="#1-4-常见模型的贝叶斯建模与推断" class="headerlink" title="1.4 常见模型的贝叶斯建模与推断"></a>1.4 常见模型的贝叶斯建模与推断</h3><p>常见模型的概率图表示、推断及学习，内容较多待整理。</p><h3 id="1-5-贝叶斯优化问题"><a href="#1-5-贝叶斯优化问题" class="headerlink" title="1.5 贝叶斯优化问题"></a>1.5 贝叶斯优化问题</h3><p>为各类机器学习模型 <em>寻找最佳超参数</em> 本身是一种优化问题，与传统优化针对目标函数做出一些假设不同，在超参数调整过程中训练模型的代价可能非常高，而且如果某些超参数是离散型的，也不存在传统优化中的梯度概念，更困难的是，从超参数到性能的映射可能高度复杂且多峰的，局部的优化可能并不会产生可接受的结果。从目前来看，解决此类问题的主要方法是贝叶斯优化方法。</p><p>暂时对此主题探讨不够深入，请参考 《贝叶斯优化》 一书的 <a href="5d350677.html">《引言》章节</a>。</p><h3 id="1-6-概率模型与神经网络的结合"><a href="#1-6-概率模型与神经网络的结合" class="headerlink" title="1.6 概率模型与神经网络的结合"></a>1.6 概率模型与神经网络的结合</h3><p><strong>（1）神经网络的不确定性量化</strong> </p><p>回归与分类任务中中的不确定性量化，参见 Gawlikowski 等 2021 年的 <a href="926f8964.html">《深度神经网络中的不确定性综述文章》</a> ： 该文全面概述了神经网络中的不确定性估计，回顾了该领域的最新进展。论文首先对不确定性来源这一关键因素进行了全面介绍，并将其分为（可还原的） <strong>模型不确定性</strong> 和（不可还原的） <strong>数据不确定性</strong> 。介绍了基于<code>单一确定性神经网络</code>、<code>贝叶斯神经网络</code>、<code>神经网络集成</code>、<code>测试时数据增强</code> 四种不确定性的建模方法，讨论了这些领域的不同分支及最新发展。在实际应用方面，我们讨论了各种不确定性的度量和校准方法，并评述了现有基线和可用成果。</p><p><strong>（2） 神经网络实现高斯过程</strong> </p><ul><li><p>Garnelo2018 年的 <a href="22316bf9.html">《条件神经过程》</a>。首次提出了条件神经过程和神经过程的概念，采用元学习实现了深度学习灵活性和概率模型不确定性的结合，算是用神经网络实现随机过程的最早尝试。该方法的问题在于无法为相同的背景点生成不同的函数样本，即缺少不确定性建模能力。</p></li><li><p>Garnelo2018 年的 <a href="650d46e1.html">《神经过程》</a>，另参见 Kaspar 2018 年的一个<a href="c49f015e.html">博文</a>。为了提升不确定性建模能力，在条件神经过程基础上增加了一个类似于 VAE 瓶颈的隐变量 $z$，$z$ 的每一个随机样本都对应于随机过程的一个具体实现，这样就可以通过多个样本在解码器网络中的前向传递，生成目标处的预测分布。作者将整个模型命名为神经过程。该方法的问题在于单个预测输出虽然包含了不确定性（即测试点处的边缘分布），但不同点处的输出之间相互独立，无法对输出的相关性建模，这从某种程度上来说，失去了随机过程的优势。</p></li><li><p>Kim 等 2019 年提出的 <a href="">《注意力神经过程》</a> : 为了实现对输出相关性建模，在神经过程中引入注意力机制。</p></li><li><p>Bruinsma 等 2021 年的 <a href="6c68a4b9.html">《高斯神经过程》</a> : 采用函数 $KL$ 散度作为训练的代价函数，同时为了解决输出相关性建模问题，引入了一个用于学习核函数的神经网络，并将其与神经过程网络的结合体称为高斯神经过程。</p></li><li><p>Markou 等 2021 年的 <a href="e85cc444.html">《高效的高斯神经过程回归》</a>： 认为 Bruinsma 的高斯神经过程方法采用的 CNN 神经网络（ 本文作者称为为 FullConvGP）会限制输入的维度（$D = 1$ ），因此提出了对原始高斯神经过程方法的改进，并将新模型称为卷积高斯神经过程（ConvGP）。</p></li><li><p>Dutordoir 等 2022 年的 <a href="c0d702a8.html">《神经扩散过程》</a>：将扩散模型引入神经过程，</p></li><li><p>Nguyen 等 2022 年的  <a href="a099fc2c.html">《transformer 神经过程》</a>: transformer 神经过程。 </p></li><li><p>Bruinsma 等 2023 年的 <a href="5f9a5d71.html">《自回归条件神经过程》</a> : 还是为了提升相关性预测能力，但自回归条件神经过程并不对模型或训练过程进行任何修改，而是像 MCDropout、神经自回归密度估计器 (NADE) 等一样，改变了 CNP 在测试阶段的部署方式，使用概率链式法则来自回归地定义联合预测分布，而不是对每个目标点独立进行预测。</p></li></ul><p><strong>（3） 高斯过程模拟和解释神经网络</strong> </p><ul><li><p><strong>Neal</strong> 等 1994 年《无线宽神经网络的先验》: 单隐层无限宽神经网络等效于高斯过程。</p></li><li><p><strong>Williams</strong> 等 1997 年  [《Computing with infinite networks》]: 计算出了单隐层神经网络的解析高斯过程核，并给出了使用高斯过程先验进行回归的精确贝叶斯推断方法。</p></li><li><p><strong>Hazan</strong> 等 2015 年的《Steps toward deep kernel methods from infinite neural networks》：讨论了无限宽深度神经网络的等效核构建问题，但只限于两个非线性隐藏层。</p></li><li><p><strong>Daniely</strong> 等 2016 年的《Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity》： 将组合核方法扩展到神经网络，利用有向无环图构造了神经网络的 “具有相同非线性全连接拓扑的组合核”。</p></li><li><p><strong>Lee</strong> 等 2017 年的 <a href="9515d5ad.html">《神经网络高斯过程》</a>： 论证分析了深度的无线宽神经网络等效于高斯过程。</p></li><li><p><strong>Matthews</strong> 等 2018 年的 <a href="bddb7fac.html">《宽深度神经网络的高斯过程表现》</a> :</p></li><li><p><strong>Jacot</strong> 等 2018 年的 <a href="92799764.html">《神经切线核》</a>：剖析了神经网络训练期间的动态特性，并认为其训练动力学可以被视为一种神经正切核机制， 入门参见 Rajatvd 2019 年的 <a href="473bc1cc.html">《神经正切核入门》</a>， Novak 2019 年的 <a href="c1f3dd64.html">《神经切线核之 Python 实现》</a></p></li><li><p><strong>Domingos</strong> 等 2020 年的  <a href="80deb1b2.html">《梯度下降学得的模型都近似于一个核机》</a>：在神经正切核基础上，提出了路径核的概念，并认为所有通过梯度下降学得的模型，都可以被视为一种核机器。</p></li><li><p><strong>Li</strong>  等 2022 年的  <a href="">《神经网络的高斯过程代理模型》</a> 。将深度学习网络视为为内部过程不透明的复杂系统，用易于解释的高斯过程取代（或模仿）复杂神经网络系统的行为，这种高斯过程代理模型能够从神经网络的自然行为中凭经验学习高斯过程的核，这与 Lee 、Matthews Domingos 等从神经网络的极限情况下推导核具有显著不同。</p></li></ul><p><strong>（4）生成式神经网络</strong> </p><p>所有的生成模型几乎都与学习数据分布以及采样有关，也是概率模型与神经网络产出最多的领域。</p><ul><li><p>受限玻尔兹曼机：Hinton 等提出的 <a href="3f3fecac.html">受限玻尔兹曼机及深度置信网络</a></p></li><li><p>变分自编码器：Kingma 等 2014 年提出的变分自编码器，入门可以先阅读<a href="65612c13.html">《初始变分自编码器》</a>，进一步可以阅读原作者 2019 年撰写的 <a href="da72f251.html">《权威综述》</a>。 </p></li><li><p>自回归神经网络：参看 Murphy 2023 年的  <a href="d097769d.html">《Probabilistic Machine Learning: Advanced Topics》 第 22 章</a>。</p></li><li><p>归一化流：参见 Papamakarios 等人 2021 年的综述文章 <a href="1e755394.html"> 《Normalizing Flows for Probabilistic Modeling and Inference》 </a></p></li><li><p>基于能量的模型：参看 Murphy 2023 年的  <a href="d8f42e3d.html">《Probabilistic Machine Learning: Advanced Topics》 第 24 章</a></p></li><li><p>生成式对抗网络： 待整理</p></li><li><p>扩散模型：参见 Yang 等 2022 年的综述 <a href="c0fb1f85.html">《Diffusion Models: A Comprehensive Survey of Methods and Applications》</a> 以及 <a href="3b7358a6.html">扩散模型概览</a></p></li></ul><h2 id="二、知识体系的构建"><a href="#二、知识体系的构建" class="headerlink" title="二、知识体系的构建"></a>二、知识体系的构建</h2><p>贝叶斯统计方法以贝叶斯规则指导，已经基本形成了以概率图为形式化工具的一套相对完整的知识体系。贝叶斯新手建议由浅入深得学习。个人建议分为三个层次：</p><h3 id="入门层次"><a href="#入门层次" class="headerlink" title="入门层次"></a>入门层次</h3><ul><li>愿 景：掌握基础概念和入门级别的工作能力。</li><li>目 的：<ul><li>理解贝叶斯思维</li><li>学会基础的贝叶斯建模流程和工作方法</li><li>学会简单的概率编程</li><li>感性认识 MCMC、变分推断等统计推断方法</li><li>掌握线性回归、多元线性回归、广义线性回归等基础回归模型</li><li>理解高斯混合模型、狄利克雷过程、连续混合模型等隐变量模型</li><li>理解高斯过程等非参数贝叶斯方法</li></ul></li><li>书 籍：<ul><li>Osvaldo Martin, [Bayesian Analysis with Python(2nd)](<a href="https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html">https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html</a></li><li>Osvaldo Martin,<a href="https://xishansnow.github.io/BayesianModelingandComputationInPython/index.html">Bayesian Modeling and Computation in Python</a></li><li>McElreath, R. . [Statistical Rethinking (2nd )]. <a href="https://doi.org/10.1201/9781315372495">https://doi.org/10.1201/9781315372495</a></li><li>Kruschke, <a href="https://sites.google.com/site/doingbayesiandataanalysis/">Doing Bayesian Data Analysis</a></li><li>Andrew Gelman et al., <a href="https://avehtari.github.io/BDA_course_Aalto/index.html">Bayesian Data Analysis(3rd)</a></li></ul></li><li>教 程：<ul><li>Herbert Lee, <a href="https://www.coursera.org/learn/bayesian-statistics?specialization=bayesian-statistics">Bayesian Statistics: From Concept to Data Analysis</a></li><li>Matthew Helner, <a href="https://www.coursera.org/learn/mcmc-bayesian-statistics?specialization=bayesian-statistics">Bayesian Statistics: Techniques and Models</a></li><li>McElreath R. et al., [Statistical Rethinking 2022] <a href="https://github.com/rmcelreath/stat_rethinking_2022">https://github.com/rmcelreath/stat_rethinking_2022</a></li><li>Andrew Gelman et al., <a href="https://avehtari.github.io/BDA_course_Aalto/index.html">Bayesian Data Analysis Course</a></li><li>Padhraic Smyth et al., <a href="https://www.ics.uci.edu/~smyth/courses/cs274/">California Unvi., CS274A-Probabilistic Learning:Theory and Algorithms</a></li></ul></li></ul><hr><h3 id="中级层次"><a href="#中级层次" class="headerlink" title="中级层次"></a>中级层次</h3><ul><li>愿 景：熟练掌握概率图模型，并利用概率图模型进行建模、学习和推断。</li><li>目 标：<ul><li>理解什么是概率图模型</li><li>掌握贝叶斯网络、马尔可夫随机场两种表示方法</li><li>掌握变量消除、消息传递等概率图推断的传统方法</li><li>掌握 MCMC、变分推断等近似推断基本原理和方法</li><li>掌握完全可观测模型、部分可观测模型的学习原理和方法</li><li>掌握高斯过程、狄利克雷过程等非参数模型的概率图方法</li><li>掌握因子分析、主组分分析、隐马尔可夫、状态空间等常用的概率图模型</li></ul></li><li>书 籍：<ul><li>Koller, D. (2009). Probabilistic graphical models: Principles and techniques. The MIT Press.</li><li>Michael I. Jordan, An Introduction to Probabilistic Graphical Models</li></ul></li><li>教 程：<ul><li>Stefano ERmon et al., <a href="https://ermongroup.github.io/cs228/">Standford Univ., CS228- Probabilistic Graphical Models</a></li><li>Daphne Koller, Standford Univ., <a href="https://www.coursera.org/specializations/probabilistic-graphical-models">Probabilistic Graphical Models: Master a new way of reasoning and learning in complex domains</a></li></ul></li><li>Erik Sudderth et al. <a href="https://canvas.eee.uci.edu/courses/35909">California Univ. CS274B-Learning in Graphical Models</a></li><li>Eric P.Xing et al. <a href="http://www.cs.cmu.edu/~epxing/Class/10708-20/index.html">CMU. 10-708-Probabilistic Graphical Models</a> ， 课程的 Lecture 和 Notes 都非常全，其中高级主题部分可以纳入下一个层次</li></ul><hr><h3 id="高级层级"><a href="#高级层级" class="headerlink" title="高级层级"></a>高级层级</h3><ul><li>愿 景：掌握概率图和神经网络的结合和应用方法。</li><li>目 标：<ul><li>熟练使用概率图和计算图建立概率神经网络模型</li></ul></li></ul>    <style>    #refplus, #refplus li{         padding:0;        margin:0;        list-style:none;    }；    </style>    <script src="https://unpkg.com/@popperjs/core@2"></script>    <script src="https://unpkg.com/tippy.js@6"></script>    <script>    document.querySelectorAll(".refplus-num").forEach((ref) => {        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');        let refel = document.querySelector(refid);        let refnum = refel.dataset.num;        let ref_content = refel.innerText.replace(`[${refnum}]`,'');        tippy(ref, {            content: ref_content,        });    });    </script>    ]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 综述概览 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 统计建模 </tag>
            
            <tag> 统计学习 </tag>
            
            <tag> 统计推断 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>🔥  贝叶斯方法索引帖</title>
      <link href="/vll-pages/posts/7a2e65c2.html"/>
      <url>/vll-pages/posts/7a2e65c2.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h2 id="一、知识要点"><a href="#一、知识要点" class="headerlink" title="一、知识要点"></a>一、知识要点</h2><h3 id="1-1-贝叶斯思维与工作流"><a href="#1-1-贝叶斯思维与工作流" class="headerlink" title="1.1 贝叶斯思维与工作流"></a>1.1 贝叶斯思维与工作流</h3><p>推荐的几本基础入门书籍：</p><ul><li>Martin 2015 年的 <a href="https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html">《Bayesian Analysis with Python》</a> </li><li>Martin 2022 年的 <a href="https://xishansnow.github.io/BayesianModelingandComputationInPython/index.html">《Bayesian Modeling and Computation in Python》</a></li><li>Kruschke 2015 年的 <a href="https://sites.google.com/site/doingbayesiandataanalysis/">《Doing Bayesian Data Analysis》</a></li></ul><h3 id="1-2-主要的贝叶斯推断方法"><a href="#1-2-主要的贝叶斯推断方法" class="headerlink" title="1.2 主要的贝叶斯推断方法"></a>1.2 主要的贝叶斯推断方法</h3><p><strong>（1） 关于基础的推断方法</strong> </p><p><a href="https://engineering.purdue.edu/kak/Tutorials/Trinity.pdf">普渡大学机器人视觉实验室的自编教程</a>： 普渡大学的一篇入门教程，清晰地从贝叶斯定理出发，阐明了最大似然估计、最大后验估计、贝叶斯估计三者之间的关系，值得仔细品读。</p><p><strong>（2）关于似然函数</strong> </p><p>Reid 等 2010 年  <a href="db36c3ec.html">《似然与基于似然的推断》</a> ： 全面地对似然函数以及基于似然的推断方法进行了综述。由于似然函数时贝叶斯方法中的重要组成部分，因此掌握这方面的知识是必要的。文中提到了似然函数及其派生量、最大似然估计及其渐进性质、剖面最大似然估计、<a href="2c19b889.html">受限最大似然估计</a>、贝叶斯估计等方法，并给出了偏似然、伪似然、组合似然、准似然、经验似然等似然函数的常用变体。另外可参考  <a href="8f2c5e85.html">《似然及其在参数估计和模型比较中的引用》</a>。</p><p>Martin 等 2022 年的 <a href="226fd4ce.html">《近似贝叶斯计算简明教程》</a> ：当似然函数无法解析建模时，只能以某种方式对似然进行近似。近似贝叶斯计算就是解决此类问题的一类方法，通过设计一个可参数化的函数来近似复杂的真实似然，进而使贝叶斯推断可以继续进行。本文节选自 Martin《Python 中的贝叶斯建模和计算》一书的第八章。 </p><p><strong>（3）关于先验</strong> </p><p>涉及共轭先验、无信息先验等内容，待整理。</p><p><strong>（4）后验推断</strong> </p><p>精确推断，待整理。</p><p>Blei 的 <a href="7e8d23a9.html">《主要的贝叶斯近似推断方法》</a> ：根据贝叶斯领域大师 Blei 关于贝叶斯推断方法的讲座整理，主要涉及蒙特卡罗方法和变分推断方法，是一篇入门贝叶斯推断方法的好资料。 </p><p>各种蒙特卡罗方法的具体介绍参见  <a href="d7dd5b59.html">《蒙特卡洛推断方法索引贴》</a>：涉及基础采样、MCMC、HMC、NUTS、SMC、SGMCMC 等重要方法。</p><p>各种变分推断方法的具体介绍参见  <a href="5f5b1f29.html">《变分推断方法索引贴》</a>： 涉及平均场变分推断、随机变分推断、黑盒变分推断、自动变分推断等里程碑方法，另外 Zhang 2018 年的 <a href="46ae35f1.html">Advances in Variational Inference</a> 介绍了变分推断的核心思想，并概述了迄今为止最主要的变分推断方法，是不可多得的好综述。</p><h3 id="1-3-基于概率图的表示、推断与学习"><a href="#1-3-基于概率图的表示、推断与学习" class="headerlink" title="1.3 基于概率图的表示、推断与学习"></a>1.3 基于概率图的表示、推断与学习</h3><p>概率图模型是利用图形化方式表达、学习和推断概率模型的优雅手段，是掌握贝叶斯方法的基本技能。</p><p>对于概率图模型比较陌生的同学，可以阅读人门帖  <a href="70f04f5e.html">《概率图模型概览》</a></p><p>进一步学习，可参考 <a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a> 和 <a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a> 课程。</p><p>主要参考书籍包括:</p><ul><li>Koller, Daphne. Probabilistic Graphical Models : Principles and Techniques. Cambridge, Massachusetts: The MIT Press, 2009.</li><li>Jordan, Michael Irwin, ed. Learning in Graphical Models. Adaptive Computation and Machine Learning. Cambridge, Mass: MIT Press, 1999.</li></ul><h3 id="1-4-常见模型的贝叶斯建模与推断"><a href="#1-4-常见模型的贝叶斯建模与推断" class="headerlink" title="1.4 常见模型的贝叶斯建模与推断"></a>1.4 常见模型的贝叶斯建模与推断</h3><p>常见模型的概率图表示、推断及学习，内容较多待整理。</p><h3 id="1-5-贝叶斯优化问题"><a href="#1-5-贝叶斯优化问题" class="headerlink" title="1.5 贝叶斯优化问题"></a>1.5 贝叶斯优化问题</h3><p>为各类机器学习模型 <em>寻找最佳超参数</em> 本身是一种优化问题，与传统优化针对目标函数做出一些假设不同，在超参数调整过程中训练模型的代价可能非常高，而且如果某些超参数是离散型的，也不存在传统优化中的梯度概念，更困难的是，从超参数到性能的映射可能高度复杂且多峰的，局部的优化可能并不会产生可接受的结果。从目前来看，解决此类问题的主要方法是贝叶斯优化方法。</p><p>暂时对此主题探讨不够深入，请参考 《贝叶斯优化》 一书的 <a href="5d350677.html">《引言》章节</a>。</p><h3 id="1-6-概率模型与神经网络的结合"><a href="#1-6-概率模型与神经网络的结合" class="headerlink" title="1.6 概率模型与神经网络的结合"></a>1.6 概率模型与神经网络的结合</h3><p><strong>（1）神经网络的不确定性量化</strong> </p><p>回归与分类任务中中的不确定性量化，参见 Gawlikowski 等 2021 年的 <a href="926f8964.html">《深度神经网络中的不确定性综述文章》</a> ： 该文全面概述了神经网络中的不确定性估计，回顾了该领域的最新进展。论文首先对不确定性来源这一关键因素进行了全面介绍，并将其分为（可还原的） <strong>模型不确定性</strong> 和（不可还原的） <strong>数据不确定性</strong> 。介绍了基于<code>单一确定性神经网络</code>、<code>贝叶斯神经网络</code>、<code>神经网络集成</code>、<code>测试时数据增强</code> 四种不确定性的建模方法，讨论了这些领域的不同分支及最新发展。在实际应用方面，我们讨论了各种不确定性的度量和校准方法，并评述了现有基线和可用成果。</p><p><strong>（2） 神经网络实现高斯过程</strong> </p><ul><li><p>Garnelo2018 年的 <a href="22316bf9.html">《条件神经过程》</a>。首次提出了条件神经过程和神经过程的概念，采用元学习实现了深度学习灵活性和概率模型不确定性的结合，算是用神经网络实现随机过程的最早尝试。该方法的问题在于无法为相同的背景点生成不同的函数样本，即缺少不确定性建模能力。</p></li><li><p>Garnelo2018 年的 <a href="650d46e1.html">《神经过程》</a>，另参见 Kaspar 2018 年的一个<a href="c49f015e.html">博文</a>。为了提升不确定性建模能力，在条件神经过程基础上增加了一个类似于 VAE 瓶颈的隐变量 $z$，$z$ 的每一个随机样本都对应于随机过程的一个具体实现，这样就可以通过多个样本在解码器网络中的前向传递，生成目标处的预测分布。作者将整个模型命名为神经过程。该方法的问题在于单个预测输出虽然包含了不确定性（即测试点处的边缘分布），但不同点处的输出之间相互独立，无法对输出的相关性建模，这从某种程度上来说，失去了随机过程的优势。</p></li><li><p>Kim 等 2019 年提出的 <a href="">《注意力神经过程》</a> : 为了实现对输出相关性建模，在神经过程中引入注意力机制。</p></li><li><p>Bruinsma 等 2021 年的 <a href="6c68a4b9.html">《高斯神经过程》</a> : 采用函数 $KL$ 散度作为训练的代价函数，同时为了解决输出相关性建模问题，引入了一个用于学习核函数的神经网络，并将其与神经过程网络的结合体称为高斯神经过程。</p></li><li><p>Markou 等 2021 年的 <a href="e85cc444.html">《高效的高斯神经过程回归》</a>： 认为 Bruinsma 的高斯神经过程方法采用的 CNN 神经网络（ 本文作者称为为 FullConvGP）会限制输入的维度（$D = 1$ ），因此提出了对原始高斯神经过程方法的改进，并将新模型称为卷积高斯神经过程（ConvGP）。</p></li><li><p>Dutordoir 等 2022 年的 <a href="c0d702a8.html">《神经扩散过程》</a>：将扩散模型引入神经过程，</p></li><li><p>Nguyen 等 2022 年的  <a href="a099fc2c.html">《transformer 神经过程》</a>: transformer 神经过程。 </p></li><li><p>Bruinsma 等 2023 年的 <a href="5f9a5d71.html">《自回归条件神经过程》</a> : 还是为了提升相关性预测能力，但自回归条件神经过程并不对模型或训练过程进行任何修改，而是像 MCDropout、神经自回归密度估计器 (NADE) 等一样，改变了 CNP 在测试阶段的部署方式，使用概率链式法则来自回归地定义联合预测分布，而不是对每个目标点独立进行预测。</p></li></ul><p><strong>（3） 高斯过程模拟和解释神经网络</strong> </p><ul><li><p><strong>Neal</strong> 等 1994 年《无线宽神经网络的先验》: 单隐层无限宽神经网络等效于高斯过程。</p></li><li><p><strong>Williams</strong> 等 1997 年  [《Computing with infinite networks》]: 计算出了单隐层神经网络的解析高斯过程核，并给出了使用高斯过程先验进行回归的精确贝叶斯推断方法。</p></li><li><p><strong>Hazan</strong> 等 2015 年的《Steps toward deep kernel methods from infinite neural networks》：讨论了无限宽深度神经网络的等效核构建问题，但只限于两个非线性隐藏层。</p></li><li><p><strong>Daniely</strong> 等 2016 年的《Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity》： 将组合核方法扩展到神经网络，利用有向无环图构造了神经网络的 “具有相同非线性全连接拓扑的组合核”。</p></li><li><p><strong>Lee</strong> 等 2017 年的 <a href="9515d5ad.html">《神经网络高斯过程》</a>： 论证分析了深度的无线宽神经网络等效于高斯过程。</p></li><li><p><strong>Matthews</strong> 等 2018 年的 <a href="bddb7fac.html">《宽深度神经网络的高斯过程表现》</a> :</p></li><li><p><strong>Jacot</strong> 等 2018 年的 <a href="92799764.html">《神经切线核》</a>：剖析了神经网络训练期间的动态特性，并认为其训练动力学可以被视为一种神经正切核机制， 入门参见 Rajatvd 2019 年的 <a href="473bc1cc.html">《神经正切核入门》</a>， Novak 2019 年的 <a href="c1f3dd64.html">《神经切线核之 Python 实现》</a></p></li><li><p><strong>Domingos</strong> 等 2020 年的  <a href="80deb1b2.html">《梯度下降学得的模型都近似于一个核机》</a>：在神经正切核基础上，提出了路径核的概念，并认为所有通过梯度下降学得的模型，都可以被视为一种核机器。</p></li><li><p><strong>Li</strong>  等 2022 年的  <a href="">《神经网络的高斯过程代理模型》</a> 。将深度学习网络视为为内部过程不透明的复杂系统，用易于解释的高斯过程取代（或模仿）复杂神经网络系统的行为，这种高斯过程代理模型能够从神经网络的自然行为中凭经验学习高斯过程的核，这与 Lee 、Matthews Domingos 等从神经网络的极限情况下推导核具有显著不同。</p></li></ul><p><strong>（4）生成式神经网络</strong> </p><p>所有的生成模型几乎都与学习数据分布以及采样有关，也是概率模型与神经网络产出最多的领域。</p><ul><li><p>受限玻尔兹曼机：Hinton 等提出的 <a href="3f3fecac.html">受限玻尔兹曼机及深度置信网络</a></p></li><li><p>变分自编码器：Kingma 等 2014 年提出的变分自编码器，入门可以先阅读<a href="65612c13.html">《初始变分自编码器》</a>，进一步可以阅读原作者 2019 年撰写的 <a href="da72f251.html">《权威综述》</a>。 </p></li><li><p>自回归神经网络：参看 Murphy 2023 年的  <a href="d097769d.html">《Probabilistic Machine Learning: Advanced Topics》 第 22 章</a>。</p></li><li><p>归一化流：参见 Papamakarios 等人 2021 年的综述文章 <a href="1e755394.html"> 《Normalizing Flows for Probabilistic Modeling and Inference》 </a></p></li><li><p>基于能量的模型：参看 Murphy 2023 年的  <a href="d8f42e3d.html">《Probabilistic Machine Learning: Advanced Topics》 第 24 章</a></p></li><li><p>生成式对抗网络： 待整理</p></li><li><p>扩散模型：参见 Yang 等 2022 年的综述 <a href="c0fb1f85.html">《Diffusion Models: A Comprehensive Survey of Methods and Applications》</a> 以及 <a href="3b7358a6.html">扩散模型概览</a></p></li></ul><h2 id="二、知识体系的构建"><a href="#二、知识体系的构建" class="headerlink" title="二、知识体系的构建"></a>二、知识体系的构建</h2><p>贝叶斯统计方法以贝叶斯规则指导，已经基本形成了以概率图为形式化工具的一套相对完整的知识体系。贝叶斯新手建议由浅入深得学习。个人建议分为三个层次：</p><h3 id="入门层次"><a href="#入门层次" class="headerlink" title="入门层次"></a>入门层次</h3><ul><li>愿 景：掌握基础概念和入门级别的工作能力。</li><li>目 的：<ul><li>理解贝叶斯思维</li><li>学会基础的贝叶斯建模流程和工作方法</li><li>学会简单的概率编程</li><li>感性认识 MCMC、变分推断等统计推断方法</li><li>掌握线性回归、多元线性回归、广义线性回归等基础回归模型</li><li>理解高斯混合模型、狄利克雷过程、连续混合模型等隐变量模型</li><li>理解高斯过程等非参数贝叶斯方法</li></ul></li><li>书 籍：<ul><li>Osvaldo Martin, [Bayesian Analysis with Python(2nd)](<a href="https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html">https://xishansnow.github.io/BayesianAnalysiswithPython2nd/index.html</a></li><li>Osvaldo Martin,<a href="https://xishansnow.github.io/BayesianModelingandComputationInPython/index.html">Bayesian Modeling and Computation in Python</a></li><li>McElreath, R. . [Statistical Rethinking (2nd )]. <a href="https://doi.org/10.1201/9781315372495">https://doi.org/10.1201/9781315372495</a></li><li>Kruschke, <a href="https://sites.google.com/site/doingbayesiandataanalysis/">Doing Bayesian Data Analysis</a></li><li>Andrew Gelman et al., <a href="https://avehtari.github.io/BDA_course_Aalto/index.html">Bayesian Data Analysis(3rd)</a></li></ul></li><li>教 程：<ul><li>Herbert Lee, <a href="https://www.coursera.org/learn/bayesian-statistics?specialization=bayesian-statistics">Bayesian Statistics: From Concept to Data Analysis</a></li><li>Matthew Helner, <a href="https://www.coursera.org/learn/mcmc-bayesian-statistics?specialization=bayesian-statistics">Bayesian Statistics: Techniques and Models</a></li><li>McElreath R. et al., [Statistical Rethinking 2022] <a href="https://github.com/rmcelreath/stat_rethinking_2022">https://github.com/rmcelreath/stat_rethinking_2022</a></li><li>Andrew Gelman et al., <a href="https://avehtari.github.io/BDA_course_Aalto/index.html">Bayesian Data Analysis Course</a></li><li>Padhraic Smyth et al., <a href="https://www.ics.uci.edu/~smyth/courses/cs274/">California Unvi., CS274A-Probabilistic Learning:Theory and Algorithms</a></li></ul></li></ul><hr><h3 id="中级层次"><a href="#中级层次" class="headerlink" title="中级层次"></a>中级层次</h3><ul><li>愿 景：熟练掌握概率图模型，并利用概率图模型进行建模、学习和推断。</li><li>目 标：<ul><li>理解什么是概率图模型</li><li>掌握贝叶斯网络、马尔可夫随机场两种表示方法</li><li>掌握变量消除、消息传递等概率图推断的传统方法</li><li>掌握 MCMC、变分推断等近似推断基本原理和方法</li><li>掌握完全可观测模型、部分可观测模型的学习原理和方法</li><li>掌握高斯过程、狄利克雷过程等非参数模型的概率图方法</li><li>掌握因子分析、主组分分析、隐马尔可夫、状态空间等常用的概率图模型</li></ul></li><li>书 籍：<ul><li>Koller, D. (2009). Probabilistic graphical models: Principles and techniques. The MIT Press.</li><li>Michael I. Jordan, An Introduction to Probabilistic Graphical Models</li></ul></li><li>教 程：<ul><li>Stefano ERmon et al., <a href="https://ermongroup.github.io/cs228/">Standford Univ., CS228- Probabilistic Graphical Models</a></li><li>Daphne Koller, Standford Univ., <a href="https://www.coursera.org/specializations/probabilistic-graphical-models">Probabilistic Graphical Models: Master a new way of reasoning and learning in complex domains</a></li></ul></li><li>Erik Sudderth et al. <a href="https://canvas.eee.uci.edu/courses/35909">California Univ. CS274B-Learning in Graphical Models</a></li><li>Eric P.Xing et al. <a href="http://www.cs.cmu.edu/~epxing/Class/10708-20/index.html">CMU. 10-708-Probabilistic Graphical Models</a> ， 课程的 Lecture 和 Notes 都非常全，其中高级主题部分可以纳入下一个层次</li></ul><hr><h3 id="高级层级"><a href="#高级层级" class="headerlink" title="高级层级"></a>高级层级</h3><ul><li>愿 景：掌握概率图和神经网络的结合和应用方法。</li><li>目 标：<ul><li>熟练使用概率图和计算图建立概率神经网络模型</li></ul></li></ul>    <style>    #refplus, #refplus li{         padding:0;        margin:0;        list-style:none;    }；    </style>    <script src="https://unpkg.com/@popperjs/core@2"></script>    <script src="https://unpkg.com/tippy.js@6"></script>    <script>    document.querySelectorAll(".refplus-num").forEach((ref) => {        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');        let refel = document.querySelector(refid);        let refnum = refel.dataset.num;        let ref_content = refel.innerText.replace(`[${refnum}]`,'');        tippy(ref, {            content: ref_content,        });    });    </script>    ]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 综述概览 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 统计建模 </tag>
            
            <tag> 统计学习 </tag>
            
            <tag> 统计推断 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nutstore 安装常见问题处理</title>
      <link href="/vll-pages/posts/ee914680.html"/>
      <url>/vll-pages/posts/ee914680.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yay -S nutstore</span><br></pre></td></tr></table></figure><h2 id="白屏"><a href="#白屏" class="headerlink" title="白屏"></a>白屏</h2><p>双击图标，白屏？</p><p><img src="https://bu.dusays.com/2022/08/10/62f3cc797bc09.webp" alt="白屏"></p><p>修改 <code>/opt/nutstore/conf/nutstore.properties</code>（ 如果安装在个人目录，查看 <code>~/.nutstore/conf/dist/conf/nutstore.properties</code>）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> sed -i <span class="string">&#x27;s|webui.enable=true|webui.enable=false|&#x27;</span> /opt/nutstore/conf/nutstore.properties</span><br></pre></td></tr></table></figure><h2 id="窗口太小无法全部显示"><a href="#窗口太小无法全部显示" class="headerlink" title="窗口太小无法全部显示"></a>窗口太小无法全部显示</h2><p>（1）利用 KDE 的窗口设置功能（左上角右键菜单&#x2F;更多操作&#x2F;窗口规则设置&#x2F;配置特殊引用程序窗口设置..）</p><p><img src="https://bu.dusays.com/2022/08/10/62f3cc822fbe9.webp" alt="调出窗口规则设置界面"></p><p>（2）添加新属性（貌似  $600 \times 500$ 比较合适）</p><p><img src="https://bu.dusays.com/2022/08/10/62f3cc864a783.webp" alt="进行设置"></p><p>（3）应用设置进行设置</p><p><img src="https://bu.dusays.com/2022/08/10/62f3cc89ca4eb.webp" alt="完成"></p><p>如此完成设置，窗口内主要要素基本都能显示出来了。</p><h2 id="桌面使用了暗色主题导致部分字体不清晰？"><a href="#桌面使用了暗色主题导致部分字体不清晰？" class="headerlink" title="桌面使用了暗色主题导致部分字体不清晰？"></a>桌面使用了暗色主题导致部分字体不清晰？</h2><p><img src="https://bu.dusays.com/2022/08/10/62f3cc8f193b1.webp" alt="这字体鬼看得见？"></p><p>这字体鬼看得见？</p><p>参考<a href="https://zhul.in/2021/09/05/wrong-fonts-color-fix-under-kde-with-a-dark-theme/">使用fakehome方案暂时解决跑在KDE暗色主题下的程序使用亮色字体的问题</a>编写启动命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bwrap --dev-bind / / --tmpfs $HOME/.config /usr/bin/nutstore</span><br></pre></td></tr></table></figure><p><a href="https://bu.dusays.com/2022/08/10/62f3cc9245d26.webp" title="测试通过"><img src="https://bu.dusays.com/2022/08/10/62f3cc9245d26.webp" alt="测试通过"></a></p><p> <strong>注意： 这种方式会导致由坚果云触发的各种引用程序界面都发生变化</strong> 。</p><h2 id="本地markdown文件的文件类型被识别成了「坚果云-Markdown」"><a href="#本地markdown文件的文件类型被识别成了「坚果云-Markdown」" class="headerlink" title="本地markdown文件的文件类型被识别成了「坚果云 Markdown」"></a>本地markdown文件的文件类型被识别成了「坚果云 Markdown」</h2><p>由于坚果云自作主张推广他自己并不好用的 lightapp，写了几条 mime 的规则，如图</p><p><img src="https://bu.dusays.com/2022/08/10/62f3cc95f0dd1.webp" alt="没错，整整5个xml"></p><p>看来在我们的启动命令中也需要防止坚果云接触到 <code>$HOME/.local/share/</code> 这个路径，所以现在的启动命令得写成这样。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bwrap --dev-bind / / --tmpfs $HOME/.config --tmpfs $HOME/.local/share/ /usr/bin/nutstore</span><br></pre></td></tr></table></figure><h2 id="修改-desktop-文件，使其使用我们自己攥写的启动命令"><a href="#修改-desktop-文件，使其使用我们自己攥写的启动命令" class="headerlink" title="修改 desktop 文件，使其使用我们自己攥写的启动命令"></a>修改 desktop 文件，使其使用我们自己攥写的启动命令</h2><p>首先，复制一份 desktop 文件到 $HOME$ 目录下，这样系统更新时不会被包管理器覆盖。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /usr/share/applications/nutstore.desktop $HOME/.local/share/applications/</span><br></pre></td></tr></table></figure><p>修改 <code>$HOME/.local/share/applications/nutstore.desktop</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[Desktop Entry]</span><br><span class="line">Encoding=UTF-8</span><br><span class="line">Type=Application</span><br><span class="line">Terminal=false</span><br><span class="line">Icon=nutstore</span><br><span class="line">-Exec=/usr/bin/nutstore</span><br><span class="line">+Exec=bwrap --dev-bind / / --tmpfs $HOME/.config --tmpfs $HOME/.local/share/applications --tmpfs $HOME/.local/share/mime /usr/bin/nutstore</span><br><span class="line">StartupWMClass=Nutstore</span><br><span class="line">Name=Nutstore</span><br><span class="line">Name[zh_CN]=坚果云 </span><br><span class="line">Comment=Data Sync, Sharing, Backup</span><br><span class="line">Comment[zh_CN]=数据同步,共享和备份</span><br><span class="line">Categories=Network;Application;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 软件安装与使用 </category>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安装与使用 </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>chezmoi个人配置管理工具的使用</title>
      <link href="/vll-pages/posts/6677fcb3.html"/>
      <url>/vll-pages/posts/6677fcb3.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h2 id="1-添加和取消配置项"><a href="#1-添加和取消配置项" class="headerlink" title="1 添加和取消配置项"></a>1 添加和取消配置项</h2><p>所有的修改均以本地的 chezmoi source 为中心（通常在<code>.local/share/chezmoi</code> 下），常用命令包括：</p><ul><li><strong>初始化本地源</strong>：                       chezmoi init</li><li><strong>向源中添加配置项</strong>：                   chezmoi add</li><li><strong>从源中取消配置项</strong>：                   chezmoi forget</li><li><strong>修改本地源并更新配置项</strong>：             chezmoi edit  并且 chezmoi apply</li><li><strong>外部修改配置项后更新本地源</strong>：         chezmoi re-add</li><li><strong>在源目录下执行 git 命令</strong>：          chezmoi git <command></li><li><strong>进入源目录</strong>：                         chezmoi cd</li><li><strong>显示源的配置</strong>：                       chezmoi cat-config</li><li><strong>编辑源的配置</strong>：                       chezmoi edit-config</li></ul><h2 id="2-修改和更新配置项："><a href="#2-修改和更新配置项：" class="headerlink" title="2 修改和更新配置项："></a>2 修改和更新配置项：</h2><ul><li><strong>方法 1: 先修改源，后更新配置</strong>:<br>   先调用 chezmoi edit 修改文件，而后 chezmoi apply 更新至真正的目标配置文件（你也可以修改 neovim 的 autocmd，让它自动 apply，不推荐）</li><li><strong>方法 2：先修改配置，再更新源</strong>:<br>   先调用外部编辑器修改目标配置文件，然后 chezmoi re-add 重新添加一次文件。需要自己做 git commit。</li></ul><h2 id="3-备份至-github"><a href="#3-备份至-github" class="headerlink" title="3 备份至 github"></a>3 备份至 github</h2><ul><li><strong>方法 1</strong>: chezmoi cd &amp;&amp; git push</li><li><strong>方法 2</strong>: chezmoi git push</li></ul><h2 id="4-从-github-恢复至本地"><a href="#4-从-github-恢复至本地" class="headerlink" title="4 从 github 恢复至本地"></a>4 从 github 恢复至本地</h2><ul><li><strong>方法 1</strong>: chezmoi git pull &amp;&amp; chezmoi apply</li><li><strong>方法 2</strong>: chezmoi update</li></ul>]]></content>
      
      
      <categories>
          
          <category> 软件安装与使用 </category>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安装与使用 </tag>
            
            <tag> Linux </tag>
            
            <tag> chezmoi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 中个人常用目录的中英文转换</title>
      <link href="/vll-pages/posts/1ab9af24.html"/>
      <url>/vll-pages/posts/1ab9af24.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>相信大家在使用Linux的时候会遇到一个问题，如果是全英状态下，那么对英语不太好的人，使用起来可能有些难度，但是如果是中文状态下的话，Linux主目录下面的几个文件夹也会变成中文，然后在终端中时候的话，输入法需要中英文切换，可能比较麻烦，下面教大家在中文状态下把文件夹的名称换成英文。</p><p>（1）首先，打开终端，修改环境变量 <code>export LANG=en_US</code> ；<br>（2）然后，输入 <code>xdg-user-dirs-gtk-update</code> ，这是一个修改目录的 GTK 程序；<br>（3）在界面中此时会根据 LANG 环境变量自动设置目标目录，选择更新目录即可；<br>（4）再将环境变量设为中文即可： <code>export LANG=zh_CH</code>。</p><p>最终实现：目录变成了英文，但是操作系统还是中文。</p>]]></content>
      
      
      <categories>
          
          <category> 软件安装与使用 </category>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安装与使用 </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>latex 编译链简介</title>
      <link href="/vll-pages/posts/c633c9af.html"/>
      <url>/vll-pages/posts/c633c9af.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>【摘 要】 在 LaTex 中，文章内容、参考文献文件、宏包文件、格式文件是相互分开的。编译过程需要将这些文件拼接起来，形成最终的 pdf 文件。此过程不是一步到位的，而是涉及到一个编译链条：将前一步编译的结果输送到下一步继续编译。</p><h2 id="1-Latex-中的各种文件"><a href="#1-Latex-中的各种文件" class="headerlink" title="1 Latex 中的各种文件"></a>1 Latex 中的各种文件</h2><p>latex 中的常见文件有如下类型（指参与编译的源文件）：</p><ul><li><strong>.tex</strong>：tex 文件是最常见的 latex 文件，也是平时编写文章主要文件 </li><li><strong>.cls</strong>：cls 文件是 latex 的格式文件，规定了 tex 源文件的排版格局，称为类文件（class），一般使用 <code>\documentclass&#123;&#125;</code> 导入 </li><li><strong>.sty</strong>：sty 文件是一种宏包文件（package），一般使用 <code>\usepackage&#123;&#125;</code> 导入 </li><li><strong>.bst</strong>：bst 文件是参考文献的排版格式文件，一般使用 <code>\bibliographystyle&#123;&#125;</code> 导入 </li><li><strong>.bib</strong>：存储参考文献数据的库文件，一般使用 <code>\bibliography&#123;&#125;</code> 导入文中使用到的文献</li><li><strong>其他文件</strong>： 图片、多媒体等其他资源文件</li></ul><p>其中 bib 文件一般如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">@article&#123;XXX,</span><br><span class="line">  title=&#123;ABC&#125;,</span><br><span class="line">  author=&#123;A, B&#125;,</span><br><span class="line">  journal=&#123;XX&#125;,</span><br><span class="line">  year=&#123;20XX&#125;</span><br><span class="line">&#125;</span><br><span class="line">@inproceedings&#123;YYY,</span><br><span class="line">  title=&#123;ABC&#125;,</span><br><span class="line">  author=&#123;A, B, C&#125;,</span><br><span class="line">  booktitle=&#123;YY&#125;,</span><br><span class="line">  pages=&#123;a--b&#125;,</span><br><span class="line">  year=&#123;20YY&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="2-一个示例"><a href="#2-一个示例" class="headerlink" title="2 一个示例"></a>2 一个示例</h2><p>假设在当前目录下有下列文件：<code>main.tex</code>、<code>A.cls</code>、<code>B.sty</code>、<code>C.bst</code>、<code>D.bib</code>。</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%main.tex 文件</span></span><br><span class="line"><span class="keyword">\documentclass</span>&#123;A&#125; <span class="comment">% 或者不使用自定义的排版文件时，使用最普通的 \documentclass&#123;article&#125; </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">\usepackage</span>&#123;B&#125; <span class="comment">% 以及导入一些其他常用的宏文件，如 amsmath、amssymb、amsthm 等数学相关的宏文件 </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">\begin</span>&#123;document&#125; </span><br><span class="line"></span><br><span class="line">XXX <span class="comment">% 正文</span></span><br><span class="line">XXX <span class="comment">% 正文</span></span><br><span class="line">XXX <span class="comment">% 正文 </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">\bibliography</span>&#123;D&#125; <span class="comment">% 导入正文中引入文献的数据 </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">\bibliographystyle</span>&#123;C&#125; <span class="comment">% 导入参考文献的格式文件 C.bst </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">\end</span>&#123;document&#125;</span><br></pre></td></tr></table></figure><h2 id="3-编译过程"><a href="#3-编译过程" class="headerlink" title="3 编译过程"></a>3 编译过程</h2><p>类似上例附带参考文献的源文件，整个编译需要完成四步：</p><p>【符号说明】：<code>(xe/pdf)latex</code> 表示可以使用 <code>latex</code>, <code>pdflatex</code> 或 <code>xelatex</code> 等工具进行编译。</p><p><strong>第一步</strong> <code>(xe/pdf)latex main.tex</code>:</p><p>生成 <code>main.aux</code>、<code>main.log</code> 和 <code>main.pdf</code> 文件。其中 <code>aux</code> 是引用标记记录文件，用于再次编译时生成参考文献和超链接。此时的 <code>pdf</code> 文件中不包含参考文献，文中的引用表现为 <code>[?]</code>。</p><p>第二步  <code>bibtex main.aux</code> </p><p>生成 <code>main.bbl</code> 和 <code>main.blg</code> 文件。其中 <code>blg</code> 为 bibtex 处理过程记录文件，<code>bbl</code> 文件为文中使用的参考文献数据。</p><p>第三步 <code>(xe/pdf)latex main.tex</code> 更新了 <code>main.aux</code>、<code>main.log</code> 和 <code>main.pdf</code> 文件。此时的 <code>pdf</code> 文件的末尾有了参考文献列表，但是正文中的引用仍然表现为 <code>[?]</code>。</p><p>第四步 <code>(xe/pdf)latex main.tex</code>，更新了 <code>main.aux</code>、<code>main.log</code> 和 <code>main.pdf</code> 文件。并生成最终的 <code>pdf</code> 文件，此时正文中的引用表现为经标记好的引用方式，如 <code>[1]</code>、<code>[2]</code> … 等。</p><p>上述编译链看起来非常复杂，因此有人开发了一个工具，可以将上述计算流程整合为一个过程，这就是 <code>latexmk</code>。</p><h2 id="4-PDFLaTeX-和-XeLaTeX-的区别"><a href="#4-PDFLaTeX-和-XeLaTeX-的区别" class="headerlink" title="4 PDFLaTeX 和 XeLaTeX 的区别"></a>4 PDFLaTeX 和 XeLaTeX 的区别</h2><p><code>PDFLaTeX</code> 编译模式与 <code>XeLaTeX</code> 区别如下： </p><ul><li>PDFLaTeX 使用的是 TeX 的标准字体，所以生成 PDF 时，会将所有的非 TeX 标准字体进行替换，其生成的 PDF 文件默认嵌入所有字体；</li><li>使用 XeLaTeX 编译，如果论文中有很多图片或者其他元素没有嵌入字体的话，生成的 PDF 文件也会有些字体没有嵌入。 XeLaTeX 对应的 XeTeX 字体的支持更好，允许用户使用操作系统字体来代替 TeX 的标准字体，而且对非拉丁字体的支持更好。 </li><li>PDFLaTeX 进行编译的速度比 XeLaTeX 快。</li></ul><h2 id="5-latexmk-使用常识"><a href="#5-latexmk-使用常识" class="headerlink" title="5 latexmk 使用常识"></a>5 latexmk 使用常识</h2><p>已经有人将上述编译过程整合在了一个命令行程序中，那就是 <code>latexmk</code>。 </p><h3 id="5-1-基本命令格式"><a href="#5-1-基本命令格式" class="headerlink" title="5.1 基本命令格式"></a>5.1 基本命令格式</h3><p><code>latexmk</code> 的通用格式为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">latexmk [options] [file]</span><br></pre></td></tr></table></figure><p>其中 <code>file</code> 可以直接写文件名, 如果直接写 <code>foo</code> 的话, <code>latexmk</code> 会按照 <code>foo.tex</code> 来处理. 有一些符号在文件名中是不能用的: <code>$</code>, <code>%</code>, <code>\</code>,`(这个东西不能放在代码块里……), <code>&amp;</code>这个符号不能作为文件名的开头, 还有一些控制字符也不可以, 不过我估计没人会闲的用控制字符来做 <code>.tex</code> 的文件名…</p><h3 id="5-2-常用命令行选项"><a href="#5-2-常用命令行选项" class="headerlink" title="5.2 常用命令行选项"></a>5.2 常用命令行选项</h3><p>（1） <code>-auxdir=FOO</code> 或者 <code>-aux-directory=FOO</code></p><p>设置存放辅助文件的文件夹, 可惜此选项只对 MacOS 的 <code>MiKTeX</code> 版本起作用, linux 的 <code>TeX Live</code> 无法使用。</p><p>（2）<code>-bibtex</code></p><p>当源文件需要用 <code>.bbl</code> 文件作为参考文献的时候, 运行 <code>bibtex</code> 或者 <code>biber</code> 来更新 <code>.bbl</code> 文件. 这个选项可以将配置文件 <code>.latexmkrc</code> 中的 <code>$bibtex_use</code> 的值设置为 <code>2</code> 来实现.</p><p>（3） <code>-bibtex-</code></p><p>从不运行 <code>bibtex</code> 或者 <code>biber</code>. 同时将 <code>.bbl</code> 文件看做珍贵的 ( 原文为 precious ), 也就是在执行清理命令的时候不会删除 <code>.bbl</code> 文件。如果我们接受到的文件中只有 <code>.bbl</code> 而没有 <code>.bib</code> 的时候会启用此选项。这个选项可以将 <code>.latexmkrc</code> 中的 <code>$bibtex_use</code> 的值设置为 <code>0</code> 来实现.</p><p>（4） <code>bibtex-cond</code></p><p>如果源文件中存在 <code>.bib</code> 文件, 则运行 <code>bibtex</code> 或 <code>biber</code> 来重写 <code>.bbl</code> 文件, 如果没有 <code>.bib</code> 文件, 则不运行 <code>bibtex</code> 或 <code>biber</code>, 这个选项也会将 <code>.bbl</code> 文件看做珍贵的. 这个选项可以将<code>.latexmkrc</code> 中的 <code>$bibtex_use</code> 的值设置为 <code>1</code> 来实现.</p><p>（5） <code>-command</code></p><p>列出 <code>latexmk</code> 在处理文件时可以使用的命令, 然后退出.</p><p>（6） <code>-c</code></p><p>清理所有由 <code>latex</code> , <code>bibtex</code>, 以及 <code>biber</code> 生成的可再生的文件, 除了 <code>.dvi</code>, <code>postscript</code>, 和 <code>.pdf</code> 文件. 会删除的文件有 <code>.log</code>, <code>.aux</code>, <code>latexmk</code> 创建的数据文件, 还有 <code>@generated_exts</code> 中指定扩展名的文件, 同时还有 <code>$clean_ext</code> 指定的文件.</p><p>（7） <code>-C</code></p><p>清理 <code>-c</code> 选项中的所有文件, 以及 <code>.pdf</code>, <code>.dvi</code>, <code>postscript</code> 文件, 以及 <code>$clean_full_ext</code> 指定的文件.</p><p>（8） <code>-cd</code></p><p>在进行处理之前切换到有源程序的文件夹, 并将所有的文件生成在这个文件夹里. 举例: 如果你的文件夹结构为 <code>parent/subfolder/main.tex</code>, 你在 <code>parent</code> 文件夹下, 命令行执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">latexmk -cd -xelatex subfolder/main</span><br></pre></td></tr></table></figure><p>会进入 <code>subfolder</code> 文件夹进行编译, 并将辅助文件和输出文件生成在 <code>subfolder</code> 文件夹下. 这里注意路径的分隔符要使用 <code>/</code> , 哪怕是 <code>Windows</code>.</p><p>（9） <code>-f</code> 和 <code>-interaction=nonstopmode</code></p><p>强制执行 <code>latexmk</code> 哪怕遇到了 <code>error</code>. 一般情况下, 当 <code>latexmk</code> 遇到了 <code>latex</code> 或者其他程序在 <strong>接下来的运行中</strong> 无法处理的问题的时候, 将不会给出处理结果。</p><p><strong>接下来的运行</strong> 指的是 如果没有 <code>error</code> 出现, 运行其他程序或者重新运行<code>latex</code> 的时候会编译完成 ( 这段其实我没看懂, 原文是 “Further processing” means the running of other Programs or the rerunning of latex (etc) that would be done if no errors had occurred. ) 额外地, 如果你想让 <code>latex</code> 程序遇到 <code>error</code> 之后不出现给用户处理 <code>error</code> 的暂停 , 你需要使用一些可以传递给程序的选项, 比如 <code>-interaction=nonstopmode</code>。 举例: 当文档里有一个严重的错误, 比如没有定义的命令。此时使用 <code>-f</code> 的命令行为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">latexmk -f -xelatex main</span><br></pre></td></tr></table></figure><p>此时在遇到错误时，依旧会暂停来等待用户处理。而使用 <code>-interaction=nonstopmode</code> 的话：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">latexmk -interction=nonstopmode -xelatex main</span><br></pre></td></tr></table></figure><p>此时，<code>latexmk</code> 会一气呵成运行到最后, 哪怕没有文件输出。</p><p>（10） <code>-g</code> </p><p>强制完整地运行一遍 <code>latexmk</code>, 哪怕在 <code>latexmk</code> 觉得源文件自从上次编译之后没有改动</p><p>（11） <code>-jobname=STRING</code></p><p>把输出文件中的文件名设置为 <code>STRING</code>, 而不是源文件名, 注意 <code>STRING</code> 中不能带空格。 允许使用占位符 <code>%A</code> , 比如</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">latexmk -xelatex -jobname=%A-xelatex main</span><br></pre></td></tr></table></figure><p>此时输出文件名就变成了 <code>main-xelatex.***</code>。</p><p>（12） <code>-outdir=FOO</code> 或者 <code>-output-directory=FOO</code></p><p>设置存放所有输出文件的文件夹。</p><p>（13）编译和预览控制</p><ul><li><code>-pdflatex</code>： 使用 <code>pdflatex</code> 作为编译器, 该编译器会关闭 <code>.dvi</code> 和 <code>.ps</code> 文件的生成。</li><li><code>-xelatex</code>： 使用 <code>xelatex</code> 作为编译器, 该编译器会关闭 <code>.dvi</code> 和 <code>.ps</code> 文件的生成。</li><li><code>-pdf</code>： 使用 <code>pdflatex</code> 编译时，输出 <code>.pdf</code> 文件。</li><li><code>-pvc</code>： 是否运行预览。编译中文文档时会使用 <code>xelatex</code> 编译器，此时不生成 <code>.dvi</code> 和 <code>.ps</code> 文件, 所以会直接预览 <code>.pdf</code> 文件。</li></ul><p>（14） <code>-r &lt;rcfile&gt;</code></p><p>人工指定配置文件，按照 <code>&lt;rcfile&gt;</code> 中的内容执行 <code>latexmk</code>。 但需注意调用顺序, <code>latexmk</code> 总是会先调用标准初始的 <code>.latexmkrc</code> 文件, 在不存在初始 <code>.latexmkrc</code> 文件时，再去调用 <code>&lt;rcfile&gt;</code>。但如果 <code>&lt;rcfile&gt;</code> 中指定了一个初始 <code>.latexmkrc</code> 文件, 那么此指定会在标准的初始 <code>.latexmkrc</code> 文件之前运行。</p><h3 id="5-3-latexmkmc-配置文件"><a href="#5-3-latexmkmc-配置文件" class="headerlink" title="5.3 .latexmkmc 配置文件"></a>5.3 <code>.latexmkmc</code> 配置文件</h3><p><code>latexmk</code> 可以通过配置文件来设置一些默认选项，标准的初始文件位置为 <code>~/.latexmkrc</code> </p><p>变量采用如下格式设置：</p><ul><li>设置一个值为 <strong>字符串型</strong> 的变量，如： <code>$bibtex=&#39;bibtex %O %B&#39;;</code></li><li>设置一个值为 <strong>数值型</strong> 的变量，如: <code>$preview_mode=1;</code></li><li>设置一个值为 <strong>数组型</strong> 的变量，如：<code>@default_files=(&#39;paper&#39;, &#39;paper1&#39;);</code> </li><li>用如下语句来给数组变量追加值，如: <code>push @default_files, &#39;paper2&#39;;</code></li></ul><p>可以注意到：简单变量的名字的开头都是 <code>$</code>, 而数组变量的名字的开头是 <code>@</code>, 每个语句都以分号 <code>；</code> 结尾。 字符串应该用 <strong>单引号</strong> 括起来, 不建议使用双引号。</p><h3 id="5-4-占位符的使用"><a href="#5-4-占位符的使用" class="headerlink" title="5.4 占位符的使用"></a>5.4 占位符的使用</h3><p>可以使用一些占位符来辅助设置 <code>latexmk</code> 中的一些命令参数, 例如： 如果想让 <code>latexmk</code> 将 <code>elatex</code> 用作编译器命令, 并且想让其使用 <code>--shell-escape</code> 等选项, 可以写为：</p><p><code>latex=&#39;elatex --shell-escape %O %S&#39;;</code></p><p>其中，两个前面带 <code>%</code> 的符号就被称为 <strong>占位符</strong>, 在实际运行命令之前，这些占位符会被替换为实际值. 因此 <code>%S</code> 会被替换为源文件名, <code>%O</code> 会被替换为将作用在此命令上的各种选项,下面列出可用的占位符：</p><ul><li><code>%A</code>: 主 <code>.tex</code> 文件的文件名。 不同于 <code>%R</code>, <code>%A</code> 不会被设置别名(jobname) 而受影响;</li><li><code>%B</code>: 最近命令中的基础名 (base name)。 比如一个 <code>document.ps</code> 文件从一个 <code>document.dvi</code> 转化得到, 那么这个文件名就是 <code>document</code></li><li><code>%D</code>: 终点 (destination) 文件名。 比如一个 <code>.ps</code> 文件由一个 <code>.dvi</code> 文件转化得到, 那么终点文件名就是这个 <code>.ps</code> 文件。名</li><li><code>%O</code>: 例子中提到的选项。</li><li><code>%P</code>: 如果变量 <code>$pre_tex_code</code> 不空, 那么 <code>%P</code> 会被替换为 <code>$pre_tex_code</code> 中 <code>\input&#123;SOURCE&#125;</code> 后面的内容, 其中 <code>SOURCE</code> 是资源文件 (source file) 的名字。 这让 TeX 代码可以在资源文件被读入之前就传给 <code>*latex</code> 引擎。 如果 <code>$pre_tex_code</code> 为空, 那么 <code>%P</code> 和 <code>%S</code>等价。</li><li><code>%R</code>: 根文件名, 这是主 <code>.tex</code> 文件的基础名, 但是这个值可以被 <code>-jobname</code> 选项或者 <code>$jobname</code> 变量改变。</li><li><code>%S</code>: 源文件名。 比如当转化一个 <code>.dvi</code> 文件到 <code>.ps</code> 的时候, <code>.dvi</code> 的文件名就是源文件名。</li><li><code>%T</code>: The name of the primary <code>.tex</code> file。</li><li><code>%U</code>: 如果变量 <code>$pre_tex_code</code> 不空, 那么这个值就被传递给 <code>%U</code>, 如果为空, 那么它被替换为一个空字符 (null string)。</li><li><code>%Y</code>: 辅助文件所在的文件夹名。 如果文件夹名不是以合法符号结束, 那么就会被添加一个文件夹分隔符 <code>/</code>。 注意: 如果设置了 <code>$out_dir</code> 却没设置 <code>$aux_dir</code>, 那么<code>latexmk</code> 就把 <code>$aux_dir</code> 设置为 <code>$out_dir</code>。</li><li><code>%Z</code>: 输出文件夹名。 如果 <code>$out_dir</code> 不空, 且没以合法符号结束, 那么就会被添加一个文件夹分隔符 <code>/</code>。</li></ul><p>如果有什么原因你要使用一个 <code>%</code> 字符, 还不是上面标出的情况, 那么用 <code>%%</code>.</p><p><strong>注意</strong>: 在文件名传递的过程中会被适当地自动添加引号, 所以不需要自行添加引号, 即使文件名中有空格。  如果你的 TeX 文件名中有空格, 那么一些过老版本的 TeX 程序可能不会很好地处理它们。如果 <code>latexmk</code> 的引号不能正确地工作, 你也可以将它关闭, 在变量 <code>$quote_filenames</code> 的设置中。 </p>]]></content>
      
      
      <categories>
          
          <category> 软件安装与使用 </category>
          
          <category> latex </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安装与使用 </tag>
            
            <tag> latex </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pyton、node.js、hexo配置指南</title>
      <link href="/vll-pages/posts/5c4624dd.html"/>
      <url>/vll-pages/posts/5c4624dd.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h1 id="1-python-环境配置注意事项"><a href="#1-python-环境配置注意事项" class="headerlink" title="1 python 环境配置注意事项"></a>1 python 环境配置注意事项</h1><h2 id="1-1-安装-conda-环境"><a href="#1-1-安装-conda-环境" class="headerlink" title="1.1 安装 conda 环境"></a>1.1 安装 conda 环境</h2><ul><li>方法1: 看帮助安装和配置 <a href="https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/">https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/</a></li><li>方法2: <code>sudo pacman -S miniconda</code> 或 <code>sudo pacman -S anaconda</code></li></ul><p>注意： miniconda 小一些，按需自行安装软件包，因为用的少，所以我通常使用 miniconda</p><h2 id="1-2-conda-常用命令"><a href="#1-2-conda-常用命令" class="headerlink" title="1.2 conda 常用命令"></a>1.2 conda 常用命令</h2><ul><li>显示 conda 的配置信息： <code>conda info</code></li><li>新建环境<code>conda create -n ENV_NAME python=版本号</code></li><li>激活环境<code>conda activate ENV_NAME</code></li><li>安装包  <code>conda install PACKAGE_NAME</code></li><li>卸载包   <code>conda remove PACKAGE_NAME</code></li><li>显示所有已安装的包<code>conda list</code></li><li>退出环境<code>conda deactivate</code></li><li>删除环境<code>conda env remove -n ENV_NAME</code></li><li>显示所有已安装的环境<code>conda env list</code></li></ul><p>注： </p><ul><li>（1）创建环境时最好按需指定 python 版本号</li><li>（2）conda install 的软件包来自 conda 及其镜像站维护的软件源；</li><li>（3） anaconda或miniconda 默认会修改 .bashrc， 可能不会修改 .zshrc，建议同步将修改内容拷贝至 .zshrc</li></ul><h2 id="1-3-pip-常用命令"><a href="#1-3-pip-常用命令" class="headerlink" title="1.3 pip 常用命令"></a>1.3 pip 常用命令</h2><ul><li><p>确认是否安装了pip ：   <code>python -m ensurepip</code></p></li><li><p>安装指定版本的pip ：   <code>python -m pip install pip==[version]</code></p></li><li><p>安装软件包：           <code>pip install [package]</code></p></li><li><p>安装指定版本的包：     <code>pip install [package==version]</code></p></li><li><p>升级软件包：           <code>pip install -U [package]</code></p></li><li><p>卸载软件包：           <code>pip uninstall [package]</code></p></li><li><p>设置 pip 镜像源：      <code>pip config set global.index-url 源地址</code>    # 如： <a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a></p><p>查询源中的软件包（因恶意搜索已停用）：      <code>pip search [package]</code>，目前只能到网站查询 <a href="https://pypi.org/search">https://pypi.org/search</a> (via a browser)</p></li></ul><p>批量安装：</p><ul><li>安装 requirements.txt 文件中指定的所有软件包：       <code>pip install [-U] -r requirements.txt</code></li><li>将当前环境中的包列表导出到 requirements.txt 文件中： <code>pip freeze &gt; requirements.txt</code></li></ul><p>注：pip 的软件源来自 <a href="https://pypi.org/">https://pypi.org</a> 及其镜像站</p><h2 id="1-4-两者区别"><a href="#1-4-两者区别" class="headerlink" title="1.4 两者区别"></a>1.4 两者区别</h2><p>（1）pip 仅仅是包管理工具，而 conda 同时还是一个环境管理工具</p><p>（2）pip 能安装 pypi 里的一切 Python 包，更加全面和专注，而 conda 可安装的 Python 包数量相比 pip 要少很多。</p><p>所以，日常使用 conda 管理环境， 在环境内部使用 pip 管理软件包。</p><h2 id="1-5-注意事项"><a href="#1-5-注意事项" class="headerlink" title="1.5 注意事项"></a>1.5 注意事项</h2><p>（1）安装 conda 后，注意是否在某个环境当中。conda 安装时默认自动进入 base 环境，可在 .bashrc 和 .zshrc 中运行 conda activate 环境名，以便登录后自动进入自己熟悉的环境，当然使用默认环境 base 也是可以的。</p><p>（2）pip 的软件包依赖关系管理比 conda 要好一些，默认情况下使用 pip 安装软件包</p><p>（3）pip 的批量安装非常好用，无论是别人发布的 python 软件，还是自己编写的软件， 都可以用 requirements.txt 文件辅助完成环境配置。</p><h1 id="2-node-js-环境配置"><a href="#2-node-js-环境配置" class="headerlink" title="2 node.js 环境配置"></a>2 node.js 环境配置</h1><h2 id="2-1-安装版本管理软件"><a href="#2-1-安装版本管理软件" class="headerlink" title="2.1 安装版本管理软件"></a>2.1 安装版本管理软件</h2><p>node.js 的版本管理软件也有多个，作用类似于 python 的 conda。最常用的版本管理软件是 nvm 和 fnm ，都很好用。此处主要介绍 nvm。</p><p>安装 nvm：    <code>sudo pacman -S nvm</code> 后，根据提示修改 .bashrc 和 .zshrc</p><p>注意：</p><p>（1）如果已安装 node 和 npm ，最好使用 pacman 删除（或手工删除，网上有方法），再使用 nvm 进行管理。</p><p>（2）如果不想使用 nvm 了，直接删除  <code>$NVM_DIR</code> 目录即可 (通常是 <code>~/.nvm</code>)</p><h2 id="2-2-nvm-常用命令"><a href="#2-2-nvm-常用命令" class="headerlink" title="2.2 nvm 常用命令"></a>2.2 nvm 常用命令</h2><ul><li>查看本地可用 node 版本：   <code>nvm list</code>  </li><li>查看源中可用 node 版本：   <code>nvm list-remote</code></li><li>安装所需 node 版本：       <code>nvm install 版本号</code></li><li>切换至指定 node 版本：     <code>nvm use 版本号</code></li><li>删除本地已安装的版本：     <code>nvm uninstall 版本号</code></li></ul><h2 id="2-3-npm-常用命令"><a href="#2-3-npm-常用命令" class="headerlink" title="2.3 npm 常用命令"></a>2.3 npm 常用命令</h2><p>npm 是 node 的软件包管理工具，类似于 python 的 pip。npm 通常会在当前目录下创建一个 package.json 文件，用于存储当前工作目录中的包列表。</p><p>npm 的软件包分为局部和全局两类，全局软件包将安装到 ~&#x2F;node_moudles 中，本人登录后随处可使用。局部软件包通常安装在当前目录的 node_moudles 目录中，只在当前目录环境中起作用。</p><ul><li><p>查看当前 npm 源： <code>npm config get registry</code>  </p></li><li><p>切换当前 npm 源： <code>npm config set registry=源地址</code>   # 如： <a href="https://registry.npmmirror.com/">https://registry.npmmirror.com</a> 或 <a href="https://mirrors.tuna.tsinghua.edu.cn/nodejs-release/">https://mirrors.tuna.tsinghua.edu.cn/nodejs-release/</a></p></li><li><p>初始化：     <code>npm init</code>      # 创建 <code>package.json</code> 文件:</p></li><li><p>安装软件包：<br><code>npm install</code>   # 安装 package.json 文件中列出的所有软件包和依赖<br><code>npm install 软件包[@版本号]</code>      # 安装指定（或默认）版本的软件包，并且将其添加到 <code>package.json</code> 文件的运行时列表中<br><code>npm install 软件包 --save-dev</code>    # 安装指定（或默认）版本的软件包，并且将其添加到 <code>package.json</code> 文件的开发时列表中<br><code>npm install --global 软件包</code>      # 全局安装<br><code>npm uninstall 软件包</code>             # 删除软件包，并将其从 <code>package.json</code> 的运行时列表中删除<br><code>npm list</code>                         # 显示已安装软件包列表</p></li></ul><p>注： npm 源可以根据网络情况自行选择，华中地区可以考虑中国科大。</p><h1 id="3-Hexo-博客环境配置"><a href="#3-Hexo-博客环境配置" class="headerlink" title="3 Hexo 博客环境配置"></a>3 Hexo 博客环境配置</h1><p>注：</p><p>（1）常见有 hexo，hugo， jecklly 等， 我已经习惯用 hexo 了，其他不熟。</p><p>（2）必须先安装 node.js ，版本需不低于 10.13，建议使用 Node.js 12.0 及以上版本</p><h2 id="3-1-安装-hexo-命令行工具"><a href="#3-1-安装-hexo-命令行工具" class="headerlink" title="3.1 安装 hexo 命令行工具"></a>3.1 安装 hexo 命令行工具</h2><p>运行：  <code>npm install -g hexo-cli</code></p><p>测试是否能用：  <code>hexo --version</code> </p><h2 id="3-2-创建博客本地工作目录"><a href="#3-2-创建博客本地工作目录" class="headerlink" title="3.2 创建博客本地工作目录"></a>3.2 创建博客本地工作目录</h2><p>运行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo init 工作目录名</span><br><span class="line">cd 工作目录名</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure><p>第一条命令会创建完整的目录结构，第三条命令会按照 <code>package.json</code> 文件自动安装所需 node 软件包</p><p>目录结构大致如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── _config.yml</span><br><span class="line">├── package.json</span><br><span class="line">├── scaffolds</span><br><span class="line">├── source</span><br><span class="line">|   ├── _drafts</span><br><span class="line">|   └── _posts</span><br><span class="line">└── themes</span><br></pre></td></tr></table></figure><p>上述目录中：</p><p>（1）<code>_config.yml</code>：   是网站配置文件，需要按需配置，配置方法见 <a href="https://hexo.io/zh-cn/docs/configuration">https://hexo.io/zh-cn/docs/configuration</a></p><p>（2）<code>package.json</code>： 所需软件包信息，无需手工改动</p><p>（3）<code>source</code> 目录：  所有源文件的存储地。其中 <code>_drafts</code> 文件夹中的内容是草稿，hexo 默认不会处理，<code>_posts</code> 目录中的 markdown 文件都会被处理。处理后的文件会自动存储在与 <code>source</code> 同级的 <code>public</code> 文件夹中。</p><p>（4）<code>themes</code> 目录：   存储网站所用的模板，<a href="https://hexo.io/themes/">https://hexo.io/themes/</a>  中有大量模板可用。</p><h2 id="3-3-创建博客"><a href="#3-3-创建博客" class="headerlink" title="3.3 创建博客"></a>3.3 创建博客</h2><p><strong>（1）markdown 文件的内容</strong></p><p>“FrontMatter” + “content”</p><ul><li><p>FrontMatter 参见： <a href="https://hexo.io/zh-cn/docs/front-matter">https://hexo.io/zh-cn/docs/front-matter</a> ，逐步会形成自己固定的FrontMatter，每次只需修改部分字段即可。</p></li><li><p>content 采用标准 markdown 即可。</p></li></ul><p><strong>（2）模板配置文件</strong></p><p>除了 <code>_config.yml</code> 配置文件外，不同网站模板自身还有一些配置需求，这通常放在网站的根目录下，命名为 <code>_config.主题名.yml</code>。不同主题的配置文件需参考相应的模板使用说明。</p><p><strong>（3）创建博客 markdown</strong></p><p>运行：<code>hexo new --path a/b/c &#39;黄金大镖客&#39; </code> 会自动在 <code>source/_posts</code> 目录下生成目录和文件 <code>a/b/c.md</code>， 并且博客标题为 <code>黄金大镖客</code> </p><p><strong>（4）撰写博客 markdown</strong></p><p>按需设置 frontmatter 各字段内容，按照 markdown 语法编写 content 部分的内容即可。</p><p>（5）构建博客</p><p>当博客编辑完成后，回到博客根目录，运行 <code>hexo generate</code> 或简写  <code>hexo g</code> 即自动开始生成网页，如果博客语法有错会有提示。</p><p>（6）本地测试博客</p><p>运行  <code>hexo service</code> 或简写 <code>hexo s</code>， 会启动本地的web服务器，并在 <code>https://localhost:4000</code> 提供博客网页服务，可用浏览器打开该网址查看博客效果。<br><code>hexo service</code> 支持动态构建，即任何对 markdown 的修改和保存，只要不出现语法等错误，都会自动生成网页并反映在浏览器中（需要手工刷新）</p><p>（7）部署到 github</p><p>运行 <code>hexo deploy</code>  或简写 <code>hexo d</code> ，会根据 <code>_config.yml</code> 中的配置自动将网站上传到 github 指定仓库的指定分支。配置方法参考 <a href="https://hexo.io/zh-cn/docs/github-pages#%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2">https://hexo.io/zh-cn/docs/github-pages#%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2</a></p><p>部署完成后，在浏览器中查看 <code>https://xxx.github.io/</code></p><p>技巧和提示：</p><p>（1） 因为博客就是一个 markdown 文件，所以不一定使用 <code>hexo new</code> 生成，可以直接手工编写。最快捷的方式是直接拷贝以前已经作好的 markdown 博客， rename 后修改 frontmatter 中的标题字段即可。<br>（2） 最好在 <code>source</code> 目录中设置合理的文件夹结构，以便区分不同类型的主题内容。<br>（3） <code>hexo</code> 会根据 frontmatter 中的 <code>categories</code> 字段自动生成网页目录和索引网页，也就是说，所有  <code>categories</code> 字段相同的博客，会在一个网页中被列表显示。因此，最好规划下自己的 <code>categories</code> 结构。<br>      一般  <code>categories</code>  只需要两层足已。<br>（4） frontmatter 中的 <code>tag</code> 字段其实就是网页的关键字，主要为了方便博客查找，可以随意设置，但通常是名词。<br>（5） 博客网页的仓库和博客源码的仓库尽量分开，博客网页的仓库是 public 的， 博客源码的仓库是 private 的。 <code>hexo deploy</code> 针对的是前者。<br>（6） 博客源码的 git 配置按照普通的仓库操作即可，但最好设置为 private。</p>]]></content>
      
      
      <categories>
          
          <category> 软件安装与使用 </category>
          
          <category> Linux </category>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安装与使用 </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nmcli网络配置命令</title>
      <link href="/vll-pages/posts/d3f42a0f.html"/>
      <url>/vll-pages/posts/d3f42a0f.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>nmcli 是 NetworkManager 的命令行工具。</p><p>nm 代表 NetworkManager，cli 代表 Command-Line 命令行。</p><h2 id="2-NetworkManager-服务"><a href="#2-NetworkManager-服务" class="headerlink" title="2 NetworkManager 服务"></a>2 NetworkManager 服务</h2><p>使用 nmcli 时，NetworkManager 必须保持开启。</p><p>NetworkManager 的相关命令：</p><ul><li>查看运行状态：systemctl status NetworkManager</li><li>启动：systemctl start NetworkManager</li><li>重启：systemctl restart NetworkManager</li><li>关闭：systemctl stop NetworkManager</li><li>查看是否开机启动：systemctl is-enabled NetworkManager</li><li>开机启动：systemctl enable NetworkManager</li><li>禁止开机启动：systemctl disable NetworkManager</li></ul><p><strong>注意：NetworkManager 中开头的 N 和中间的 M 必须大写。</strong></p><h2 id="3-nmcli-常用命令"><a href="#3-nmcli-常用命令" class="headerlink" title="3 nmcli 常用命令"></a>3 nmcli 常用命令</h2><p>下面仅介绍常用的命令，其它命令可以查看帮助文档。</p><h3 id="01-nmcli-networking"><a href="#01-nmcli-networking" class="headerlink" title="01. nmcli networking"></a>01. nmcli networking</h3><ul><li><p>显示 NetworkManager 是否接管网络设置：  nmcli networking</p><ul><li>networking 可以简写为 n、ne、net、netw…… 所以以上命令可以简写为： nmcli n</li></ul></li><li><p>查看网络连接状态： nmcli n connectivity</p><ul><li>网络连接状态共有五种：full、limited（连网，但无法上网）、portal（连网，但需要认证登录后才能上网）、none（没连网）和 unknown。</li><li>connectivity 可以简写为 c，所以以上命令可简写为： nmcli n c</li></ul></li><li><p>设定 NetworkManager 接管网络设置：  nmcli n on</p></li><li><p>取消 NetworkManager 接管网络设置：  nmcli n off</p></li></ul><h3 id="02-nmcli-general"><a href="#02-nmcli-general" class="headerlink" title="02. nmcli general"></a>02. nmcli general</h3><ul><li><p>显示系统网络状态：  nmcli general status</p><ul><li>general 可以简写为 g、ge、gen、gene……</li><li>status 是 general 的默认项，可以省略不写。所以，以上命令可简写为：   nmcli g</li></ul></li><li><p>显示主机名：   nmcli g hostname  或  nmcli g h</p></li><li><p>更改主机名：   nmcli g hostname newHostName  或   nmcli g h newHostName</p><ul><li>newHostName是你设置的新主机名。</li><li>主机名存放在 &#x2F;etc&#x2F;hostname 文件中。</li><li>修改主机名后，需要重启 NetworkManager。</li></ul></li></ul><h3 id="03-nmcli-connection"><a href="#03-nmcli-connection" class="headerlink" title="03. nmcli connection"></a>03. nmcli connection</h3><ul><li><p>显示所有网络连接的信息：    nmcli connection show</p><ul><li>connection 可以简写为 c、co、con、conn……</li><li>show 是 connection 的默认项，可以省略不写。所以，以上命令可简写为：  nmcli c</li><li>nmcli connection show 有一个 -active 参数，可以只显示当前启动的连接：    nmcli c s –active   或 nmcli c s -a        # 因为 show 后面有参数项，所以此时的 show 不能省</li></ul></li><li><p>显示某一特定连接的详细信息（以 ens33 为例）：  nmcli c s ens33</p></li><li><p>启动指定连接：  nmcli c up ens33</p><ul><li>如果 ens33 本来就出于连接状态，那此命令会重启 ens33。</li></ul></li><li><p>关闭指定连接：  nmcli c down ens33</p><ul><li>关闭连接后，使用 nmcli c 命令， DEVICE 项将显示为 – 。</li></ul></li><li><p>修改连接：   nmcli c modify ens33  [ + | - ]选项 选项值   或   nmcli c m ens33  [ + | - ]选项 选项值</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">下面给出常用修改示例：</span><br><span class="line"></span><br><span class="line">nmcli c m ens33 ipv4.address 192.168.80.10/24  # 修改 IP 地址和子网掩码</span><br><span class="line">nmcli c m ens33 ipv4.method manual             # 修改为静态配置，默认是 auto</span><br><span class="line">nmcli c m ens33 ipv4.gateway 192.168.80.2      # 修改默认网关</span><br><span class="line">nmcli c m ens33 ipv4.dns 192.168.80.2          # 修改 DNS</span><br><span class="line">nmcli c m ens33 +ipv4.dns 114.114.114.114      # 添加一个 DNS</span><br><span class="line">nmcli c m ens33 ipv6.method disabled           # 将 IPv6 禁用</span><br><span class="line">nmcli c m ens33 connection.autoconnect yes     # 开机启动</span><br><span class="line">注意，必须先修改 ipv4.address，然后才能修改 ipv4.method！</span><br><span class="line"></span><br><span class="line">用空引号&quot;&quot;代替选项的值，可将选项设回默认值（以 ipv4.method 为例）：</span><br><span class="line"></span><br><span class="line">nmcli c m ens33 ipv4.method &quot;&quot;</span><br><span class="line">选项有很多，详细信息可以通过以下命令查看：</span><br><span class="line"></span><br><span class="line">man 5 nm-settings-nmcli</span><br><span class="line">主要可以看里面 connection setting 和 ipv4 setting 部分。</span><br></pre></td></tr></table></figure><ul><li>新增连接：   nmcli c add type 连接类型 选项 选项值   或   nmcli c a type 连接类型 选项 选项值<br>type 为必选项，我们通常用到的是 802-3-ethernet（别名 ethernet）。下面给出一个示例：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nmcli c a type ethernet con-name ens36 ifname ens36</span><br></pre></td></tr></table></figure><ul><li><p>删除指定连接：   nmcli c delete ens33   或   nmcli c de ens33      </p><ul><li>delete 不可简写为 d，否则与 down 冲突，但可以简写为 de</li></ul></li><li><p>重载所有连接的配置文件：   nmcli c reload   或  nmcli c r</p></li><li><p>重载某一指定连接的配置文件：   nmcli c load ifcfg-ens33   或   nmcli c l ifcfg-ens33    </p><ul><li>网络配置文件默认保存在 &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F; 路径下，如果配置文件在其它位置，则需要填写完整路径。</li><li>网络配置文件的命名方式就是ifcfg-连接名，例如 ens33 的配置文件名为ifcfg-ens33。</li></ul></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">网络配置文件说明：</span><br><span class="line"></span><br><span class="line">TYPE=Ethernet           # 以太网</span><br><span class="line">PROXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=none          # 静态配置，等同于 ipv4.method manual</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=no</span><br><span class="line">NAME=ens33              # 连接名</span><br><span class="line">UUID=16862343-b55e-4248-b05d-a3ea662a84a4</span><br><span class="line">DEVICE=ens33            # 设备名</span><br><span class="line">ONBOOT=yes              # 开机自启</span><br><span class="line">IPADDR=192.168.80.10    # IP 地址</span><br><span class="line">PREFIX=24               # 子网掩码</span><br><span class="line">GATEWAY=192.168.80.2    # 网关</span><br><span class="line">DNS1=192.168.80.2       # DNS1</span><br><span class="line">DNS2=114.114.114.114    # DNS2</span><br><span class="line">建议使用 nmcli 命令来设置网络参数，不要直接修改此文件。</span><br></pre></td></tr></table></figure><h3 id="04-nmcli-device"><a href="#04-nmcli-device" class="headerlink" title="04. nmcli device"></a>04. nmcli device</h3><ul><li><p>显示所有网络接口设备的状态：   nmcli device status</p><ul><li>device 可以简写为 d、de、dev……</li><li>status 是 device 的默认项，可以省略不写。所以，以上命令可简写为：   nmcli d</li></ul></li><li><p>显示所有设备的详细信息：    nmcli d show  # 或    nmcli d sh    </p><ul><li>show 不可简写为 s，否则与 status 冲突，但可以简写为 sh</li></ul></li><li><p>显示某一特定设备的详细信息：  nmcli d sh ens33</p></li><li><p>连接设备：   nmcli d connect ens33   或  nmcli d c ens33  </p></li><li><p>如果 ens33 本来就出于连接状态，那此命令会重启 ens33。</p></li><li><p>断开设备：    nmcli d disconnect ens33   或    nmcli d d ens33  </p></li><li><p>更新设备信息：    nmcli d reapply ens33   或   nmcli d r ens33  </p><ul><li>只有在设备处于连接状态，才可以更新设备。</li><li>更新设备相当于重启连接。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 软件安装与使用 </category>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安装与使用 </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RIME输入法方案配置手册</title>
      <link href="/vll-pages/posts/41ac964d.html"/>
      <url>/vll-pages/posts/41ac964d.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h1 id="Rime-输入法方案配置手册"><a href="#Rime-输入法方案配置手册" class="headerlink" title="Rime 输入法方案配置手册"></a>Rime 输入法方案配置手册</h1><h2 id="一、-Schema-yaml-文件详解"><a href="#一、-Schema-yaml-文件详解" class="headerlink" title="一、 Schema.yaml 文件详解"></a>一、 <code>Schema.yaml</code> 文件详解</h2><h3 id="1-1-开始之前"><a href="#1-1-开始之前" class="headerlink" title="1.1 开始之前"></a>1.1 开始之前</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Rime schema</span><br><span class="line"># encoding: utf-8</span><br></pre></td></tr></table></figure><h3 id="1-2-描述档"><a href="#1-2-描述档" class="headerlink" title="1.2 描述档"></a>1.2 描述档</h3><ol><li><code>name:</code> 方案的显示名偁〔即出现于方案选单中以示人的，通常为中文〕</li><li><code>schema_id:</code> 方案内部名，在代码中引用此方案时以此名为正，通常由英文、数字、下划线组成</li><li><code>author:</code> 发明人、撰写者。如果您对方案做出了修改，请保留原作者名，并将自己的名字加在后面</li><li><code>description:</code> 请简要描述方案历史、码表来源、该方案规则等</li><li><code>dependencies:</code> 如果本方案依赖于其它方案〔通常来说会依頼其它方案做为反查，抑或是两种或多种方案混用时〕</li><li><code>version:</code> 版本号，在发布新版前请确保已陞版本号</li></ol><h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a><strong>示例</strong></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">schema:</span><br><span class="line">  name: &quot;苍颉检字法&quot;</span><br><span class="line">  schema_id: cangjie6</span><br><span class="line">  author:</span><br><span class="line">    - &quot;发明人 朱邦复先生、沉红莲女士&quot;</span><br><span class="line">  dependencies:</span><br><span class="line">    - luna_pinyin</span><br><span class="line">    - jyutping</span><br><span class="line">    - zyenpheng</span><br><span class="line">  description: |</span><br><span class="line">    第六代仓颉输入法</span><br><span class="line">    码表由雪斋、惜缘和crazy4u整理</span><br><span class="line">  version: 0.19</span><br></pre></td></tr></table></figure><h3 id="1-3-开关"><a href="#1-3-开关" class="headerlink" title="1.3 开关"></a>1.3 开关</h3><p>通常包含以下数个：</p><ol><li><code>ascii_mode</code> 是中英文转换开关。默认<code>0</code>为中文，<code>1</code>为英文</li><li><code>full_shape</code> 是全角符号／半角符号开关。注意，开启全角时英文字母亦为全角。<code>0</code>为半角，<code>1</code>为全角</li><li><code>extended_charset</code> 是字符集开关。<code>0</code>为CJK基本字符集，<code>1</code>为CJK全字符集</li></ol><ul><li>仅<code>table_translator</code>可用</li></ul><ol start="5"><li><code>ascii_punct</code> 是中西文标点转换开关，<code>0</code>为中文句读，<code>1</code>为西文标点。</li><li><code>simplification</code> 是转化字开关。一般情况下与上同，<code>0</code>为不开启转化，<code>1</code>为转化。</li></ol><ul><li><code>simplification</code>选项名偁可自定义，亦可添加多套替换用字方案：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- name: zh_cn</span><br><span class="line">  states: [&quot;汉字&quot;, &quot;汉字&quot;]</span><br><span class="line">  reset: 0</span><br></pre></td></tr></table></figure><p>或</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">- options: [ zh_trad, zh_cn, zh_mars ]</span><br><span class="line">  states:</span><br><span class="line">    - 字型 → 汉字</span><br><span class="line">    - 字型 → 汉字</span><br><span class="line">    - 字型 → 䕼茡</span><br><span class="line">  reset: 0</span><br></pre></td></tr></table></figure><ul><li><code>name</code>&#x2F;<code>options</code>名：须与<code>simplifier</code>中<code>option_name</code>相同</li><li><code>states</code>：可不写，如不写则此开关存在但不可见，可由快捷键操作</li><li><code>reset</code>：设置默认状态〔<code>reset</code>可不写，此时切换窗口时不会重置到默认状态〕</li></ul><ol start="9"><li>字符集过滤。此选项没有默认名偁，须配合<code>charset_filter</code>使用。可单用，亦可添加多套字符集：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- name: gbk</span><br><span class="line">  states: [ 增广, 常用 ]</span><br><span class="line">  reset: 0</span><br></pre></td></tr></table></figure><p>或</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">- options: [ utf-8, big5hkscs, big5, gbk, gb2312 ]</span><br><span class="line">  states:</span><br><span class="line">    - 字集 → 全</span><br><span class="line">    - 字集 → 港台</span><br><span class="line">    - 字集 → 台</span><br><span class="line">    - 字集 → 大陆</span><br><span class="line">    - 字集 → 简体</span><br><span class="line">  reset: 0</span><br></pre></td></tr></table></figure><ul><li><code>name</code>&#x2F;<code>options</code>名：须与<code>charset_filter``@</code>后的tag相同</li><li>避免同时使用字符集过滤和<code>extended_charset</code></li></ul><h4 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a><strong>示例</strong></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">switches:</span><br><span class="line">  - name: ascii_mode</span><br><span class="line">    reset: 0</span><br><span class="line">    states: [&quot;中文&quot;, &quot;西文&quot;]</span><br><span class="line">  - name: full_shape</span><br><span class="line">    states: [&quot;半角&quot;, &quot;全角&quot;]</span><br><span class="line">  - name: extended_charset</span><br><span class="line">    states: [&quot;通用&quot;, &quot;增广&quot;]</span><br><span class="line">  - name: simplification</span><br><span class="line">    states: [&quot;汉字&quot;, &quot;汉字&quot;]</span><br><span class="line">  - name: ascii_punct</span><br><span class="line">    states: [&quot;句读&quot;, &quot;符号&quot;]</span><br></pre></td></tr></table></figure><h3 id="1-4-引擎"><a href="#1-4-引擎" class="headerlink" title="1.4 引擎"></a>1.4 引擎</h3><ul><li>以下<strong>加粗</strong>项为可细配者，_斜体_者为不常用者</li></ul><p>引擎分四组：</p><h4 id="一、processors"><a href="#一、processors" class="headerlink" title="一、processors"></a>一、<code>processors</code></h4><ul><li>这批组件处理各类按键消息</li></ul><ol><li><code>ascii_composer</code> 处理西文模式及中西文切</li><li><strong><code>recognizer</code></strong> 与<code>matcher</code>搭配，处理符合特定规则的输入码，如网址、反查等<code>tags</code></li><li><strong><code>key_binder</code></strong> 在特定条件下将按键绑定到其他按键，如重定义逗号、句号为候选翻页、开关快捷键等</li><li><strong><code>speller</code></strong> 拼写处理器，接受字符按键，编辑输入</li><li><strong><code>punctuator</code></strong> 句读处理器，将单个字符按键直接映射为标点符号或文字</li><li><code>selector</code> 选字处理器，处理数字选字键〔可以换成别的哦〕、上、下候选定位、换页</li><li><code>navigator</code> 处理输入栏内的光标移动</li><li><code>express_editor</code> 编辑器，处理空格、回车上屏、回退键</li><li><em><code>fluid_editor</code></em> 句式编辑器，用于以空格断词、回车上屏的【注音】、【语句流】等输入方案，替换<code>express_editor</code></li><li><em><code>chord_composer</code></em> 和絃作曲家或曰并击处理器，用于【宫保拼音】等多键并击的输入方案</li><li><code>lua_processor</code> 使用<code>lua</code>自定义按键，后接<code>@</code>+<code>lua</code>函数名</li></ol><ul><li><code>lua</code>函数名即用户文件夹内<code>rime.lua</code>中函数名，参数为<code>(key, env)</code></li></ul><h4 id="二、segmentors"><a href="#二、segmentors" class="headerlink" title="二、segmentors"></a>二、<code>segmentors</code></h4><ul><li>这批组件识别不同内容类型，将输入码分段并加上<code>tag</code></li></ul><ol><li><code>ascii_segmentor</code> 标识西文段落〔譬如在西文模式下〕字母直接上屛</li><li><code>matcher</code> 配合<code>recognizer</code>标识符合特定规则的段落，如网址、反查等，加上特定<code>tag</code></li><li><strong><code>abc_segmentor</code></strong> 标识常规的文字段落，加上<code>abc</code>这个<code>tag</code></li><li><code>punct_segmentor</code> 标识句读段落〔键入标点符号用〕加上<code>punct</code>这个<code>tag</code></li><li><code>fallback_segmentor</code> 标识其他未标识段落</li><li><strong><code>affix_segmentor</code></strong> 用户自定义<code>tag</code></li></ol><ul><li>此项可加载多个实例，后接<code>@</code>+<code>tag</code>名</li></ul><ol start="8"><li><em><code>lua_segmentor</code></em> 使用<code>lua</code>自定义切分，后接<code>@</code>+<code>lua</code>函数名</li></ol><h4 id="三、translators"><a href="#三、translators" class="headerlink" title="三、translators"></a>三、<code>translators</code></h4><ul><li>这批组件翻译特定类型的编码段为一组候选文字</li></ul><ol><li><code>echo_translator</code> 没有其他候选字时，回显输入码〔输入码可以<code>Shift</code>+<code>Enter</code>上屛〕</li><li><code>punct_translator</code> 配合<code>punct_segmentor</code>转换标点符号</li><li><strong><code>table_translator</code></strong> 码表翻译器，用于仓颉、五笔等基于码表的输入方案<br>- 此项可加载多个实例，后接<code>@</code>+翻译器名〔如：<code>cangjie</code>、<code>wubi</code>等〕7. <strong><code>script_translator</code></strong> 脚本翻译器，用于拼音、粤拼等基于音节表的输入方案<br>- 此项可加载多个实例，后接<code>@</code>+翻译器名〔如：<code>pinyin</code>、<code>jyutping</code>等〕11. <em><code>reverse_lookup_translator</code></em> 反查翻译器，用另一种编码方案查码</li><li><strong><code>lua_translator</code></strong> 使用<code>lua</code>自定义输入，例如动态输入当前日期、时间，后接<code>@</code>+<code>lua</code>函数名</li></ol><ul><li><code>lua</code>函数名即用户文件夹内<code>rime.lua</code>中函数名，参数为<code>(input, seg, env)</code></li><li>可以<code>env.engine.context:get_option(&quot;option_name&quot;)</code>方式绑定到<code>switch</code>开关／<code>key_binder</code>快捷键</li></ul><h4 id="四、filters"><a href="#四、filters" class="headerlink" title="四、filters"></a>四、<code>filters</code></h4><ul><li>这批组件过滤翻译的结果</li></ul><ol><li><code>uniquifier</code> 过滤重复的候选字，有可能来自**<code>simplifier</code>**</li><li><code>cjk_minifier</code> 字符集过滤〔仅用于<code>script_translator</code>，使之支持<code>extended_charset</code>开关〕</li><li><strong><code>single_char_filter</code></strong> 单字过滤器，如加载此组件，则屛敝词典中的词组〔仅<code>table_translator</code>有效〕</li><li><strong><code>simplifier</code></strong> 用字转换</li><li><strong><code>reverse_lookup_filter</code></strong> 反查滤镜，以更灵活的方式反查，Rime1.0后替代_<code>reverse_lookup_translator</code>_</li></ol><ul><li>此项可加载多个实例，后接<code>@</code>+滤镜名〔如：<code>pinyin_lookup</code>、<code>jyutping_lookup</code>等〕</li></ul><ol start="7"><li><strong><code>charset_filter</code></strong> 字符集过滤</li></ol><ul><li>后接<code>@</code>+字符集名〔如：<code>utf-8</code>(无过滤)、<code>big5</code>、<code>big5hkscs</code>、<code>gbk</code>、<code>gb2312</code>〕</li></ul><ol start="9"><li><strong><code>lua_filter</code></strong> 使用<code>lua</code>自定义过滤，例如过滤字符集、调整排序，后接<code>@</code>+<code>lua</code>函数名</li></ol><ul><li><code>lua</code>函数名即用户文件夹内<code>rime.lua</code>中函数名，参数为<code>(input, env)</code></li><li>可以<code>env.engine.context:get_option(&quot;option_name&quot;)</code>方式绑定到<code>switch</code>开关／<code>key_binder</code>快捷键</li></ul><h4 id="示例-2"><a href="#示例-2" class="headerlink" title="示例"></a><strong>示例</strong></h4><p>cangjie6.schema.yaml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">engine:</span><br><span class="line">  processors:</span><br><span class="line">    - ascii_composer</span><br><span class="line">    - recognizer</span><br><span class="line">    - key_binder</span><br><span class="line">    - speller</span><br><span class="line">    - punctuator</span><br><span class="line">    - selector</span><br><span class="line">    - navigator</span><br><span class="line">    - express_editor</span><br><span class="line">  segmentors:</span><br><span class="line">    - ascii_segmentor</span><br><span class="line">    - matcher</span><br><span class="line">    - affix_segmentor@pinyin</span><br><span class="line">    - affix_segmentor@jyutping</span><br><span class="line">    - affix_segmentor@pinyin_lookup</span><br><span class="line">    - affix_segmentor@jyutping_lookup</span><br><span class="line">    - affix_segmentor@reverse_lookup</span><br><span class="line">    - abc_segmentor</span><br><span class="line">    - punct_segmentor</span><br><span class="line">    - fallback_segmentor</span><br><span class="line">  translators:</span><br><span class="line">    - punct_translator</span><br><span class="line">    - table_translator</span><br><span class="line">    - script_translator@pinyin</span><br><span class="line">    - script_translator@jyutping</span><br><span class="line">    - script_translator@pinyin_lookup</span><br><span class="line">    - script_translator@jyutping_lookup</span><br><span class="line">    - lua_translator@get_date</span><br><span class="line">  filters:</span><br><span class="line">    - simplifier@zh_simp</span><br><span class="line">    - uniquifier</span><br><span class="line">    - cjk_minifier</span><br><span class="line">    - charset_filter@gbk</span><br><span class="line">    - reverse_lookup_filter@middle_chinese</span><br><span class="line">    - reverse_lookup_filter@pinyin_reverse_lookup</span><br><span class="line">    - reverse_lookup_filter@jyutping_reverse_lookup</span><br><span class="line">    - lua_filter@single_char_first</span><br></pre></td></tr></table></figure><h3 id="1-5-细项配置"><a href="#1-5-细项配置" class="headerlink" title="1.5 细项配置"></a>1.5 细项配置</h3><ul><li>凡<code>comment_format</code>、<code>preedit_format</code>、<code>speller/algebra</code>所用之正则表达式，请参阅<a href="http://www.boost.org/doc/libs/1_49_0/libs/regex/doc/html/boost_regex/syntax/perl_syntax.html">「Perl正则表达式」</a></li></ul><p><strong>引擎中所举之加粗者均可在下方详细描述，格式为：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">name:</span><br><span class="line">  branches: configurations</span><br></pre></td></tr></table></figure><p>或</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name:</span><br><span class="line">  branches:</span><br><span class="line">    - configurations</span><br></pre></td></tr></table></figure><h4 id="一、speller"><a href="#一、speller" class="headerlink" title="一、speller"></a>一、<code>speller</code></h4><ol><li><code>alphabet:</code> 定义本方案输入键</li><li><code>initials:</code> 定义仅作始码之键</li><li><code>finals:</code> 定义仅作末码之键</li><li><code>delimiter:</code> 上屛时的音节间分音符</li><li><code>algebra:</code> 拼写运算规则，由之算出的拼写汇入<code>prism</code>中</li><li><code>max_code_length:</code> 形码最大码长，超过则顶字上屛〔<code>number</code>〕</li><li><code>auto_select:</code> 自动上屛〔<code>true</code>或<code>false</code>〕</li><li><code>auto_select_pattern:</code> 自动上屏规则，以正则表达式描述，当输入串可以被匹配时自动顶字上屏。</li><li><code>use_space:</code> 以空格作输入码〔<code>true</code>或<code>false</code>〕</li></ol><ul><li><code>speller</code>的演算包含：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">xform --改写〔不保留原形〕</span><br><span class="line">derive --衍生〔保留原形〕</span><br><span class="line">abbrev --简拼〔出字优先级较上两组更低〕</span><br><span class="line">fuzz --畧拼〔此种简拼仅组词，不出单字〕</span><br><span class="line">xlit --变换〔适合大量一对一变换〕</span><br><span class="line">erase --删除</span><br></pre></td></tr></table></figure><h5 id="示例-3"><a href="#示例-3" class="headerlink" title="示例"></a><strong>示例</strong></h5><p>luna_pinyin.schema.yaml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">speller:</span><br><span class="line">  alphabet: zyxwvutsrqponmlkjihgfedcba</span><br><span class="line">  delimiter: &quot; &#x27;&quot;</span><br><span class="line">  algebra:</span><br><span class="line">    - erase/^xx$/</span><br><span class="line">    - abbrev/^([a-z]).+$/$1/</span><br><span class="line">    - abbrev/^([zcs]h).+$/$1/</span><br><span class="line">    - derive/^([nl])ve$/$1ue/</span><br><span class="line">    - derive/^([jqxy])u/$1v/</span><br><span class="line">    - derive/un$/uen/</span><br><span class="line">    - derive/ui$/uei/</span><br><span class="line">    - derive/iu$/iou/</span><br><span class="line">    - derive/([aeiou])ng$/$1gn/</span><br><span class="line">    - derive/([dtngkhrzcs])o(u|ng)$/$1o/</span><br><span class="line">    - derive/ong$/on/</span><br><span class="line">    - derive/ao$/oa/</span><br><span class="line">    - derive/([iu])a(o|ng?)$/a$1$2/</span><br></pre></td></tr></table></figure><h4 id="二、segmentor"><a href="#二、segmentor" class="headerlink" title="二、segmentor"></a>二、<code>segmentor</code></h4><ul><li><code>segmentor</code>配合<code>recognizer</code>标记出<code>tag</code>。这里会用到<code>affix_segmentor</code>和<code>abc_translator</code></li><li><code>tag</code>用在<code>translator</code>、<code>reverse_lookup_filter</code>、<code>simplifier</code>中用以标定各自作用范围</li><li>如果不需要用到<code>extra_tags</code>则不需要单独配置<code>segmentor</code></li></ul><ol><li><code>tag:</code> 设置其<code>tag</code></li><li><code>prefix:</code> 设置其前缀标识，可不塡，不塡则无前缀</li><li><code>suffix:</code> 设置其尾缀标识，可不塡，不塡则无尾缀</li><li><code>tips:</code> 设置其输入前提示符，可不塡，不塡则无提示符</li><li><code>closing_tips:</code> 设置其结束输入提示符，可不塡，不塡则无提示符</li><li><code>extra_tags:</code> 为此<code>segmentor</code>所标记的段落插上其它<code>tag</code></li></ol><p><strong>当<code>affix_segmentor</code>和<code>translator</code>重名时，两者可併在一处配置，此处1-5条对应下面19-23条。<code>abc_segmentor</code>仅可设<code>extra_tags</code></strong></p><h5 id="示例-4"><a href="#示例-4" class="headerlink" title="示例"></a><strong>示例</strong></h5><p>cangjie6.schema.yaml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">reverse_lookup:</span><br><span class="line">  tag: reverse_lookup</span><br><span class="line">  prefix: &quot;`&quot;</span><br><span class="line">  suffix: &quot;;&quot;</span><br><span class="line">  tips: &quot;【反查】&quot;</span><br><span class="line">  closing_tips: &quot;【苍颉】&quot;</span><br><span class="line">  extra_tags:</span><br><span class="line">    - pinyin_lookup</span><br><span class="line">    - jyutping_lookup</span><br></pre></td></tr></table></figure><h4 id="三、translator"><a href="#三、translator" class="headerlink" title="三、translator"></a>三、<code>translator</code></h4><ul><li>每个方案有一个主<code>translator</code>，在引擎列表中不以<code>@</code>+翻译器名定义，在细项配置时直接以<code>translator:</code>命名。以下加粗项为可在主<code>translator</code>中定义之项，其它可在副〔以<code>@</code>+翻译器名命名〕<code>translator</code>中定义</li></ul><ol><li><strong><code>enable_charset_filter:</code></strong> 是否开启字符集过滤〔仅<code>table_translator</code>有效。启用<code>cjk_minifier</code>后可适用于<code>script_translator</code>〕</li><li><strong><code>enable_encoder:</code></strong> 是否开启自动造词〔仅<code>table_translator</code>有效〕</li><li><strong><code>encode_commit_history:</code></strong> 是否对已上屛词自动成词〔仅<code>table_translator</code>有效〕</li><li><strong><code>max_phrase_length:</code></strong> 最大自动成词词长〔仅<code>table_translator</code>有效〕</li><li><strong><code>enable_completion:</code></strong> 提前显示尚未输入完整码的字〔仅<code>table_translator</code>有效〕</li><li><strong><code>sentence_over_completion:</code></strong> 在无全码对应字而仅有逐键提示时也开启智能组句〔仅<code>table_translator</code>有效〕</li><li><strong><code>strict_spelling:</code></strong> 配合<code>speller</code>中的<code>fuzz</code>规则，仅以畧拼码组词〔仅<code>table_translator</code>有效〕</li><li><strong><code>disable_user_dict_for_patterns:</code></strong> 禁止某些编码录入用户词典</li><li><strong><code>enable_sentence:</code></strong> 是否开启自动造句</li><li><strong><code>enable_user_dict:</code></strong> 是否开启用户词典〔用户词典记录动态字词频、用户词〕</li></ol><ul><li>以上选塡<code>true</code>或<code>false</code></li></ul><ol start="12"><li><strong><code>dictionary:</code></strong> 翻译器将调取此字典文件</li><li><strong><code>prism:</code></strong> 设置由此主翻译器的<code>speller</code>生成的棱镜文件名，或此副编译器调用的棱镜名</li><li><strong><code>user_dict:</code></strong> 设置用户词典名</li><li><strong><code>db_class:</code></strong> 设置用户词典类型，可设<code>tabledb</code>〔文本〕或<code>userdb</code>〔二进制〕</li><li><strong><code>preedit_format:</code></strong> 上屛码自定义</li><li><strong><code>comment_format:</code></strong> 提示码自定义</li><li><strong><code>spelling_hints:</code></strong> 设置多少字以内候选标注完整带调拼音〔仅<code>script_translator</code>有效〕</li><li><strong><code>initial_quality:</code></strong> 设置此翻译器出字优先级</li><li><code>tag:</code> 设置此翻译器针对的<code>tag</code>。可不塡，不塡则仅针对<code>abc</code></li><li><code>prefix:</code> 设置此翻译器的前缀标识，可不塡，不塡则无前缀</li><li><code>suffix:</code> 设置此翻译器的尾缀标识，可不塡，不塡则无尾缀</li><li><code>tips:</code> 设置此翻译器的输入前提示符，可不塡，不塡则无提示符</li><li><code>closing_tips:</code> 设置此翻译器的结束输入提示符，可不塡，不塡则无提示符</li><li><code>contextual_suggestions:</code> 是否使用语言模型优化输出结果〔需配合<code>grammar</code>使用〕</li><li><code>max_homophones:</code> 最大同音簇长度〔需配合<code>grammar</code>使用〕</li><li><code>max_homographs:</code> 最大同形簇长度〔需配合<code>grammar</code>使用〕</li></ol><h5 id="示例-5"><a href="#示例-5" class="headerlink" title="示例"></a><strong>示例</strong></h5><p>cangjie6.schema.yaml 苍颉主翻译器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">translator:</span><br><span class="line">  dictionary: cangjie6</span><br><span class="line">  enable_charset_filter: true</span><br><span class="line">  enable_sentence: true</span><br><span class="line">  enable_encoder: true</span><br><span class="line">  encode_commit_history: true</span><br><span class="line">  max_phrase_length: 5</span><br><span class="line">  preedit_format:</span><br><span class="line">    - xform/^([a-z ])$/$1｜\U$1\E/</span><br><span class="line">    - xform/(?&lt;=[a-z])\s(?=[a-z])//</span><br><span class="line">    - &quot;xlit|ABCDEFGHIJKLMNOPQRSTUVWXYZ|日月金木水火土竹戈十大中一弓人心手口尸廿山女田止卜片|&quot;</span><br><span class="line">  comment_format:</span><br><span class="line">    - &quot;xlit|abcdefghijklmnopqrstuvwxyz~|日月金木水火土竹戈十大中一弓人心手口尸廿山女田止卜片・|&quot;</span><br><span class="line">  disable_user_dict_for_patterns:</span><br><span class="line">    - &quot;^z.$&quot;</span><br><span class="line">  initial_quality: 0.75</span><br></pre></td></tr></table></figure><p>cangjie6.schema.yaml 拼音副翻译器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">pinyin:</span><br><span class="line">  tag: pinyin</span><br><span class="line">  dictionary: luna_pinyin</span><br><span class="line">  enable_charset_filter: true</span><br><span class="line">  prefix: &#x27;P&#x27; #须配合recognizer</span><br><span class="line">  suffix: &#x27;;&#x27; #须配合recognizer</span><br><span class="line">  preedit_format:</span><br><span class="line">    - &quot;xform/([nl])v/$1ü/&quot;</span><br><span class="line">    - &quot;xform/([nl])ue/$1üe/&quot;</span><br><span class="line">    - &quot;xform/([jqxy])v/$1u/&quot;</span><br><span class="line">  tips: &quot;【汉拼】&quot;</span><br><span class="line">  closing_tips: &quot;【苍颉】&quot;</span><br></pre></td></tr></table></figure><p>pinyin_simp.schema.yaml 拼音・简化字主翻译器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">translator:</span><br><span class="line">  dictionary: luna_pinyin</span><br><span class="line">  prism: luna_pinyin_simp</span><br><span class="line">  preedit_format:</span><br><span class="line">    - xform/([nl])v/$1ü/</span><br><span class="line">    - xform/([nl])ue/$1üe/</span><br><span class="line">    - xform/([jqxy])v/$1u/</span><br></pre></td></tr></table></figure><p>luna_pinyin.schema.yaml 朙月拼音用户短语</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">custom_phrase: #这是一个table_translator</span><br><span class="line">  dictionary: &quot;&quot;</span><br><span class="line">  user_dict: custom_phrase</span><br><span class="line">  db_class: tabledb</span><br><span class="line">  enable_sentence: false</span><br><span class="line">  enable_completion: false</span><br><span class="line">  initial_quality: 1</span><br></pre></td></tr></table></figure><h4 id="四、reverse-lookup-filter"><a href="#四、reverse-lookup-filter" class="headerlink" title="四、reverse_lookup_filter"></a>四、<code>reverse_lookup_filter</code></h4><ul><li>此滤镜须挂在<code>translator</code>上，不影响该<code>translator</code>工作</li></ul><ol><li><code>tags:</code> 设置其作用范围</li><li><code>overwrite_comment:</code> 是否覆盖其他提示</li><li><code>dictionary:</code> 反查所得提示码之码表</li><li><code>comment_format:</code> 自定义提示码格式</li></ol><h5 id="示例-6"><a href="#示例-6" class="headerlink" title="示例"></a><strong>示例</strong></h5><p>cangjie6.schema.yaml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pinyin_reverse_lookup: #该反查滤镜名</span><br><span class="line">  tags: [ pinyin_lookup ] #挂在这个tag所对应的翻译器上</span><br><span class="line">  overwrite_comment: true</span><br><span class="line">  dictionary: cangjie6 #反查所得为苍颉码</span><br><span class="line">  comment_format:</span><br><span class="line">    - &quot;xform/$/〕/&quot;</span><br><span class="line">    - &quot;xform/^/〔/&quot;</span><br><span class="line">    - &quot;xlit|abcdefghijklmnopqrstuvwxyz |日月金木水火土竹戈十大中一弓人心手口尸廿山女田止卜片、|&quot;</span><br></pre></td></tr></table></figure><h4 id="五、simplifier"><a href="#五、simplifier" class="headerlink" title="五、simplifier"></a>五、<code>simplifier</code></h4><ol><li><code>option_name:</code> 对应<code>switches</code>中设置的切换项名</li><li><code>opencc_config:</code> 用字转换配置文件</li></ol><ul><li>位于：<code>rime_dir/opencc/</code>，自带之配置文件含：</li></ul><ol><li><p>繁转简〔默认〕：<code>t2s.json</code></p></li><li><p>繁转台湾：<code>t2tw.json</code></p></li><li><p>繁转香港：<code>t2hk.json</code></p></li><li><p>简转繁：<code>s2t.json</code></p></li><li><p><code>tags:</code> 设置转换范围</p></li><li><p><code>tips:</code> 设置是否提示转换前的字，可塡<code>none</code>〔或不塡〕、<code>char</code>〔仅对单字有效〕、<code>all</code></p></li><li><p><code>show_in_comment:</code> 设置是否仅将转换结果显示在备注中</p></li><li><p><em><code>excluded_types:</code></em> 取消特定范围〔一般为_<code>reverse_lookup_translator</code>_〕转化用字</p></li></ol><h5 id="示例-7"><a href="#示例-7" class="headerlink" title="示例"></a><strong>示例</strong></h5><p>修改自 luna_pinyin_kunki.schema</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zh_tw:</span><br><span class="line">  option_name: zh_tw</span><br><span class="line">  opencc_config: t2tw.json</span><br><span class="line">  tags: [ abc ] #abc对应abc_segmentor</span><br><span class="line">  tips: none</span><br></pre></td></tr></table></figure><h4 id="六、chord-composer"><a href="#六、chord-composer" class="headerlink" title="六、chord_composer_"></a>六、<code>chord_composer</code>_</h4><ul><li>并击把键盘分两半，相当于两块键盘。两边同时击键，系统默认在其中一半上按的键先于另一半，由此得出上屛码</li></ul><ol><li><code>alphabet:</code> 字母表，包含用于并击的按键。击键虽有先后，形成并击时，一律以字母表顺序排列</li><li><code>algebra:</code> 拼写运算规则，将一组并击编码转换为拼音音节</li><li><code>output_format:</code> 并击完成后套用的式样，追加隔音符号</li><li><code>prompt_format:</code> 并击过程中套用的式样，加方括弧</li></ol><h5 id="示例-8"><a href="#示例-8" class="headerlink" title="示例"></a><strong>示例</strong></h5><p>combo_pinyin.schema.yaml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">chord_composer:</span><br><span class="line">  # 字母表，包含用于并击的按键</span><br><span class="line">  # 击键虽有先后，形成并击时，一律以字母表顺序排列</span><br><span class="line">  alphabet: &quot;swxdecfrvgtbnjum ki,lo.&quot;</span><br><span class="line">  # 拼写运算规则，将一组并击编码转换为拼音音节</span><br><span class="line">  algebra:</span><br><span class="line">    # 先将物理按键字符对应到宫保拼音键位中的拼音字母</span><br><span class="line">    - &#x27;xlit|swxdecfrvgtbnjum ki,lo.|sczhlfgdbktpRiuVaNIUeoE|&#x27;</span><br><span class="line">    # 以下根据宫保拼音的键位分别变换声母、韵母部分</span><br><span class="line">    # 组合声母</span><br><span class="line">    - xform/^zf/zh/</span><br><span class="line">    - xform/^cl/ch/</span><br><span class="line">    - xform/^fb/m/</span><br><span class="line">    - xform/^ld/n/</span><br><span class="line">    - xform/^hg/r/</span><br><span class="line">    ……</span><br><span class="line">    # 声母独用时补足隠含的韵母</span><br><span class="line">    - xform/^([bpf])$/$1u/</span><br><span class="line">    - xform/^([mdtnlgkh])$/$1e/</span><br><span class="line">    - xform/^([mdtnlgkh])$/$1e/</span><br><span class="line">    - xform/^([zcsr]h?)$/$1i/</span><br><span class="line">  # 并击完成后套用的式样，追加隔音符号</span><br><span class="line">  output_format:</span><br><span class="line">    - &quot;xform/^([a-z]+)$/$1&#x27;/&quot;</span><br><span class="line">  # 并击过程中套用的式样，加方括弧</span><br><span class="line">  prompt_format:</span><br><span class="line">    - &quot;xform/^(.*)$/[$1]/&quot;</span><br></pre></td></tr></table></figure><h4 id="七、lua"><a href="#七、lua" class="headerlink" title="七、lua"></a>七、<code>lua</code></h4><ul><li>请参攷<a href="https://github.com/hchunhui/librime-lua">hchunhui&#x2F;librime-lua</a> 以寻求更多灵感。</li></ul><ol><li><code>lua_translator</code></li><li><code>lua_filter</code></li><li><code>lua_processor</code></li><li><code>lua_segmentor</code></li></ol><h5 id="示例-9"><a href="#示例-9" class="headerlink" title="示例"></a><strong>示例</strong></h5><p>rime.lua</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">function get_date(input, seg, env)</span><br><span class="line">  --- 以 show_date 为开关名或 key_binder 中 toggle 的对象</span><br><span class="line">  on = env.engine.context:get_option(&quot;show_date&quot;)</span><br><span class="line">  if (on and input == &quot;date&quot;) then</span><br><span class="line">    --- Candidate(type, start, end, text, comment)</span><br><span class="line">    yield(Candidate(&quot;date&quot;, seg.start, seg._end, os.date(&quot;%Y年%m月%d日&quot;), &quot; 日期&quot;))</span><br><span class="line">  end</span><br><span class="line">end</span><br></pre></td></tr></table></figure><h4 id="八、其它"><a href="#八、其它" class="headerlink" title="八、其它"></a>八、其它</h4><ul><li>包括<code>recognizer</code>、<code>key_binder</code>、<code>punctuator</code>。<strong>标点</strong>、<strong>快捷键</strong>、<strong>二三选重</strong>、<strong>特殊字符</strong>等均于此设置</li></ul><ol><li><p><strong><code>import_preset:</code></strong> 由外部统一文件导入</p></li><li><p><code>grammar:</code> 下设：</p><ul><li><code>language:</code> 取值<code>zh-han[ts]-t-essay-bg[wc]</code></li><li><code>collocation_max_length:</code> 最大搭配长度（整句输入可忽畧此项）</li><li><code>collocation_min_length:</code> 最小搭配长度（整句输入可忽畧此项）</li></ul></li><li><p><code>recognizer:</code> 下设<code>patterns:</code> 配合<code>segmentor</code>的<code>prefix</code>和<code>suffix</code>完成段落划分、<code>tag</code>分配</p><ul><li>前字段可以为以<code>affix_segmentor@someTag</code>定义的<code>Tag</code>名，或者<code>punct</code>、<code>reverse_lookup</code>两个内设的字段。其它字段不调用输入法引擎，输入即输出〔如<code>url</code>等字段〕</li></ul></li><li><p><code>key_binder:</code> 下设<code>bindings:</code> 设置功能性快捷键</p><ul><li><p>每一条<code>binding</code>可能包含：<code>accept</code>实际所按之键、<code>send</code>输出效果、<code>toggle</code>切换开关和<code>when</code>作用范围〔<code>send</code>和<code>toggle</code>二选一〕</p></li><li><p><code>toggle</code>可用字段包含各开关名</p></li><li><p><code>when</code>可用字段包含：</p></li></ul> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">paging翻䈎用</span><br><span class="line">has_menu操作候选项用</span><br><span class="line">composing操作输入码用</span><br><span class="line">always全域</span><br></pre></td></tr></table></figure><ul><li><code>accept</code>和<code>send</code>可用字段除A-Za-z0-9外，还包含以下键板上实际有的键：</li></ul> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line">BackSpace退格</span><br><span class="line">Tab水平定位符</span><br><span class="line">Linefeed换行</span><br><span class="line">Clear清除</span><br><span class="line">Return回车</span><br><span class="line">Pause暂停</span><br><span class="line">Sys_Req印屏</span><br><span class="line">Escape退出</span><br><span class="line">Delete删除</span><br><span class="line">Home原位</span><br><span class="line">Left左箭头</span><br><span class="line">Up上箭头</span><br><span class="line">Right右箭头</span><br><span class="line">Down下箭头</span><br><span class="line">Prior、Page_Up上翻</span><br><span class="line">Next、Page_Down下翻</span><br><span class="line">End末位</span><br><span class="line">Begin始位</span><br><span class="line">Shift_L左Shift</span><br><span class="line">Shift_R右Shift</span><br><span class="line">Control_L左Ctrl</span><br><span class="line">Control_R右Ctrl</span><br><span class="line">Meta_L左Meta</span><br><span class="line">Meta_R右Meta</span><br><span class="line">Alt_L左Alt</span><br><span class="line">Alt_R右Alt</span><br><span class="line">Super_L左Super</span><br><span class="line">Super_R右Super</span><br><span class="line">Hyper_L左Hyper</span><br><span class="line">Hyper_R右Hyper</span><br><span class="line">Caps_Lock大写锁</span><br><span class="line">Shift_Lock上档锁</span><br><span class="line">Scroll_Lock滚动锁</span><br><span class="line">Num_Lock小键板锁</span><br><span class="line">Select选定</span><br><span class="line">Print打印</span><br><span class="line">Execute执行</span><br><span class="line">Insert插入</span><br><span class="line">Undo还原</span><br><span class="line">Redo重做</span><br><span class="line">Menu菜单</span><br><span class="line">Find蒐寻</span><br><span class="line">Cancel取消</span><br><span class="line">Help帮助</span><br><span class="line">Break中断</span><br><span class="line">space</span><br><span class="line">exclam!</span><br><span class="line">quotedbl&quot;</span><br><span class="line">numbersign#</span><br><span class="line">dollar$</span><br><span class="line">percent%</span><br><span class="line">ampersand&amp;</span><br><span class="line">apostrophe&#x27;</span><br><span class="line">parenleft(</span><br><span class="line">parenright)</span><br><span class="line">asterisk*</span><br><span class="line">plus+</span><br><span class="line">comma,</span><br><span class="line">minus-</span><br><span class="line">period.</span><br><span class="line">slash/</span><br><span class="line">colon:</span><br><span class="line">semicolon;</span><br><span class="line">less&lt;</span><br><span class="line">equal=</span><br><span class="line">greater&gt;</span><br><span class="line">question?</span><br><span class="line">at@</span><br><span class="line">bracketleft[</span><br><span class="line">backslash</span><br><span class="line">bracketright]</span><br><span class="line">asciicircum^</span><br><span class="line">underscore_</span><br><span class="line">grave`</span><br><span class="line">braceleft&#123;</span><br><span class="line">bar|</span><br><span class="line">braceright&#125;</span><br><span class="line">asciitilde~</span><br><span class="line">KP_Space小键板空格</span><br><span class="line">KP_Tab小键板水平定位符</span><br><span class="line">KP_Enter小键板回车</span><br><span class="line">KP_Delete小键板删除</span><br><span class="line">KP_Home小键板原位</span><br><span class="line">KP_Left小键板左箭头</span><br><span class="line">KP_Up小键板上箭头</span><br><span class="line">KP_Right小键板右箭头</span><br><span class="line">KP_Down小键板下箭头</span><br><span class="line">KP_Prior、KP_Page_Up小键板上翻</span><br><span class="line">KP_Next、KP_Page_Down小键板下翻</span><br><span class="line">KP_End小键板末位</span><br><span class="line">KP_Begin小键板始位</span><br><span class="line">KP_Insert小键板插入</span><br><span class="line">KP_Equal小键板等于</span><br><span class="line">KP_Multiply小键板乘号</span><br><span class="line">KP_Add小键板加号</span><br><span class="line">KP_Subtract小键板减号</span><br><span class="line">KP_Divide小键板除号</span><br><span class="line">KP_Decimal小键板小数点</span><br><span class="line">KP_0小键板0</span><br><span class="line">KP_1小键板1</span><br><span class="line">KP_2小键板2</span><br><span class="line">KP_3小键板3</span><br><span class="line">KP_4小键板4</span><br><span class="line">KP_5小键板5</span><br><span class="line">KP_6小键板6</span><br><span class="line">KP_7小键板7</span><br><span class="line">KP_8小键板8</span><br><span class="line">KP_9小键板9</span><br></pre></td></tr></table></figure></li><li><p><code>editor</code>用以订制操作键〔不支持<code>import_preset:</code>〕，键板键名同<code>key_binder/bindings</code>中的<code>accept</code>和<code>send</code>，效果定义如下：</p></li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">confirm上屏候选项</span><br><span class="line">commit_comment上屏候选项备注</span><br><span class="line">commit_raw_input上屏原始输入</span><br><span class="line">commit_script_text上屏变换后输入</span><br><span class="line">commit_composition语句流单字上屏</span><br><span class="line">revert撤消上次输入</span><br><span class="line">back按字符回退</span><br><span class="line">back_syllable按音节回退</span><br><span class="line">delete_candidate删除候选项</span><br><span class="line">delete向后删除</span><br><span class="line">cancel取消输入</span><br><span class="line">noop空</span><br></pre></td></tr></table></figure><ol start="7"><li><code>punctuator:</code> 下设<code>full_shape:</code>和<code>half_shape:</code>分别控制全角模式下的符号和半角模式下的符号，另有<code>use_space:</code>空格顶字〔<code>true</code>或<code>false</code>〕<ul><li>每条标点项可加<code>commit</code>直接上屏和<code>pair</code>交替上屏两种模式，默认为选单模式</li></ul></li></ol><h5 id="示例-10"><a href="#示例-10" class="headerlink" title="示例"></a><strong>示例</strong></h5><p>修改自 cangjie6.schema.yaml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">key_binder:</span><br><span class="line">  import_preset: default</span><br><span class="line">  bindings:</span><br><span class="line">    - &#123;accept: semicolon, send: 2, when: has_menu&#125; #分号选第二重码</span><br><span class="line">    - &#123;accept: apostrophe, send: 3, when: has_menu&#125; #引号选第三重码</span><br><span class="line">    - &#123;accept: &quot;Control+1&quot;, select: .next, when: always&#125;</span><br><span class="line">    - &#123;accept: &quot;Control+2&quot;, toggle: full_shape, when: always&#125;</span><br><span class="line">    - &#123;accept: &quot;Control+3&quot;, toggle: simplification, when: always&#125;</span><br><span class="line">    - &#123;accept: &quot;Control+4&quot;, toggle: extended_charset, when: always&#125;</span><br><span class="line">editor:</span><br><span class="line">bindings:</span><br><span class="line">Return: commit_comment</span><br><span class="line">punctuator:</span><br><span class="line">import_preset: symbols</span><br><span class="line">half_shape:</span><br><span class="line">&quot;&#x27;&quot;: &#123;pair: [&quot;「&quot;, &quot;」&quot;]&#125; #第一次按是「，第二次是」</span><br><span class="line">&quot;(&quot;: [&quot;〔&quot;, &quot;［&quot;] #弹出选单</span><br><span class="line">.: &#123;commit: &quot;。&quot;&#125; #无选单，直接上屛。优先级最高</span><br></pre></td></tr></table></figure><h3 id="1-6-其它"><a href="#1-6-其它" class="headerlink" title="1.6 其它"></a>1.6 其它</h3><ul><li>Rime还为每个方案提供选单和一定的外观订制能力</li><li>通常情况下<code>menu</code>在<code>default.yaml</code>中定义〔或用户修改档<code>default.custom.yaml</code>〕，<code>style</code>在<code>squirrel.yaml</code>或<code>weasel.yaml</code>〔或用户修改档<code>squirrel.custom.yaml</code>或<code>weasel.custom.yaml</code>〕</li></ul><h4 id="示例-11"><a href="#示例-11" class="headerlink" title="示例"></a><strong>示例</strong></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">menu:</span><br><span class="line">  alternative_select_labels: [ ①, ②, ③, ④, ⑤, ⑥, ⑦, ⑧, ⑨ ]  # 修改候选标籤</span><br><span class="line">  alternative_select_keys: ASDFGHJKL #如编码字符占用数字键则须另设选字键</span><br><span class="line">  page_size: 5 #选单每䈎显示个数</span><br></pre></td></tr></table></figure><h2 id="Dict-yaml-详解"><a href="#Dict-yaml-详解" class="headerlink" title="Dict.yaml 详解"></a><code>Dict.yaml</code> 详解</h2><ul><li></li></ul><h3 id="开始之前"><a href="#开始之前" class="headerlink" title="开始之前"></a>开始之前</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Rime dict</span><br><span class="line"># encoding: utf-8</span><br><span class="line">〔你还可以在这注释字典来源、变动记录等〕</span><br></pre></td></tr></table></figure><h3 id="描述档"><a href="#描述档" class="headerlink" title="描述档"></a>描述档</h3><ol><li><code>name:</code> 内部字典名，也即<code>schema</code>所引用的字典名，确保与文件名相一致</li><li><code>version:</code> 如果发布，请确保每次改动陞版本号</li></ol><h4 id="示例-12"><a href="#示例-12" class="headerlink" title="示例"></a><strong>示例</strong></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">name: &quot;cangjie6.extended&quot;</span><br><span class="line">version: &quot;0.1&quot;</span><br></pre></td></tr></table></figure><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><ol><li><p><code>sort:</code> 字典<strong>初始</strong>排序，可选<code>original</code>或<code>by_weight</code></p></li><li><p><code>use_preset_vocabulary:</code> 是否引入「八股文」〔含字词频、词库〕</p></li><li><p><code>max_phrase_length:</code> 配合<code>use_preset_vocabulary:</code>，设置导入词条最大词长</p></li><li><p><code>min_phrase_weight:</code> 配合<code>use_preset_vocabulary:</code>，设置导入词条最小词频</p></li><li><p><code>columns:</code> 定义码表以<code>Tab</code>分隔出的各列，可设<code>text</code>【文本】、<code>code</code>【码】、<code>weight</code>【权重】、<code>stem</code>【造词码】</p></li><li><p><code>import_tables:</code> 加载其它外部码表</p></li><li><p><code>encoder:</code> 形码造词规则</p></li><li><p><code>exclude_patterns:</code></p></li><li><p><code>rules:</code> 可用<code>length_equal:</code>和<code>length_in_range:</code>定义。大写字母表示字序，小写字母表示其所跟随的大写字母所以表的字中的编码序</p></li><li><p><code>tail_anchor:</code> 造词码包含结构分割符〔仅用于仓颉〕</p></li><li><p><code>exclude_patterns</code> 取消某编码的造词资格</p></li></ol><h4 id="示例-13"><a href="#示例-13" class="headerlink" title="示例"></a><strong>示例</strong></h4><p>cangjie6.extended.dict.yaml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">sort: by_weight</span><br><span class="line">use_preset_vocabulary: false</span><br><span class="line">import_tables:</span><br><span class="line">  - cangjie6 #单字码表由cangjie6.dict.yaml导入</span><br><span class="line">columns: #此字典为纯词典，无单字编码，仅有字和词频</span><br><span class="line">  - text #字／词</span><br><span class="line">  - weight #字／词频</span><br><span class="line">encoder:</span><br><span class="line">  exclude_patterns:</span><br><span class="line">    - &#x27;^z.*$&#x27;</span><br><span class="line">  rules:</span><br><span class="line">    - length_equal: 2 #对于二字词</span><br><span class="line">      formula: &quot;AaAzBaBbBz&quot; #取第一字首尾码、第二字首次尾码</span><br><span class="line">    - length_equal: 3 #对于三字词</span><br><span class="line">      formula: &quot;AaAzBaYzZz&quot; #取第一字首尾码、第二字首尾码、第三后缀码</span><br><span class="line">    - length_in_range: [4, 5] #对于四至五字词</span><br><span class="line">      formula: &quot;AaBzCaYzZz&quot; #取第一字首码，第二后缀码、第三字首码、倒数第二后缀码、最后一后缀码</span><br><span class="line">  tail_anchor: &quot;&#x27;&quot;</span><br></pre></td></tr></table></figure><h3 id="码表"><a href="#码表" class="headerlink" title="码表"></a>码表</h3><ul><li>以<code>Tab</code>分隔各列，各列依<code>columns:</code>定义排列。</li></ul><h4 id="示例-14"><a href="#示例-14" class="headerlink" title="示例"></a><strong>示例</strong></h4><p>cangjie6.dict.yaml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">columns:</span><br><span class="line">  - text #第一列字／词</span><br><span class="line">  - code #第二列码</span><br><span class="line">  - weight #第三列字／词频</span><br><span class="line">  - stem #第四列造词码</span><br></pre></td></tr></table></figure><p>cangjie6.dict.yaml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">个owjr246268ow&#x27;jr</span><br><span class="line">看hqbu245668</span><br><span class="line">中l243881</span><br><span class="line">呢rsp242970</span><br><span class="line">来doo235101</span><br><span class="line">吗rsqf221092</span><br><span class="line">为bhnf211340</span><br><span class="line">会owfa209844</span><br><span class="line">她vpd204725</span><br><span class="line">与xyc203975</span><br><span class="line">给vfor193007</span><br><span class="line">等hgdi183340</span><br><span class="line">这yymr181787</span><br><span class="line">用bq168934b&#x27;q</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 软件安装与使用 </category>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安装与使用 </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RIME配置指南</title>
      <link href="/vll-pages/posts/6811304.html"/>
      <url>/vll-pages/posts/6811304.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h2 id="1-写在前面"><a href="#1-写在前面" class="headerlink" title="1 写在前面"></a>1 写在前面</h2><h3 id="适合人群"><a href="#适合人群" class="headerlink" title="适合人群"></a>适合人群</h3><p>适合人群：寻找适合自己输入法的人、喜欢折腾的人。</p><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p><strong>Rime</strong> — 不是一种输入法，而是从常见键盘输入法中提炼出来的抽象输入算法框架。 Rime 涵盖了大多数输入法的「 共性 」部分，所以通过不同配置，Rime 可化身为不同输入法。</p><p><strong>Rime　输入法方案</strong> — 要让 Rime 实现某种具体输入法的功能，就需要一些数据来描述其工作方式，即定义该输入法的「 个性 」。例如：「汉语拼音」、「注音」、「五笔字型」 等输入法可凭借 Rime 提供的通用设施、通过设定不同工作参数来实现。</p><p>本文的重点就是讲解如何制作一套个性化的输入法方案。</p><h3 id="为什么要这么繁琐？"><a href="#为什么要这么繁琐？" class="headerlink" title="为什么要这么繁琐？"></a>为什么要这么繁琐？</h3><p>一键就搞掂，则必然选项少、功能单一、可玩性低。</p><h2 id="2-准备开工"><a href="#2-准备开工" class="headerlink" title="2 准备开工"></a>2 准备开工</h2><h3 id="2-1-Rime-的主要发行版"><a href="#2-1-Rime-的主要发行版" class="headerlink" title="2.1 Rime 的主要发行版"></a>2.1 Rime 的主要发行版</h3><p>Rime 是跨平台的输入法软件，本文介绍的 Rime　输入法方案通用于以下发行版：</p><ul><li>【中州韵】 ibus-rime → Linux &#x2F; fcitx-rime &#x2F; fcitx5-rime</li><li>【小狼毫】 Weasel → Windows</li><li>【鼠鬚管】 Squirrel → Mac OS X</li></ul><p>你可以根据自身操作系统安装最新版的 Rime 输入法 ！！！</p><h3 id="2-2-Rime-采用文本文件做定制"><a href="#2-2-Rime-采用文本文件做定制" class="headerlink" title="2.2 Rime 采用文本文件做定制"></a>2.2 Rime 采用文本文件做定制</h3><p>Rime 的配置文件、输入法方案定义及词典文件，均为特定格式的文本文件。因此，只需要一款够专业的 <strong>文本编辑器</strong> ，就可以设计 Rime Schema。 Rime 中的所有文本文件，均要求以 UTF-8 编码，并建议使用 UNIX 换行符（LF）。</p><p>鉴于一些文本编辑器会为 UTF-8 编码的文件自动添加 BOM 标记，为防止误将该字符混入文中，请不要从文件的第一行开始正文，建议在第一行的行首以 # 记号起一行注释，如：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Rime default settings</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Rime schema: My First Cool Schema</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Rime dictionary: Lingua Latina</span></span><br></pre></td></tr></table></figure><p>当然也可继续以注释行写下方案简介、码表来源、制作者、修订记录等信息，然后再切入正文。</p><h3 id="2-3-文本文件的格式"><a href="#2-3-文本文件的格式" class="headerlink" title="2.3 文本文件的格式"></a>2.3 文本文件的格式</h3><p>Rime 输入法采用扩展名为「<code>.yaml</code>」的文本文件，以　YAML　数据描述语言编写。请访问 <a href="http://yaml.org/">http://yaml.org/</a> 了解 YAML 文件格式。下文只对部分语法作简要说明，而将重点放在对　Rime 语义的解读上。</p><p>Rime 输入法方案的配置文件中亦会用到「　正则表达式　」实现一些高级功能。您需要掌握这份文件所描述的 <a href="http://www.boost.org/doc/libs/1_49_0/libs/regex/doc/html/boost_regex/syntax/perl_syntax.html">Perl 正则表达式语法</a>。</p><h3 id="2-4-Rime-的文件分布及作用"><a href="#2-4-Rime-的文件分布及作用" class="headerlink" title="2.4 Rime 的文件分布及作用"></a>2.4 Rime 的文件分布及作用</h3><p>除执行程序以外，Rime 还包括多种数据文件。这些数据文件主要存储在两个位置：</p><p><a href="https://github.com/rime/home/wiki/SharedData">全用户共享文件夹</a></p><ul><li>【中州韵】  <code>/usr/share/rime-data/</code></li><li>【小狼毫】  <code>&quot;安装目录\data&quot;</code></li><li>【鼠鬚管】  <code>&quot;/Library/Input Methods/Squirrel.app/Contents/SharedSupport/&quot;</code></li></ul><p><a href="https://github.com/rime/home/wiki/UserData">用户私人文件夹</a></p><ul><li>【中州韵】<ul><li><code>ibus</code> 为 <code>~/.config/ibus/rime/</code> </li><li><code>fcitx5</code> 为 <code>~/.local/fcitx5/rime/</code></li></ul></li><li>【小狼毫】  <code>%APPDATA%\Rime</code></li><li>【鼠鬚管】  <code>~/Library/Rime/</code></li></ul><p><a href="https://github.com/rime/home/wiki/SharedData">共享资料夹</a> 包含默认输入法方案的源文件。这些文件属于 Rime 发行软件的一部份，在访问权限控制较严格的系统上对用户是只读的，因此谢绝软件版本更新以外的任何修改。一旦用户修改这里的文件，很可能影响后续的软件升级或在升级时丢失数据。在执行「<a href="https://github.com/rime/home/wiki/CustomizationGuide#%E9%87%8D%E6%96%B0%E4%BD%88%E7%BD%B2%E7%9A%84%E6%93%8D%E4%BD%9C%E6%96%B9%E6%B3%95">部署</a>」操作时，将用到这里的输入法方案源文件、并结合用户定制的内容来编译默认输入法方案。</p><p><a href="https://github.com/rime/home/wiki/UserData">用户资料夹</a> 则包含为用户准备的内容，如：</p><ul><li>〔全局配置文件〕 <code>default.yaml</code></li><li>〔发行版的配置文件〕 <code>weasel.yaml</code></li><li>〔输入法方案的配置文件〕 <code>&lt;方案标识&gt;.schema.yaml</code></li><li>※〔安装信息〕 <code>installation.yaml</code></li><li>※〔用户状态信息〕 <code>user.yaml</code></li></ul><p>编译输入法方案所产出的二进制文件：</p><ul><li>〔Rime 棱镜〕 <code>&lt;方案标识&gt;.prism.bin</code></li><li>〔Rime 固态词典〕 <code>&lt;词典名&gt;.table.bin</code></li><li>〔Rime 反查词典〕 <code>&lt;词典名&gt;.reverse.bin</code></li></ul><p>记录用户写作习惯的文件：</p><ul><li>※〔用户词典〕 <code>&lt;词典名&gt;.userdb/</code> 或 <code>&lt;词典名&gt;.userdb.kct</code></li><li>※〔用户词典快照〕 <code>&lt;词典名&gt;.userdb.txt</code>、<code>&lt;词典名&gt;.userdb.kct.snapshot</code> 见于同步文件夹</li></ul><p>以及用户自己设置的：</p><ul><li>※〔用户对全局设置的定制信息〕 <code>default.custom.yaml</code></li><li>※〔用户对默认输入法方案的定制信息〕 <code>&lt;方案标识&gt;.custom.yaml</code></li><li>※〔用户自制输入法方案〕及配套的词典源文件</li></ul><p>注：以上标有 ※ 号的文件，为用户个性化的重要资料，您在清理文件时要注意备份！</p><h2 id="3-详解输入法方案"><a href="#3-详解输入法方案" class="headerlink" title="3 详解输入法方案"></a>3 详解输入法方案</h2><p>一套输入法方案，通常包含「方案定义」文件和「词典」文件。「方案定义」文件命名为 <code>&lt;方案标识&gt;.schema.yaml</code>，是一份包含输入法方案配置信息的 YAML 文件。下面按照板块来介绍该文件的详细设置方法。</p><h3 id="3-1-方案描述"><a href="#3-1-方案描述" class="headerlink" title="3.1 方案描述"></a>3.1 方案描述</h3><p>文件中需要有这样一组方案描述：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下代码片段节选自 luna_pinyin.schema.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">schema:</span></span><br><span class="line">  <span class="attr">schema_id:</span> <span class="string">luna_pinyin</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">朙月拼音</span></span><br><span class="line">  <span class="attr">version:</span> <span class="string">&quot;0.9&quot;</span></span><br><span class="line">  <span class="attr">author:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">佛振</span> <span class="string">&lt;chen.sst@gmail.com&gt;</span></span><br><span class="line">  <span class="attr">description:</span> <span class="string">|</span></span><br><span class="line">    <span class="string">Rime</span> <span class="string">默认的拼音输入法方案。</span></span><br></pre></td></tr></table></figure><ul><li><p><strong>方案名称</strong>。<code>schema/name</code> 字段是显示在　Rime〔方案选单〕中的名称。上例中 <code>朙月拼音</code> 即为一个方案名称。</p></li><li><p><strong>方案标识</strong>。每一个输入法方案在整个 Rime 中必须有唯一的「方案标识」，即 <code>schema/schema_id</code> 字段。方案标识由小写字母、数字、下划线构成。该标识仅在 Rime 内部使用，并且是方案定义文件名的组成部分，为了兼容不同文件系统，建议不要用大写字母、汉字、空格和其他符号做方案标识。如上例中，输入法方案的标识为<code>luna_pinyin</code>。</p></li><li><p><strong>方案版本号</strong>。方案如做升级，可以通过版本号（<code>schema/version</code>）来区分新旧版本。版本号是以「.」分隔的整数（或文字）构成的字符串。如果方案的升级会导致原有用户输入习惯无法在新方案中继续使用，应当更换一个新方案标识以示区别。例如：<code>【仓颉五代】之于【仓颉三代】</code>、<code>【五笔 98】之于【五笔 86】</code>，其实都已是互不兼容的输入法。下面是版本号常见的一些形式：</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- <span class="string">&quot;1&quot;</span>      <span class="comment"># 最好加引号表明是字符串！</span></span><br><span class="line">- <span class="string">&quot;1.0&quot;</span>    <span class="comment"># 最好加引号表明是字符串！</span></span><br><span class="line">- <span class="string">&quot;0.9.8&quot;</span></span><br><span class="line">- <span class="string">&quot;0.9.8.custom.86427531&quot;</span>  <span class="comment"># 这是经过用户自定义并自动生成的版本</span></span><br></pre></td></tr></table></figure><ul><li><strong>方案作者</strong>。采用字段　<code>schema/author</code> 列出作者和主要贡献者，格式为文字列表：</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">schema:</span></span><br><span class="line">  <span class="attr">author:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">作者甲</span> <span class="string">&lt;alpha@rime.org&gt;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">作者乙</span> <span class="string">&lt;beta@rime.org&gt;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">作者丙</span></span><br></pre></td></tr></table></figure><ul><li><strong>方案简介</strong>。 采用　<code>schema/description</code> 字段对输入法方案作简要介绍的多行文字。</li></ul><p>注意：</p><p>上述 <code>schema/schema_id</code>、<code>schema/version</code> 字段用于在程序中识别输入法方案，而 <code>schema/name</code>、<code>schema/author</code>、<code>schema/description</code> 则主要是展示给用户的信息。</p><h3 id="3-2-输入法引擎与功能组件"><a href="#3-2-输入法引擎与功能组件" class="headerlink" title="3.2 输入法引擎与功能组件"></a>3.2 输入法引擎与功能组件</h3><p>除了　<code>3.1 节</code>　中的方案描述外，方案定义文件中还包含各种功能设置，控制着输入法引擎的工作方式。 Rime 输入法内部的工作流程大致为：</p><blockquote><p>按键消息 → 后台「服务」 → 分配给对应的「会话」 → 交由「方案选单」或「输入引擎」处理……</p></blockquote><p>此处的会话指：多窗口、多线程操作，例如同时与好几位 MM 聊天，会产生好几组会话。每一组会话中都有一部输入引擎来完成从按键序列到文字的转换过程。Rime 支持在不同会话中使用「方案选单」指定的不同输入引擎。「方案选单」本身可以响应一些按键，但大多数时候都是把按键传递给「输入引擎」做处理，也就是说真正做处理的是本节要介绍的「输入引擎」。</p><p><code>输入引擎</code> 内部的工作流程大致如下：</p><blockquote><p>加载输入法方案&#x2F;预备功能组件 –&gt; 进入处理按键消息、处理按键消息的循环。</p></blockquote><p>在输入引擎中，响应各种按键并产生效果的工作，由不同功能组件分担。例如代码：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># luna_pinyin.schema.yaml</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="attr">engine:</span>                    <span class="comment"># 输入引擎设置，即挂接组件的「处方」</span></span><br><span class="line">  <span class="attr">processors:</span>              <span class="comment"># 一、这批组件处理各类按键消息</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ascii_composer</span>       <span class="comment"># ※ 处理西文模式及中西文切换</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">recognizer</span>           <span class="comment"># ※ 与 matcher 搭配，处理符合特定规则的输入码，如网址、反查等</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">key_binder</span>           <span class="comment"># ※ 在特定条件下将按键绑定到其他按键，如重定义逗号、句号为候选翻页键</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">speller</span>              <span class="comment"># ※ 拼写处理器，接受字符按键，编辑输入码</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">punctuator</span>           <span class="comment"># ※ 句读处理器，将单个字符按键直接映射为文字符号</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">selector</span>             <span class="comment"># ※ 选字处理器，处理数字选字键、上、下候选定位、换页键</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">navigator</span>            <span class="comment"># ※ 处理输入栏内的光标移动键</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">express_editor</span>       <span class="comment"># ※ 编辑器，处理空格、回车上屏、回退键等</span></span><br><span class="line">  <span class="attr">segmentors:</span>              <span class="comment"># 二、这批组件识别不同内容类型，将输入码分段</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ascii_segmentor</span>      <span class="comment"># ※ 标识西文段落</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">matcher</span>              <span class="comment"># ※ 标识符合特定规则的段落，如网址、反查等</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">abc_segmentor</span>        <span class="comment"># ※ 标识常规的文字段落</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">punct_segmentor</span>      <span class="comment"># ※ 标识句读段落</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">fallback_segmentor</span>   <span class="comment"># ※ 标识其他未标识段落</span></span><br><span class="line">  <span class="attr">translators:</span>             <span class="comment"># 三、这批组件翻译特定类型的编码段为一组候选文字</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">echo_translator</span>      <span class="comment"># ※ 没有其他候选字时，回显输入码</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">punct_translator</span>     <span class="comment"># ※ 转换标点符号</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">script_translator</span>    <span class="comment"># ※ 脚本翻译器，用于拼音等基于音节表的输入法方案</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">reverse_lookup_translator</span>  <span class="comment"># ※ 反查翻译器，用另一种编码方案查码</span></span><br><span class="line">  <span class="attr">filters:</span>                 <span class="comment"># 四、这批组件过滤翻译的结果</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">simplifier</span>           <span class="comment"># ※ 繁简转换</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">uniquifier</span>           <span class="comment"># ※ 过滤重复的候选字，有可能来自繁简转换</span></span><br></pre></td></tr></table></figure><p>注：除了示例代码中引用的组件外，尚有</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="string">fluid_editor</span>      <span class="comment"># ※ 句式编辑器，用于以空格断词、回车上屏的【注音】、【语句流】等输入法方案，替换 express_editor，也可以写作 fluency_editor</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">chord_composer</span>    <span class="comment"># ※ 和絃作曲家或曰并击处理器，用于【宫保拼音】等多键并击的输入法方案</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">table_translator</span>  <span class="comment"># ※ 码表翻译器，用于仓颉、五笔等基于码表的输入法方案，替换 script_translator</span></span><br></pre></td></tr></table></figure><p>输入引擎把完成具体功能的逻辑拆分为可装卸、组合的部件。在加载输入法方案时，按照配置文件给出的配方挂接所需的各种功能组件、加载各自设置，并准备执行各自的动作。所有功能组件接下来要完成的任务，均由输入引擎收到的某个按键消息触发。</p><p>从总体上看，输入引擎以按键消息为输入，其主要输出三个部分的内容：</p><ul><li>一是对系统按键消息的响应，如反馈给操作系统 <code>接受</code> 还是 <code>拒绝</code> 按键消息。</li><li>二是暂存于输入法、尚未完成处理的内容，会展现在输入法候选窗中。</li><li>三是要「上屏」的文字，并不是每按一键都有输出。通常中文字会伴随「确认」动作而上屏，有些按键也会直接导致符号上屏，但这要视具体场景而定。</li></ul><h4 id="3-2-1-Processors-功能组件"><a href="#3-2-1-Processors-功能组件" class="headerlink" title="3.2.1 Processors 功能组件"></a>3.2.1 Processors 功能组件</h4><p>第一类功能组件 <code>processor</code>，起着比较笼统地、「处理」按键消息的作用。</p><p>按键消息依次送往列表中的 <code>processor</code>，由他给出对按键的三种可能处理意见之一：</p><ul><li>「收」：即由 Rime 响应该按键；</li><li>「拒」：回禀操作系统 Rime 不做响应、请对按键做默认处理；</li><li>「转」：这个按键我不认得、请下一个 <code>processor</code> 继续处理。</li></ul><p>优先级依照 <code>processors</code> 列表顺序排列，接收按键者会针对按键消息做处理。</p><p>虽然看起来 <code>processor</code> 通过组合可以承担引擎的全部任务，但为了将逻辑继续细分、Rime 又为引擎设置了另外三类功能组件。这些组件都可以访问引擎中的数据对象（即输入上下文），并将各自所做处理的阶段成果存于其中。</p><p><code>processor</code> 最常见的处理，便是将按键所产生的字符记入上下文中的「输入码」序列。<br>当「输入码」发生变更时，下一组组件 <code>segmentors</code> 开始一轮新的作业。</p><h4 id="3-2-2-Segmentors-功能组件"><a href="#3-2-2-Segmentors-功能组件" class="headerlink" title="3.2.2 Segmentors 功能组件"></a>3.2.2 Segmentors 功能组件</h4><p>Rime 可对包含文字、数字、符号等不同内容的连续输入码进行识别，将其划分成若干段分而治之。这可以通过对整个输入序列代码的多轮划分操作完成。在每一轮中，所有 <code>segmentor</code> 各自识别起始于某一处、符合特定格式的编码段，并其中最长的编码段作为本轮划分的结果，给出此划分的（一个或多个） <code>segmentor</code> 组件则为该编码段打上「类型标签」；然后从此新编码段的结束位置，开始下一轮划分，直到整个输入码序列划分完毕。</p><p>举例来说，【朙月拼音】中，输入码 <code>2012nian\</code>，划分为三个编码段：<code>2012</code>（　被打上 <code>number</code> 标签）、<code>nian</code>（　被打上 <code>abc</code> 标签）、<code>\</code>（被打上 <code>punct</code> 标签）。</p><p>这些标签是初步划分后判定的类型，也可能有一个编码段被打上多个标签的情况。下一个阶段中，<code>translator</code> 会把特定类型的编码段翻译为文字。</p><h4 id="3-2-3-Translators-功能组件"><a href="#3-2-3-Translators-功能组件" class="headerlink" title="3.2.3 Translators 功能组件"></a>3.2.3 Translators 功能组件</h4><p>顾名思义，<code>translator</code> 完成由编码到文字的翻译。但有几个要点：</p><ul><li>一、翻译的对象是划分好的一个编码段。</li><li>二、某个 <code>translator</code> 组件往往只翻译具有特定标签的编码段。</li><li>三、翻译结果可能有多条，每条结果都会成为展现给用户的候选项。</li><li>四、同一编码段可由多个 <code>translator</code> 分别翻译，其翻译结果按一定规则合并为候选。</li><li>五、候选项所对应的编码未必是整个编码段。用拼音敲一个词组时，词组后面继续列出单字候选就是这种情况。</li></ul><p>下表给出了内存中编码段、标签和翻译结果的可视化示例：</p><pre><code>input | tag    | translations------+--------+-------------------------------------2012  | number | [ &quot;2012&quot; ], [ &quot;二〇一二&quot; ]nian  | abc    | [ &quot;年&quot;, &quot;念&quot;, &quot;唸&quot;,... ], [ &quot;nian&quot; ]\     | punct  | [ &quot;、&quot;, &quot;\&quot; ]</code></pre><p>一个输入串可被划分为多个编码段、每段编码又可具有多组翻译结果；取各编码段的首选结果连接起来，就是预备上屏的文字「<code>2012 年、</code>」。可以将上述示例数据视为一篇未定稿的「作文」，输入法界面此时会显示预备上屏的文字「<code>2012 年、</code>」，并列出最末一个编码段上的候选「<code>、</code>」及「<code>\</code>」以供选择。</p><p>有两款主力 <code>translator</code> 完成了大部分文字内容翻译工作，其实现方式很不一样：</p><ul><li><code>script_translator</code> ：也叫做 <code>r10n_translator</code>，主要实现罗马字分析法，以「固定音节表」为算法基础，识别输入码的音节构成，推敲排列组合、完成遣词造句。</li><li><code>table_translator</code> ：主要实现码表方法，基于规则的动态码表，构成编码空间内一个开放的编码集合。</li></ul><p>拼音、注音、方言拼音等皆是以 <code>固定音节表</code> 上的拼写为基础，通过排列组合方式产生编码，故适用罗马字分析法。而仓颉、五笔字型之类则是适用于码表输入法。</p><p>如果以码表方式来写拼音输入法方案，是怎样的效果呢？虽然仍可完成输入，但无法完全实现支持简拼、模糊拼音、使用隔音符号的动态调频、智能语句等特性。</p><p>以罗马字方式使用码表输入法，则无法实现定长编码顶字上屏、按编码规则构词等功能。在 Rime 各发行版默认输入法方案中，有一款「速成」输入法方案，即是用 <code>script_translator</code> 翻译仓颉码，从而实现全、简码混合的语句输入。</p><p>总结起来，这是两种构造新编码的方式：</p><ul><li>罗马字式输入法方案以一组固定的基本音节码为基础，通过构造新组合而构词；</li><li>码表式输入法方案则以一定码长为限，通过构造新编码映射而构词。</li></ul><h4 id="3-2-4-Filters-功能组件"><a href="#3-2-4-Filters-功能组件" class="headerlink" title="3.2.4 Filters 功能组件"></a>3.2.4 Filters 功能组件</h4><p>上一步已经收集到各个编码段的翻译结果，当输入法需要在界面中呈现一页候选项时，就会从最末一个编码段的结果集中挑选，直至取够方案定义文件中指定的<code>页最大候选数</code>。</p><p>在翻译结果进入候选序列之前，Rime 会对从结果集中选出每一条字词，做一组 <code>filter</code> 过滤。多个 <code>filter</code> 串行工作，进入候选序列的是串行过滤最终产出的结果。</p><p><code>filter</code> 可以：</p><ul><li>改装正在处理的候选项，修改某些属性值：简化字、火星文、菊花文有无有？</li><li>消除当前候选项，比如检测到重复（由不同 <code>translator</code> 产生）的候选条目</li><li>插入新的候选项，比如根据已有条目插入关联的结果</li><li>修改已有的候选序列</li></ul><h3 id="3-3-词典文件与码表"><a href="#3-3-词典文件与码表" class="headerlink" title="3.3 词典文件与码表"></a>3.3 词典文件与码表</h3><h4 id="3-3-1-词典文件"><a href="#3-3-1-词典文件" class="headerlink" title="3.3.1 词典文件"></a>3.3.1 词典文件</h4><p>词典是 <code>translator</code> 的参考书，通常与同名的输入法方案配套使用，如拼音输入法的词典以拼音码查字，仓颉输入法的词典以仓颉码查字。但也可以由若干编码属于同一系统的输入法方案共用，如各种双拼方案都会使用和拼音同样的词典，不仅可以复用码表数据，也可以共享用户录入的自造词。</p><p>Rime 的词典文件，命名为 <code>&lt;词典名&gt;.dict.yaml</code>，包含一份 <code>码表</code> 及对应的 <code>规则说明</code>。　词典文件的前半部份为 YAML 语法的<code>规则说明</code>：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">luna_pinyin</span></span><br><span class="line"><span class="attr">version:</span> <span class="string">&quot;0.9&quot;</span></span><br><span class="line"><span class="attr">sort:</span> <span class="string">by_weight</span></span><br><span class="line"><span class="attr">use_preset_vocabulary:</span> <span class="literal">true</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></table></figure><p>解释：</p><ul><li><code>name</code>: 内部使用的词典名，命名原则与「方案标识」一致，但可以与其配套的输入法方案标识不同；</li><li><code>version</code>: 管理词典的版本，规则同输入法方案定义文件中的版本号；</li><li><code>sort</code>: 词条初始排序方式，可选填 <code>by_weight</code>（按词频高低排序）或 <code>original</code>（保持原码表中的顺序）；</li><li><code>use_preset_vocabulary</code>: 填 <code>true</code> 或 <code>false</code>，选择是否导入默认词汇表【八股文】。</li></ul><h4 id="3-3-2-码表"><a href="#3-3-2-码表" class="headerlink" title="3.3.2 码表"></a>3.3.2 码表</h4><p>码表定义了输入法中编码与文字之间的对应关系。码表位于词典文件中 YAML 结束标记之后的部份。其格式为：以 <code>制表符分隔</code> 的值，每行定义一条「文字－编码」的对应关系：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 单字</span></span><br><span class="line"><span class="string">你</span><span class="string">ni</span></span><br><span class="line"><span class="string">我</span><span class="string">wo</span></span><br><span class="line"><span class="string">的</span><span class="string">de</span><span class="number">99</span><span class="string">%</span></span><br><span class="line"><span class="string">的</span><span class="string">di</span><span class="number">1</span><span class="string">%</span></span><br><span class="line"><span class="string">地</span><span class="string">de</span><span class="number">10</span><span class="string">%</span></span><br><span class="line"><span class="string">地</span><span class="string">di</span><span class="number">90</span><span class="string">%</span></span><br><span class="line"><span class="string">目</span><span class="string">mu</span></span><br><span class="line"><span class="string">好</span><span class="string">hao</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 词组</span></span><br><span class="line"><span class="string">你我</span></span><br><span class="line"><span class="string">你的</span></span><br><span class="line"><span class="string">我的</span></span><br><span class="line"><span class="string">我的天</span></span><br><span class="line"><span class="string">天地</span><span class="string">tian</span> <span class="string">di</span></span><br><span class="line"><span class="string">好天</span></span><br><span class="line"><span class="string">好好地</span></span><br><span class="line"><span class="string">目的</span><span class="string">mu</span> <span class="string">di</span></span><br><span class="line"><span class="string">目的地</span><span class="string">mu</span> <span class="string">di</span> <span class="string">di</span></span><br></pre></td></tr></table></figure><blockquote><p>※注意： </p><p><strong>不要</strong> 从网页复制以上代码到实际的词典文件中！因为网页里的制表符会被转换成空格，不符合 Rime 词典的格式要求。一些文本编辑器也会将制表符自动转换为空格，请注意检查和设置。</p></blockquote><p>码表部份，除了以上格式的编码行，还可以包含空行（不含任何字符）及注释行（行首为 # 符号）。</p><ul><li><strong>文字</strong>：以制表符分隔的第一个字段是所定义的文字，可以是单字或词组；</li><li><strong>编码</strong>：第二个字段是与文字对应的编码；若该编码由多个「音节」组成，音节之间以 <code>空格</code> 分开；</li><li><strong>频度</strong>：第三个字段为可选项，表示该字词的权重频度值（非负整数），或相对于默认权值的百分比（浮点数 %）。在拼音输入法中，往往多音字的若干种读音使用的场合不同，于是指定不同百分比来修正每个读音的使用频度。</li></ul><p>词组如果满足以下条件，则可以省去编码字段：</p><ul><li>词组中每个单字均有编码定义</li><li>词组中不包含多音字（例：你我），或多音字在该词组中读音的权值超过该多音字全部读音权值的 5%（例：我的）</li></ul><p>因为，此时词组的编码可由单字编码的组合推导出来。</p><p>反之，则有必要给出词组的编码以消除自动注音的不确定性（例：天地）。</p><p>当含有多音字的词组缺少编码字段时，自动注音程序会利用权重百分比高于 5% 的读音进行组合、生成全部可能的注音，如：</p><p>  「好好地」在编译时自动注音为「<code>hao hao de</code>」、「<code>hao hao di</code>」</p><h3 id="3-4-设置项速查手册"><a href="#3-4-设置项速查手册" class="headerlink" title="3.4 设置项速查手册"></a>3.4 设置项速查手册</h3><p><a href="https://github.com/LEOYoon-Tsaw/Rime_collections/blob/master/Rime_description.md">雪斋的文件</a> 全面而详细解释了输入法方案及词典中各设置项的含义及用法。</p><h3 id="3-5-八股文"><a href="#3-5-八股文" class="headerlink" title="3.5 八股文"></a>3.5 八股文</h3><p>Rime 有一份名为【八股文】的默认词汇表。</p><p>多数输入法方案需要用到一些标准白话文中的通用词汇。为免重复在每份码表中包含这些词汇，减少输入法方案维护成本，Rime 提供了一份默认词汇表及自动编码（注音）的设施。</p><p>创作输入法方案时，如果希望完全控制词汇表的内容而不采用【八股文】中的词组，只须直接将词汇编入码表即可。</p><p>否则，在词典文件中设置 <code>use_preset_vocabulary: true</code> 将【八股文】导入该词典；<br>在码表中给出单字码、需要分辨多音字的词组编码、以及该输入法方案特有的词汇，其他交给自动注音来做就好啦。</p><p>Rime 默认输入法方案正是利用这份词汇表及自动注音工具，在不牺牲效果及可维护性的前提下、使词典文件压缩到最小的行数。</p><p>【八股文】包含从 CC-CEDICT、新酷音等开源项目中整理出来的约二十三万条词汇，其用字及词频数据针对传统汉字做过调整。因此基于这份词汇表产生的输入结果，比较接近以传统汉字行文的实际用法。</p><p>为了充分利用【八股文】提供的词汇，自定义的词典应保证单字码表收录了符合 opencc 字形标准的常用字。特别注意，该标准对以下几组异体字的取舍，【八股文】将这些字（包括词组及字频）统一到左边一列的字形。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">为为</span><br><span class="line">伪伪</span><br><span class="line">嬀妫</span><br><span class="line">潙沩</span><br><span class="line">凶凶</span><br><span class="line">启启</span><br><span class="line">棱稜</span><br><span class="line">污污</span><br><span class="line">泄洩</span><br><span class="line">涌涌</span><br><span class="line">牀床</span><br><span class="line">着着</span><br><span class="line">灶灶</span><br><span class="line">众众</span><br><span class="line">里里</span><br><span class="line">踊踊</span><br><span class="line">麪面</span><br><span class="line">羣群</span><br><span class="line">峯峰</span><br></pre></td></tr></table></figure><p>请务必在码表中收录左列的单字；并建议收全右列的单字。</p><p>输入法词典往往对下列几组字不做严格区分，opencc 则倾向于细分异体字的不同用法。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">喫吃</span><br><span class="line">霉霉</span><br><span class="line">攷考</span><br><span class="line">覈核</span><br></pre></td></tr></table></figure><p>请尽量在码表中收全以上单字。</p><p>部署过程中，未能完成自动注音的字、词会以警告形式出现在日志文件里。如果所报告的字为生僻字、您可以忽略他；如果警告中大量出现某个常用字，那么应该注意到码表里缺失了该字的注音。</p><h3 id="3-6-编译输入法方案"><a href="#3-6-编译输入法方案" class="headerlink" title="3.6 编译输入法方案"></a>3.6 编译输入法方案</h3><p>将写好的输入法方案布署到 Rime 输入法的过程，称为「编译」：</p><p>为查询效率故，输入法工作时不直接加载文本格式的词典源文件，而要在编译过程中，为输入法方案生成专为高速查询设计的「.bin」文件。</p><p>编译时程序做以下几件事：</p><ul><li>将用户的定制内容合併到输入法方案定义中，在用户资料夹生成 .schema.yaml 文件副本；</li><li>依照输入法方案中指定的词典：求得音节表（不同种编码的集合）、单字表；</li><li>对词典中未提供编码的词组自动注音，也包括从【八股文】导入的词组；</li><li>建立按音节编码检索词条的索引，制作 Rime 固态词典；</li><li>建立按词条检索编码的索引，制作 Rime 反查词典；</li><li>依照音节表和方案定义中指定的拼写运算规则，制作 Rime 棱镜。</li></ul><h3 id="3-7-布署-Rime"><a href="#3-7-布署-Rime" class="headerlink" title="3.7 布署 Rime"></a>3.7 布署 Rime</h3><p>初次安装 Rime 输入法，无有任何输入法方案和用户设置。因此安装的最后一个步骤即是把发行版默认的输入法方案和设置文件布署到 Rime 为该用户创建的工作目录，至此 Rime 才成为一部可以发动的输入引擎。</p><p>此后无论是修改已有方案的设置，或是添加了新的输入法方案，都需要「重新布署」成功后方可使用户的新设置作用于 Rime 输入法。</p><p>〔★〕重新布署的方法是：</p><ul><li>【小狼毫】从开始菜单选择「重新部署」；或当开启托盘图标时，在托盘图标上右键选择「重新布署」；</li><li>【鼠鬚管】在系统语言文字选单中选择「重新布署」；</li><li>【中州韵】点击输入法状态栏（或 IBus 菜单）上的 ⟲ (Deploy) 按钮；</li><li>早于 ibus-rime 0.9.2 的版本：删除用户资料夹的 <code>default.yaml</code> 之后、执行 <code>ibus-daemon -drx</code> 重载 IBus</li></ul><h2 id="4-定制指南"><a href="#4-定制指南" class="headerlink" title="4 定制指南"></a>4 定制指南</h2><p>Rime 输入法方案，将 Rime 输入法的设置整理成完善的、可分发的形式。但并非一定要创作新的输入法方案，才可以改变 Rime 的行为。当用户需要对 Rime 中的各种设置做小幅的调节，最直接、但不完全正确的做法是：编辑用户资料夹中那些 .yaml 文件。</p><p>这一方法有弊端：</p><ul><li>当 Rime 软件升级时，也会升级各种设置档、默认输入法方案。用户编辑过的文件会被覆写为更高版本，所做调整也便丢失了。</li><li>即使在软件升级后再手动恢复经过编辑的文件，也会因设置档的其他部分未得到更新而失去本次升级新增和修复的功能。</li></ul><p>因此，对于随 Rime 发行的设置档及默认输入法方案，推荐的定制方法是：</p><p>创建一个文件名的主体部份（　即「.」之前的部分　）与要定制的文件相同、次级扩展名（　即扩展名「.yaml」之前的部分　）为 <code>.custom</code> 的定制文件，如：<code>default.yaml</code> –&gt; <code>default.custom.yaml</code> 。 该文件应当用　<code>patch</code> 关键字定义一组设置的「补丁」，Rime 会对源文件中的相同设置进行替换、或写入新的设置项。例如：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">patch:</span></span><br><span class="line">  <span class="string">&quot;一级设置项/二级设置项/三级设置项&quot;</span><span class="string">:</span> <span class="string">新的设置值</span></span><br><span class="line">  <span class="string">&quot;另一个设置项&quot;</span><span class="string">:</span> <span class="string">新的设置值</span></span><br><span class="line">  <span class="string">&quot;再一个设置项&quot;</span><span class="string">:</span> <span class="string">新的设置值</span></span><br><span class="line">  <span class="string">&quot;含列表的设置项/@0&quot;</span><span class="string">:</span> <span class="string">列表第一个元素新的设置值</span></span><br><span class="line">  <span class="string">&quot;含列表的设置项/@last&quot;</span><span class="string">:</span> <span class="string">列表最后一个元素新的设置值</span></span><br><span class="line">  <span class="string">&quot;含列表的设置项/@before 0&quot;</span><span class="string">:</span> <span class="string">在列表第一个元素之前插入新的设置值（不建议在补靪中使用）</span></span><br><span class="line">  <span class="string">&quot;含列表的设置项/@after last&quot;</span><span class="string">:</span> <span class="string">在列表最后一个元素之后插入新的设置值（不建议在补靪中使用）</span></span><br><span class="line">  <span class="string">&quot;含列表的设置项/@next&quot;</span><span class="string">:</span> <span class="string">在列表最后一个元素之后插入新的设置值（不建议在补靪中使用）</span></span><br></pre></td></tr></table></figure><p>以下这些例子，另载于 <a href="https://github.com/rime/home/wiki/CustomizationGuide">《定制指南》</a>，其中所介绍的知识和技巧，覆盖了不少本文未讨论的细节，应当会对创作新的输入法方案有所启发。</p><ul><li>例１：<a href="https://github.com/rime/home/wiki/CustomizationGuide#%E4%B8%80%E4%BE%8B%E5%AE%9A%E8%A3%BD%E6%AF%8F%E9%A0%81%E5%80%99%E9%81%B8%E6%95%B8">定制每页候选数</a></li><li>例２：<a href="https://github.com/rime/home/wiki/CustomizationGuide#%E4%B8%80%E4%BE%8B%E5%AE%9A%E8%A3%BD%E6%A8%99%E9%BB%9E%E7%AC%A6%E8%99%9F">定制标点符号</a></li><li>例３：<a href="https://github.com/rime/home/wiki/CustomizationGuide#%E4%B8%80%E4%BE%8B%E5%AE%9A%E8%A3%BD%E7%B0%A1%E5%8C%96%E5%AD%97%E8%BC%B8%E5%87%BA">定制简化字输出</a></li><li>例４：<a href="https://github.com/rime/home/wiki/CustomizationGuide#%E4%B8%80%E4%BE%8B%E9%BB%98%E8%AA%8D%E8%8B%B1%E6%96%87%E8%BC%B8%E5%87%BA">默认英文输出</a></li><li>例５：<a href="https://github.com/rime/home/wiki/CustomizationGuide#%E4%B8%80%E4%BE%8B%E5%AE%9A%E8%A3%BD%E6%96%B9%E6%A1%88%E9%81%B8%E5%96%AE">定制方案选单</a></li></ul><p>重要！创作了新的输入法方案，最后一步就是在「方案选单」里启用他。</p><h1 id="拼写运算"><a href="#拼写运算" class="headerlink" title="拼写运算"></a>拼写运算</h1><p>应该算是 Rime 输入法最主要的独创技术。</p><p>概括来说就是将方案中的编码通过规则映射到一组全新的拼写形式！<br>也就是说能让 Rime 输入法方案在不修改码表的情况下、适应不同的输入习惯。</p><p>拼写运算能用来：</p><ul><li>改革拼写法<ul><li>将编码映射到基于同一音系的其他拼写法，如注音、拼音、普通话罗马字相互转换</li><li>重定义注音键盘、双拼方案</li></ul></li><li>实现简拼查询</li><li>在音节表上灵活地定义模糊音规则</li><li>实现音节自动纠错</li><li>变换回显的输入码或提示码，如将输入码显示为字根、注音符号、带声调标注的罗马字</li></ul><p>给力吗？</p><p>[[★这里|SpellingAlgebra]] 有介绍拼写运算的专题文章。</p><h1 id="综合演练"><a href="#综合演练" class="headerlink" title="综合演练"></a>综合演练</h1><p>如果你安装好了 Rime 却不会玩，就一步一步跟我学吧。</p><p>本系列课程每个步骤的完整代码可由此查阅：</p><p><a href="https://github.com/lotem/rimeime/tree/master/doc/tutorial">https://github.com/lotem/rimeime/tree/master/doc/tutorial</a></p><h2 id="【一】破空中出鞘"><a href="#【一】破空中出鞘" class="headerlink" title="【一】破空中出鞘"></a>【一】破空中出鞘</h2><h3 id="Hello-Rime"><a href="#Hello-Rime" class="headerlink" title="Hello, Rime!"></a>Hello, Rime!</h3><p>第一个例子，总是最简单的（也是最傻的）。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Rime schema</span></span><br><span class="line"><span class="comment"># encoding: utf-8</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 最简单的 Rime 输入法方案</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="attr">schema:</span></span><br><span class="line">  <span class="attr">schema_id:</span> <span class="string">hello</span>    <span class="comment"># 注意此 ID 与文件名里 .schema.yaml 之前的部分相同</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">大家好</span>        <span class="comment"># 将在〔方案选单〕中显示</span></span><br><span class="line">  <span class="attr">version:</span> <span class="string">&quot;1&quot;</span>        <span class="comment"># 这是文字类型而非整数或小数，如 &quot;1.2.3&quot;</span></span><br></pre></td></tr></table></figure><p>起首几行是注释。而后只有一组必要的方案描述信息。</p><p>这一课主要练习建立格式正确的 YAML 文件。</p><ul><li>要点一，让你的文本编辑器以 UTF-8 编码保存该文件；</li><li>要点二，注意将 <code>schema:</code> 之下的三行代码以空格缩进——我的习惯是用两个空格——而 <strong>不要</strong> 用 Tab 字符来缩进。</li></ul><p>缩进表示设置项所属的层次。在他处引用到此文件中的设置项，可分别以 <code>schema/schema_id</code>, <code>schema/name</code>, <code>schema/version</code> 来指称。</p><p>我现在把写好的方案文件命名为 <code>hello.schema.yaml</code>，丢进<a href="https://github.com/rime/home/wiki/UserData">用户资料夹</a>，只要这一个文件就妥了；</p><p>然后，启用他。有些版本会有「方案选单设置」这个界面，在那里勾选【大家好】这个方案即可。若无有设置界面，则按照上文《定制方案选单》一节来做。</p><p>好运！我已建立了一款名为【大家好】的新方案！虽然他没有实现任何效果、按键仍会像无有输入法一样直接输出西文。</p><h3 id="开始改装"><a href="#开始改装" class="headerlink" title="开始改装"></a>开始改装</h3><p>为了处理字符按键、生成输入码，本例向输入引擎添加两个功能组件。</p><p>以下代码仍是 ID 为 <code>hello</code> 的新款输入法方案，但增加了 <code>schema/version</code> 的数值。<br>以后每个版本，都以前一个版本为基础改写，引文略去无有改动的部分，以突出重点。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="attr">schema:</span></span><br><span class="line">  <span class="attr">schema_id:</span> <span class="string">hello</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">大家好</span></span><br><span class="line">  <span class="attr">version:</span> <span class="string">&quot;2&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">engine:</span></span><br><span class="line">  <span class="attr">processors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">fluid_editor</span></span><br><span class="line">  <span class="attr">segmentors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">fallback_segmentor</span></span><br></pre></td></tr></table></figure><p><code>fluid_editor</code> 将字符按键记入输入上下文，<code>fallback_segmentor</code> 将输入码连缀成一段。于是重新布署后，按下字符键不再直接上屏，而显示为输入码。</p><p>你会发现，该输入法只是收集了键盘上的可打印字符，并于按下空格、回车键时令输入码上屏。</p><p>现在就好似写输入法程序的过程中，将将取得一小点成果，还有很多逻辑未实现。不同的是，在 Rime 输入法方案里写一行代码，顶 Rime 开发者所写的上百上千行。因此我可以很快地组合各种逻辑组件、搭建出心里想的输入法。</p><h3 id="创建候选项"><a href="#创建候选项" class="headerlink" title="创建候选项"></a>创建候选项</h3><p>第二版的【大家好】将键盘上所有字符都记入输入码，这对整句输入有用，但是时下流行输入法只处理编码字符、其他字符直接上屏的形式。为了对编码字符做出区分，以下改用 <code>speller</code> + <code>express_editor</code> 的组合取代 <code>fluid_editor</code>：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="attr">schema:</span></span><br><span class="line">  <span class="comment"># ...</span></span><br><span class="line">  <span class="attr">version:</span> <span class="string">&quot;3&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">engine:</span></span><br><span class="line">  <span class="attr">processors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">speller</span>          <span class="comment"># 把字母追加到编码串</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">express_editor</span>   <span class="comment"># 空格确认当前输入、其他字符直接上屏</span></span><br><span class="line">  <span class="attr">segmentors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">fallback_segmentor</span></span><br></pre></td></tr></table></figure><p><code>speller</code> 默认只接受小写拉丁字母作为输入码。<br>试试看，输入其他字符如大写字母、数字、标点，都会直接上屏。并且如果已经输入了编码时，下一个直接上屏的字符会将输入码顶上屏。</p><p>再接着，创建一个最简单的候选项——把编码串本身作为一个选项。故而会提供这个选项的新组件名叫 <code>echo_translator</code>。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="attr">engine:</span></span><br><span class="line">  <span class="comment"># ...</span></span><br><span class="line">  <span class="attr">translators:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">echo_translator</span>  <span class="comment"># （无有其他结果时，）创建一个与编码串一个模样的候选项</span></span><br></pre></td></tr></table></figure><p>至此，【大家好】看上去与一个真正的输入法形似啦。只是还不会打出「大家好」哇？</p><h3 id="编写词典"><a href="#编写词典" class="headerlink" title="编写词典"></a>编写词典</h3><p>那就写一部词典，码表中设置以 <code>hello</code> 作为短语「大家好」的编码：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Rime dictionary</span></span><br><span class="line"><span class="comment"># encoding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">hello</span></span><br><span class="line"><span class="attr">version:</span> <span class="string">&quot;1&quot;</span></span><br><span class="line"><span class="attr">sort:</span> <span class="string">original</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"></span><br><span class="line"><span class="string">大家好</span><span class="string">hello</span></span><br><span class="line"><span class="string">再见</span><span class="string">bye</span></span><br><span class="line"><span class="string">再会</span><span class="string">bye</span></span><br></pre></td></tr></table></figure><p>※注意： <strong>不要</strong> 从网页复制以上代码到实际的词典文件！因为网页里制表符被转换成空格从而不符合 Rime 词典要求的格式。一些文本编辑器也会将使用者输入的制表符自动转换为空格，请注意检查和设置。</p><p>同时修改方案定义：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#...</span></span><br><span class="line"></span><br><span class="line"><span class="attr">schema:</span></span><br><span class="line">  <span class="comment"># ...</span></span><br><span class="line">  <span class="attr">version:</span> <span class="string">&quot;4&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">engine:</span></span><br><span class="line">  <span class="comment">#...</span></span><br><span class="line">  <span class="attr">segmentors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">abc_segmentor</span>       <span class="comment"># 标记输入码的类型</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">fallback_segmentor</span></span><br><span class="line">  <span class="attr">translators:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">echo_translator</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">table_translator</span>    <span class="comment"># 码表式转换</span></span><br><span class="line"></span><br><span class="line"><span class="attr">translator:</span></span><br><span class="line">  <span class="attr">dictionary:</span> <span class="string">hello</span>       <span class="comment"># 设置 table_translator 使用的词典名</span></span><br></pre></td></tr></table></figure><p>工作流程是这样的：</p><ul><li><code>speller</code> 将字母键加入输入码序列</li><li><code>abc_segmentor</code> 给输入码打上标签 <code>abc</code></li><li><code>table_translator</code> 把带有 <code>abc</code> 籤的输入码以查表的方式译为中文</li><li><code>table_translator</code> 所查的码表在 <code>translator/dictionary</code> 所指定的词典里</li></ul><p>现在可以敲 <code>hello</code> 而打出「大家好」。完工！</p><h3 id="实现选字及换页"><a href="#实现选字及换页" class="headerlink" title="实现选字及换页"></a>实现选字及换页</h3><p>等一下。</p><p>记得 <code>hello</code> 词典里，还有个编码叫做 <code>bye</code>。敲 <code>bye</code>，Rime 给出「再见」、「再会」两个候选短语。</p><p>这时敲空格键，就会打出「再见」；那么怎样打出「再会」呢？</p><p>大家首先想到的方法，是：打完编码 <code>bye</code>，按 <code>1</code> 选「再见」，按 <code>2</code> 选「再会」。<br>可是现在按下 <code>2</code> 去，却是上屏「再见」和数字「2」。可见并没有完成数字键选字的处理，而是将数字同其他符号一样做了顶字上屏处理。</p><p>增加一部 <code>selector</code>，即可实现以数字键选字。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">schema:</span></span><br><span class="line">  <span class="comment"># ...</span></span><br><span class="line">  <span class="attr">version:</span> <span class="string">&quot;5&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">engine:</span></span><br><span class="line">  <span class="attr">processors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">speller</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">selector</span>         <span class="comment"># 选字、换页</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">navigator</span>        <span class="comment"># 移动插入点</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">express_editor</span></span><br><span class="line">  <span class="comment"># ...</span></span><br></pre></td></tr></table></figure><p><code>selector</code> 除了数字键，还响应前次页、上下方向键。因此选择第二候选「再会」，既可以按数字 <code>2</code>，又可以按方向键「↓」将「再会」高亮、再按空格键确认。</p><p><code>navigator</code> 处理左右方向键、<code>Home</code>、<code>End</code> 键，实现移动插入点的编辑功能。有两种情况需要用到他：一是发现输入码有误需要定位修改，二是缩小候选词对应的输入码的范围、精准地编辑新词组。</p><p>接下来向词典添加一组重码，以检验换页的效果：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">hello</span></span><br><span class="line"><span class="attr">version:</span> <span class="string">&quot;2&quot;</span></span><br><span class="line"><span class="attr">sort:</span> <span class="string">original</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"></span><br><span class="line"><span class="string">大家好</span><span class="string">hello</span></span><br><span class="line"><span class="string">再见</span><span class="string">bye</span></span><br><span class="line"><span class="string">再会</span><span class="string">bye</span></span><br><span class="line"></span><br><span class="line"><span class="string">星期一</span><span class="string">monday</span></span><br><span class="line"><span class="string">星期二</span><span class="string">tuesday</span></span><br><span class="line"><span class="string">星期三</span><span class="string">wednesday</span></span><br><span class="line"><span class="string">星期四</span><span class="string">thursday</span></span><br><span class="line"><span class="string">星期五</span><span class="string">friday</span></span><br><span class="line"><span class="string">星期六</span><span class="string">saturday</span></span><br><span class="line"><span class="string">星期日</span><span class="string">sunday</span></span><br><span class="line"></span><br><span class="line"><span class="string">星期一</span><span class="string">weekday</span></span><br><span class="line"><span class="string">星期二</span><span class="string">weekday</span></span><br><span class="line"><span class="string">星期三</span><span class="string">weekday</span></span><br><span class="line"><span class="string">星期四</span><span class="string">weekday</span></span><br><span class="line"><span class="string">星期五</span><span class="string">weekday</span></span><br><span class="line"><span class="string">星期六</span><span class="string">weekday</span></span><br><span class="line"><span class="string">星期日</span><span class="string">weekday</span></span><br></pre></td></tr></table></figure><p>默认每页候选数为 5，输入 <code>weekday</code>，显示「星期一」至「星期五」。再敲 <code>Page_Down</code> 显示第二页后选词「星期六、星期日」。</p><h3 id="输出中文标点"><a href="#输出中文标点" class="headerlink" title="输出中文标点"></a>输出中文标点</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">schema:</span></span><br><span class="line">  <span class="comment"># ...</span></span><br><span class="line">  <span class="attr">version:</span> <span class="string">&quot;6&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">engine:</span></span><br><span class="line">  <span class="attr">processors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">speller</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">punctuator</span>        <span class="comment"># 处理符号按键</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">selector</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">navigator</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">express_editor</span></span><br><span class="line">  <span class="attr">segmentors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">abc_segmentor</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">punct_segmentor</span>   <span class="comment"># 划界，与前后方的其他编码区分开</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">fallback_segmentor</span></span><br><span class="line">  <span class="attr">translators:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">echo_translator</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">punct_translator</span>  <span class="comment"># 转换</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">table_translator</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="attr">punctuator:</span>             <span class="comment"># 设置符号表，这里直接导入默认的</span></span><br><span class="line">  <span class="attr">import_preset:</span> <span class="string">default</span></span><br></pre></td></tr></table></figure><p>这次的修改，要注意 <code>punctuator</code>, <code>punct_segmentor</code>, <code>punct_translator</code> 相对于其他组件的位置。</p><p><code>punctuator/import_preset</code> 告诉 Rime 使用一套默认的符号表。他的值 <code>default</code> 可以换成其他名字如 <code>xxx</code>，则 Rime 会读取 <code>xxx.yaml</code> 里面定义的符号表。</p><p>如今再敲 <code>hello.</code> 就会得到「大家好。」</p><h3 id="用符号键换页"><a href="#用符号键换页" class="headerlink" title="用符号键换页"></a>用符号键换页</h3><p>早先流行用 <code>-</code> 和 <code>=</code> 这一对符号换页，如今流行用 <code>,</code> 和 <code>.</code> 。<br>在第六版中「，」「。」是会顶字上屏的。现在要做些处理以达到一键两用的效果。</p><p>Rime 提供了 <code>key_binder</code> 组件，他能够在一定条件下，将指定按键绑定为另一个按键。对于本例就是：</p><ul><li>当展现候选菜单时，句号键（<code>period</code>）绑定为向后换页（<code>Page_Down</code>）</li><li>当已有（向后）换页动作时，逗号键（<code>comma</code>）绑定为向前换页（<code>Page_Up</code>）</li></ul><p>逗号键向前换页的条件之所以比句号键严格，是为了「，」仍可在未进行换页的情况下顶字上屏。</p><p>经过 <code>key_binder</code> 的处理，用来换页的逗号、句号键改头换面为前、后换页键，从而绕过 <code>punctuator</code>，最终被 <code>selector</code> 当作换页来处理。</p><p>最终的代码如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">schema:</span></span><br><span class="line">  <span class="attr">schema_id:</span> <span class="string">hello</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">大家好</span></span><br><span class="line">  <span class="attr">version:</span> <span class="string">&quot;7&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">engine:</span></span><br><span class="line">  <span class="attr">processors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">key_binder</span>  <span class="comment"># 抢在其他 processor 处理之前判定是否换页用的符号键</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">speller</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">punctuator</span>  <span class="comment"># 否则「，。」就会由此上屏</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">selector</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">navigator</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">express_editor</span></span><br><span class="line">  <span class="attr">segmentors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">abc_segmentor</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">punct_segmentor</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">fallback_segmentor</span></span><br><span class="line">  <span class="attr">translators:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">echo_translator</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">punct_translator</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">table_translator</span></span><br><span class="line"></span><br><span class="line"><span class="attr">translator:</span></span><br><span class="line">  <span class="attr">dictionary:</span> <span class="string">hello</span></span><br><span class="line"></span><br><span class="line"><span class="attr">punctuator:</span></span><br><span class="line">  <span class="attr">import_preset:</span> <span class="string">default</span></span><br><span class="line"></span><br><span class="line"><span class="attr">key_binder:</span></span><br><span class="line">  <span class="attr">bindings:</span>             <span class="comment"># 每条定义包含条件、接收按键（IBus 规格的键名，可加修饰符，如「Control+Return」）、发送按键</span></span><br><span class="line"></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">when:</span>   <span class="string">paging</span>    <span class="comment"># 仅当已发生向后换页时，</span></span><br><span class="line">      <span class="attr">accept:</span> <span class="string">comma</span>     <span class="comment"># 将「逗号」键……</span></span><br><span class="line">      <span class="attr">send:</span>   <span class="string">Page_Up</span>   <span class="comment"># 关联到「向前换页」；于是 navigator 将收到一发 Page_Up</span></span><br><span class="line"></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">when:</span>   <span class="string">has_menu</span>  <span class="comment"># 只要有候选字即满足条件</span></span><br><span class="line">      <span class="attr">accept:</span> <span class="string">period</span></span><br><span class="line">      <span class="attr">send:</span>   <span class="string">Page_Down</span></span><br></pre></td></tr></table></figure><h2 id="【二】修炼之道"><a href="#【二】修炼之道" class="headerlink" title="【二】修炼之道"></a>【二】修炼之道</h2><p>与【大家好】这个方案不同。以下一组示例，主要演示如何活用符号键盘，以及罗马字转写式输入。</p><h3 id="改造键盘"><a href="#改造键盘" class="headerlink" title="改造键盘"></a>改造键盘</h3><p>莫以为【大家好】是最最简单的输入法方案。码表式输入法，不如「键盘式」输入法来得简单明快！</p><p>用 <code>punctuator</code> 这一套组件，就可实现一款键盘输入法：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Rime schema</span></span><br><span class="line"><span class="comment"># encoding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="attr">schema:</span></span><br><span class="line">  <span class="attr">schema_id:</span> <span class="string">numbers</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">数字之道</span></span><br><span class="line">  <span class="attr">version:</span> <span class="string">&quot;1&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">engine:</span></span><br><span class="line">  <span class="attr">processors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">punctuator</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">express_editor</span></span><br><span class="line">  <span class="attr">segmentors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">punct_segmentor</span></span><br><span class="line">  <span class="attr">translators:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">punct_translator</span></span><br><span class="line"></span><br><span class="line"><span class="attr">punctuator:</span></span><br><span class="line">  <span class="attr">half_shape:</span> <span class="meta">&amp;symtable</span></span><br><span class="line">    <span class="string">&quot;1&quot;</span> <span class="string">:</span> <span class="string">一</span></span><br><span class="line">    <span class="string">&quot;2&quot;</span> <span class="string">:</span> <span class="string">二</span></span><br><span class="line">    <span class="string">&quot;3&quot;</span> <span class="string">:</span> <span class="string">三</span></span><br><span class="line">    <span class="string">&quot;4&quot;</span> <span class="string">:</span> <span class="string">四</span></span><br><span class="line">    <span class="string">&quot;5&quot;</span> <span class="string">:</span> <span class="string">五</span></span><br><span class="line">    <span class="string">&quot;6&quot;</span> <span class="string">:</span> <span class="string">六</span></span><br><span class="line">    <span class="string">&quot;7&quot;</span> <span class="string">:</span> <span class="string">七</span></span><br><span class="line">    <span class="string">&quot;8&quot;</span> <span class="string">:</span> <span class="string">八</span></span><br><span class="line">    <span class="string">&quot;9&quot;</span> <span class="string">:</span> <span class="string">九</span></span><br><span class="line">    <span class="string">&quot;0&quot;</span> <span class="string">:</span> <span class="string">〇</span></span><br><span class="line">    <span class="string">&quot;s&quot;</span> <span class="string">:</span> <span class="string">十</span></span><br><span class="line">    <span class="string">&quot;b&quot;</span> <span class="string">:</span> <span class="string">百</span></span><br><span class="line">    <span class="string">&quot;q&quot;</span> <span class="string">:</span> <span class="string">千</span></span><br><span class="line">    <span class="string">&quot;w&quot;</span> <span class="string">:</span> <span class="string">万</span></span><br><span class="line">    <span class="string">&quot;n&quot;</span> <span class="string">:</span> <span class="string">年</span></span><br><span class="line">    <span class="string">&quot;y&quot;</span> <span class="string">:</span> [ <span class="string">月</span>, <span class="string">元</span>, <span class="string">亿</span> ]</span><br><span class="line">    <span class="string">&quot;r&quot;</span> <span class="string">:</span> <span class="string">日</span></span><br><span class="line">    <span class="string">&quot;x&quot;</span> <span class="string">:</span> <span class="string">星期</span></span><br><span class="line">    <span class="string">&quot;j&quot;</span> <span class="string">:</span> <span class="string">角</span></span><br><span class="line">    <span class="string">&quot;f&quot;</span> <span class="string">:</span> <span class="string">分</span></span><br><span class="line">    <span class="string">&quot;z&quot;</span> <span class="string">:</span> [ <span class="string">之</span>, <span class="string">整</span> ]</span><br><span class="line">    <span class="string">&quot;d&quot;</span> <span class="string">:</span> <span class="string">第</span></span><br><span class="line">    <span class="string">&quot;h&quot;</span> <span class="string">:</span> <span class="string">号</span></span><br><span class="line">    <span class="string">&quot;.&quot;</span> <span class="string">:</span> <span class="string">点</span></span><br><span class="line">  <span class="attr">full_shape:</span> <span class="meta">*symtable</span></span><br></pre></td></tr></table></figure><p>对，所谓「键盘输入法」，就是按键和字直接对应的输入方式。</p><p>这次，不再写 <code>punctuator/import_preset</code> 这项，而是自订了一套符号表。</p><p>鸹！原来 <code>punctuator</code> 不单可以用来打出标点符号；还可以重定义空格以及全部 94 个可打印 ASCII 字符（码位 0x20 至 0x7e）。</p><p>在符号表代码里，用对应的 ASCII 字符表示按键。记得这些按键字符要放在引号里面，YAML 才能够正确解析喔。</p><p>示例代码表演了两种符号的映射方式：一对一及一对多。一对多者，按键后符号不会立即上屏，而是……嘿嘿，自己体验吧 <code>:-)</code></p><p>关于代码里 <code>symtable</code> 的一点解释：</p><p>  这是 YAML 的一种语法，<code>&amp;symtable</code> 叫做「锚点标签」，给紧随其后的内容起个名字叫 <code>symtable</code>；<br>  <code>*symtable</code> 则相当于引用了 <code>symtable</code> 所标记的那段内容，从而避免重复。</p><p>Rime 里的符号有「全角」、「半角」两种状态。本方案里暂不作区分，教 <code>half_shape</code>、<code>full_shape</code> 使用同一份符号表。</p><h3 id="大写数字键盘"><a href="#大写数字键盘" class="headerlink" title="大写数字键盘"></a>大写数字键盘</h3><p>灵机一动，不如利用「全、半角」模式来区分「大、小写」中文数字！</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">schema:</span></span><br><span class="line">  <span class="comment"># ...</span></span><br><span class="line">  <span class="attr">version:</span> <span class="string">&quot;2&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">switches:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">full_shape</span></span><br><span class="line">    <span class="attr">states:</span> [ <span class="string">小写</span>, <span class="string">大写</span> ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure><p>先来定义状态开关：<code>0</code> 态改「半角」为「小写」，<code>1</code> 态改「全角」为「大写」。</p><p>这样一改，再打开「方案选单」，方案「数字之道」底下就会多出个「小写→大写」的选项，每选定一次、状态随之反转一次。</p><p>接着给 <code>half_shape</code>、<code>full_shape</code> 定义不同的符号表：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">punctuator:</span></span><br><span class="line">  <span class="attr">half_shape:</span></span><br><span class="line">    <span class="string">&quot;1&quot;</span> <span class="string">:</span> <span class="string">一</span></span><br><span class="line">    <span class="string">&quot;2&quot;</span> <span class="string">:</span> <span class="string">二</span></span><br><span class="line">    <span class="string">&quot;3&quot;</span> <span class="string">:</span> <span class="string">三</span></span><br><span class="line">    <span class="string">&quot;4&quot;</span> <span class="string">:</span> <span class="string">四</span></span><br><span class="line">    <span class="string">&quot;5&quot;</span> <span class="string">:</span> <span class="string">五</span></span><br><span class="line">    <span class="string">&quot;6&quot;</span> <span class="string">:</span> <span class="string">六</span></span><br><span class="line">    <span class="string">&quot;7&quot;</span> <span class="string">:</span> <span class="string">七</span></span><br><span class="line">    <span class="string">&quot;8&quot;</span> <span class="string">:</span> <span class="string">八</span></span><br><span class="line">    <span class="string">&quot;9&quot;</span> <span class="string">:</span> <span class="string">九</span></span><br><span class="line">    <span class="string">&quot;0&quot;</span> <span class="string">:</span> <span class="string">〇</span></span><br><span class="line">    <span class="string">&quot;s&quot;</span> <span class="string">:</span> <span class="string">十</span></span><br><span class="line">    <span class="string">&quot;b&quot;</span> <span class="string">:</span> <span class="string">百</span></span><br><span class="line">    <span class="string">&quot;q&quot;</span> <span class="string">:</span> <span class="string">千</span></span><br><span class="line">    <span class="string">&quot;w&quot;</span> <span class="string">:</span> <span class="string">万</span></span><br><span class="line">    <span class="string">&quot;n&quot;</span> <span class="string">:</span> <span class="string">年</span></span><br><span class="line">    <span class="string">&quot;y&quot;</span> <span class="string">:</span> [ <span class="string">月</span>, <span class="string">元</span>, <span class="string">亿</span> ]</span><br><span class="line">    <span class="string">&quot;r&quot;</span> <span class="string">:</span> <span class="string">日</span></span><br><span class="line">    <span class="string">&quot;x&quot;</span> <span class="string">:</span> <span class="string">星期</span></span><br><span class="line">    <span class="string">&quot;j&quot;</span> <span class="string">:</span> <span class="string">角</span></span><br><span class="line">    <span class="string">&quot;f&quot;</span> <span class="string">:</span> <span class="string">分</span></span><br><span class="line">    <span class="string">&quot;z&quot;</span> <span class="string">:</span> [ <span class="string">之</span>, <span class="string">整</span> ]</span><br><span class="line">    <span class="string">&quot;d&quot;</span> <span class="string">:</span> <span class="string">第</span></span><br><span class="line">    <span class="string">&quot;h&quot;</span> <span class="string">:</span> <span class="string">号</span></span><br><span class="line">    <span class="string">&quot;.&quot;</span> <span class="string">:</span> <span class="string">点</span></span><br><span class="line">  <span class="attr">full_shape:</span></span><br><span class="line">    <span class="string">&quot;1&quot;</span> <span class="string">:</span> <span class="string">壹</span></span><br><span class="line">    <span class="string">&quot;2&quot;</span> <span class="string">:</span> <span class="string">贰</span></span><br><span class="line">    <span class="string">&quot;3&quot;</span> <span class="string">:</span> <span class="string">参</span></span><br><span class="line">    <span class="string">&quot;4&quot;</span> <span class="string">:</span> <span class="string">肆</span></span><br><span class="line">    <span class="string">&quot;5&quot;</span> <span class="string">:</span> <span class="string">伍</span></span><br><span class="line">    <span class="string">&quot;6&quot;</span> <span class="string">:</span> <span class="string">陆</span></span><br><span class="line">    <span class="string">&quot;7&quot;</span> <span class="string">:</span> <span class="string">柒</span></span><br><span class="line">    <span class="string">&quot;8&quot;</span> <span class="string">:</span> <span class="string">捌</span></span><br><span class="line">    <span class="string">&quot;9&quot;</span> <span class="string">:</span> <span class="string">玖</span></span><br><span class="line">    <span class="string">&quot;0&quot;</span> <span class="string">:</span> <span class="string">零</span></span><br><span class="line">    <span class="string">&quot;s&quot;</span> <span class="string">:</span> <span class="string">拾</span></span><br><span class="line">    <span class="string">&quot;b&quot;</span> <span class="string">:</span> <span class="string">佰</span></span><br><span class="line">    <span class="string">&quot;q&quot;</span> <span class="string">:</span> <span class="string">千</span></span><br><span class="line">    <span class="string">&quot;w&quot;</span> <span class="string">:</span> <span class="string">万</span></span><br><span class="line">    <span class="string">&quot;n&quot;</span> <span class="string">:</span> <span class="string">年</span></span><br><span class="line">    <span class="string">&quot;y&quot;</span> <span class="string">:</span> [ <span class="string">月</span>, <span class="string">圆</span>, <span class="string">亿</span> ]</span><br><span class="line">    <span class="string">&quot;r&quot;</span> <span class="string">:</span> <span class="string">日</span></span><br><span class="line">    <span class="string">&quot;x&quot;</span> <span class="string">:</span> <span class="string">星期</span></span><br><span class="line">    <span class="string">&quot;j&quot;</span> <span class="string">:</span> <span class="string">角</span></span><br><span class="line">    <span class="string">&quot;f&quot;</span> <span class="string">:</span> <span class="string">分</span></span><br><span class="line">    <span class="string">&quot;z&quot;</span> <span class="string">:</span> [ <span class="string">之</span>, <span class="string">整</span> ]</span><br><span class="line">    <span class="string">&quot;d&quot;</span> <span class="string">:</span> <span class="string">第</span></span><br><span class="line">    <span class="string">&quot;h&quot;</span> <span class="string">:</span> <span class="string">号</span></span><br><span class="line">    <span class="string">&quot;.&quot;</span> <span class="string">:</span> <span class="string">点</span></span><br></pre></td></tr></table></figure><p>哈，调出选单切换一下大小写，输出的字全变样！酷。</p><p>但是要去选单切换，总不如按下 <code>Shift</code> 就全都有了：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">punctuator:</span></span><br><span class="line">  <span class="attr">half_shape:</span></span><br><span class="line">    <span class="comment"># ... 添加以下这些</span></span><br><span class="line">    <span class="string">&quot;!&quot;</span> <span class="string">:</span> <span class="string">壹</span></span><br><span class="line">    <span class="string">&quot;@&quot;</span> <span class="string">:</span> <span class="string">贰</span></span><br><span class="line">    <span class="string">&quot;#&quot;</span> <span class="string">:</span> <span class="string">参</span></span><br><span class="line">    <span class="string">&quot;$&quot;</span> <span class="string">:</span> [ <span class="string">肆</span>, <span class="string">￥</span>, <span class="string">&quot;$&quot;</span>, <span class="string">&quot;€&quot;</span>, <span class="string">&quot;£&quot;</span> ]</span><br><span class="line">    <span class="string">&quot;%&quot;</span> <span class="string">:</span> [ <span class="string">伍</span>, <span class="string">百分之</span> ]</span><br><span class="line">    <span class="string">&quot;^&quot;</span> <span class="string">:</span> <span class="string">陆</span></span><br><span class="line">    <span class="string">&quot;&amp;&quot;</span> <span class="string">:</span> <span class="string">柒</span></span><br><span class="line">    <span class="string">&quot;*&quot;</span> <span class="string">:</span> <span class="string">捌</span></span><br><span class="line">    <span class="string">&quot;(&quot;</span> <span class="string">:</span> <span class="string">玖</span></span><br><span class="line">    <span class="string">&quot;)&quot;</span> <span class="string">:</span> <span class="string">零</span></span><br><span class="line">    <span class="string">&quot;S&quot;</span> <span class="string">:</span> <span class="string">拾</span></span><br><span class="line">    <span class="string">&quot;B&quot;</span> <span class="string">:</span> <span class="string">佰</span></span><br><span class="line">    <span class="string">&quot;Q&quot;</span> <span class="string">:</span> <span class="string">千</span></span><br><span class="line">    <span class="string">&quot;Y&quot;</span> <span class="string">:</span> <span class="string">圆</span></span><br></pre></td></tr></table></figure><p>于是在「小写」态，只要按 <code>Shift</code> + 数字键即可打出大写数字。</p><p>用了几下，发现一处小小的不满意：敲 <code>$</code> 这个键，可选的符号有五个之多。想要打出殴元、英镑符号只得多敲几下 <code>$</code> 键使想要的符号高亮；但是按上、下方向键并没有效果，按符号前面标示的数字序号，更是不仅上屏了错误的符号、还多上屏一个数字——</p><p>这反映出两个问题。一是 <code>selector</code> 组件缺席使得选字、移动选字光标的动作未得到响应。立即加上：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="attr">engine:</span></span><br><span class="line">  <span class="attr">processors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">punctuator</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">selector</span>        <span class="comment"># 加在这里</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">express_editor</span></span><br><span class="line">  <span class="comment"># ...</span></span><br></pre></td></tr></table></figure><p>因为要让 <code>punctuator</code> 来转换数字键，所以 <code>selector</code> 得放在他后头。</p><p>好。二一个问题还在：无法用数字序号选字。为解决这个冲突，改用闲置的字母键来选字：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="attr">menu:</span></span><br><span class="line">  <span class="attr">alternative_select_keys:</span> <span class="string">&quot;acegi&quot;</span></span><br></pre></td></tr></table></figure><p>完工。</p><h3 id="罗马字之道"><a href="#罗马字之道" class="headerlink" title="罗马字之道"></a>罗马字之道</h3><p>毕竟，键盘上只有 47 个字符按键、94 个编码字符，对付百十个字还管使。可要输入上千个常用汉字，嫌键盘式输入的编码空间太小，必得采用多字符编码。</p><p>罗马字，以拉丁字母的特定排列作为汉语音节的转写形式。一个音节代表一组同音字，再由音节拼写组合成词、句。</p><p>凡此单字（音节）编码自然连用而生词、句的输入法，皆可用 <code>script_translator</code> 组件完成基于音节码切分的智能词句转换。他有个别名 <code>r10n_translator</code>——<code>r10n</code> 为 <code>romanization</code> 的简写。但不限于「拼音」、「注音」、「双拼」、「粤拼」等一族基于语音编码的输入法：形式相似者，如「速成」，虽以字形为本，亦可应用。</p><p>现在来把【数字之道】改成拼音→中文数字的变换。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">schema:</span></span><br><span class="line">  <span class="attr">schema_id:</span> <span class="string">numbers</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">数字之道</span></span><br><span class="line">  <span class="attr">version:</span> <span class="string">&quot;3&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">engine:</span></span><br><span class="line">  <span class="attr">processors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">speller</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">punctuator</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">selector</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">express_editor</span></span><br><span class="line">  <span class="attr">segmentors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">abc_segmentor</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">punct_segmentor</span></span><br><span class="line">  <span class="attr">translators:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">punct_translator</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">script_translator</span></span><br><span class="line"></span><br><span class="line"><span class="attr">translator:</span></span><br><span class="line">  <span class="attr">dictionary:</span> <span class="string">numbers</span></span><br><span class="line"></span><br><span class="line"><span class="attr">punctuator:</span></span><br><span class="line">  <span class="attr">half_shape:</span> <span class="meta">&amp;symtable</span></span><br><span class="line">    <span class="string">&quot;!&quot;</span> <span class="string">:</span> <span class="string">壹</span></span><br><span class="line">    <span class="string">&quot;@&quot;</span> <span class="string">:</span> <span class="string">贰</span></span><br><span class="line">    <span class="string">&quot;#&quot;</span> <span class="string">:</span> <span class="string">参</span></span><br><span class="line">    <span class="string">&quot;$&quot;</span> <span class="string">:</span> [ <span class="string">肆</span>, <span class="string">￥</span>, <span class="string">&quot;$&quot;</span>, <span class="string">&quot;€&quot;</span>, <span class="string">&quot;£&quot;</span> ]</span><br><span class="line">    <span class="string">&quot;%&quot;</span> <span class="string">:</span> [ <span class="string">伍</span>, <span class="string">百分之</span> ]</span><br><span class="line">    <span class="string">&quot;^&quot;</span> <span class="string">:</span> <span class="string">陆</span></span><br><span class="line">    <span class="string">&quot;&amp;&quot;</span> <span class="string">:</span> <span class="string">柒</span></span><br><span class="line">    <span class="string">&quot;*&quot;</span> <span class="string">:</span> <span class="string">捌</span></span><br><span class="line">    <span class="string">&quot;(&quot;</span> <span class="string">:</span> <span class="string">玖</span></span><br><span class="line">    <span class="string">&quot;)&quot;</span> <span class="string">:</span> <span class="string">零</span></span><br><span class="line">    <span class="string">&quot;S&quot;</span> <span class="string">:</span> <span class="string">拾</span></span><br><span class="line">    <span class="string">&quot;B&quot;</span> <span class="string">:</span> <span class="string">佰</span></span><br><span class="line">    <span class="string">&quot;Q&quot;</span> <span class="string">:</span> <span class="string">千</span></span><br><span class="line">    <span class="string">&quot;W&quot;</span> <span class="string">:</span> <span class="string">万</span></span><br><span class="line">    <span class="string">&quot;N&quot;</span> <span class="string">:</span> <span class="string">年</span></span><br><span class="line">    <span class="string">&quot;Y&quot;</span> <span class="string">:</span> [ <span class="string">月</span>, <span class="string">圆</span>, <span class="string">亿</span> ]</span><br><span class="line">    <span class="string">&quot;R&quot;</span> <span class="string">:</span> <span class="string">日</span></span><br><span class="line">    <span class="string">&quot;X&quot;</span> <span class="string">:</span> <span class="string">星期</span></span><br><span class="line">    <span class="string">&quot;J&quot;</span> <span class="string">:</span> <span class="string">角</span></span><br><span class="line">    <span class="string">&quot;F&quot;</span> <span class="string">:</span> <span class="string">分</span></span><br><span class="line">    <span class="string">&quot;Z&quot;</span> <span class="string">:</span> [ <span class="string">之</span>, <span class="string">整</span> ]</span><br><span class="line">    <span class="string">&quot;D&quot;</span> <span class="string">:</span> <span class="string">第</span></span><br><span class="line">    <span class="string">&quot;H&quot;</span> <span class="string">:</span> <span class="string">号</span></span><br><span class="line">    <span class="string">&quot;.&quot;</span> <span class="string">:</span> <span class="string">点</span></span><br><span class="line">  <span class="attr">full_shape:</span> <span class="meta">*symtable</span></span><br></pre></td></tr></table></figure><p>符号表里，把小写字母、数字键都空出来了。小写字母用来拼音，数字键用来选重。重点是本次用了 <code>script_translator</code> 这组件。与 <code>table_translator</code> 相似，该组件与 <code>translator/dictionary</code> 指名的词典相关联。</p><p>编制词典：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Rime dictionary</span></span><br><span class="line"><span class="comment"># encoding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">numbers</span></span><br><span class="line"><span class="attr">version:</span> <span class="string">&quot;1&quot;</span></span><br><span class="line"><span class="attr">sort:</span> <span class="string">by_weight</span></span><br><span class="line"><span class="attr">use_preset_vocabulary:</span> <span class="literal">true</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"></span><br><span class="line"><span class="string">一</span><span class="string">yi</span></span><br><span class="line"><span class="string">二</span><span class="string">er</span></span><br><span class="line"><span class="string">三</span><span class="string">san</span></span><br><span class="line"><span class="string">四</span><span class="string">si</span></span><br><span class="line"><span class="string">五</span><span class="string">wu</span></span><br><span class="line"><span class="string">六</span><span class="string">liu</span></span><br><span class="line"><span class="string">七</span><span class="string">qi</span></span><br><span class="line"><span class="string">八</span><span class="string">ba</span></span><br><span class="line"><span class="string">九</span><span class="string">jiu</span></span><br><span class="line"><span class="string">〇</span><span class="string">ling</span></span><br><span class="line"><span class="string">零</span><span class="string">ling</span></span><br><span class="line"><span class="string">十</span><span class="string">shi</span></span><br><span class="line"><span class="string">百</span><span class="string">bai</span></span><br><span class="line"><span class="string">千</span><span class="string">qian</span></span><br><span class="line"><span class="string">万</span><span class="string">wan</span></span><br><span class="line"><span class="string">亿</span><span class="string">yi</span></span><br><span class="line"><span class="string">年</span><span class="string">nian</span></span><br><span class="line"><span class="string">月</span><span class="string">yue</span></span><br><span class="line"><span class="string">日</span><span class="string">ri</span></span><br><span class="line"><span class="string">星</span><span class="string">xing</span></span><br><span class="line"><span class="string">期</span><span class="string">qi</span></span><br><span class="line"><span class="string">时</span><span class="string">shi</span></span><br><span class="line"><span class="string">分</span><span class="string">fen</span></span><br><span class="line"><span class="string">秒</span><span class="string">miao</span></span><br><span class="line"><span class="string">元</span><span class="string">yuan</span></span><br><span class="line"><span class="string">角</span><span class="string">jiao</span></span><br><span class="line"><span class="string">之</span><span class="string">zhi</span></span><br><span class="line"><span class="string">整</span><span class="string">zheng</span></span><br><span class="line"><span class="string">第</span><span class="string">di</span></span><br><span class="line"><span class="string">号</span><span class="string">hao</span></span><br><span class="line"><span class="string">点</span><span class="string">dian</span></span><br><span class="line"><span class="string">是</span><span class="string">shi</span></span><br></pre></td></tr></table></figure><p>※注意： <strong>不要</strong> 从网页复制以上代码到实际的词典文件！因为网页里制表符被转换成空格从而不符合 Rime 词典要求的格式。一些文本编辑器也会将使用者输入的制表符自动转换为空格，请注意检查和设置。</p><p>码表里给出了一个「示例」规格的小字集。其中包含几组重码字。</p><p>要诀 <code>sort: by_weight</code> 意图是不以码表的顺序排列重码字，而是比较字频。那字频呢？没写出来。</p><p>要诀 <code>use_preset_vocabulary: true</code> 用在输入法方案需要支持输入词组、而码表中词组相对匮乏时。编译输入法方案期间引入 Rime 默认的【八股文】词汇——及词频资料！这就是码表中未具字频的原因。</p><p>使用【八股文】，要注意码表所用的字形是否与该词汇表一致。八股文的词汇及词频统计都遵照 opencc 繁体字形标准。</p><p>如果缺少单字的编码定义，自然也无法导入某些词汇。所以本方案只会导入这个数字「小字集」上的词汇。</p><h3 id="用拼写运算定义简码"><a href="#用拼写运算定义简码" class="headerlink" title="用拼写运算定义简码"></a>用拼写运算定义简码</h3><p>如今有了一款专门输入数字的拼音输入法。比一比昇阳拼音、朙月拼音和地球拼音，还有哪里不一样？</p><p>很快我发现敲 <code>xingqiwu</code> 或 <code>xingqiw</code> 都可得到来自【八股文】的词组「星期五」，这很好。可是敲 <code>xqw</code> 怎会不中呢？</p><p>原来 <code>script_translator</code> 罗马字中译的方法是，将输入码序列切分为音节表中的拼写形式，再按音节查词典。不信你找本词典瞧瞧，是不是按完整的拼音（注音）编排的。Rime 词典也一样。并没有 <code>xqw</code> 这样的检索码。</p><p>现在我要用 Rime 独门绝活「拼写运算」来定义一种「音序查字法」。令 <code>x</code> 作 <code>xing</code> 的简码，<code>q</code> 作数字之道所有音节中起首为 <code>q</code> 者的简码，即略代音节 <code>qi</code> 与 <code>qian</code>。</p><p>「汉语拼音」里还有三个双字母的声符，<code>zh, ch, sh</code> 也可做简码。</p><p>添加拼写运算规则：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">schema:</span></span><br><span class="line">  <span class="comment"># ...</span></span><br><span class="line">  <span class="attr">version:</span> <span class="string">&quot;4&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#...</span></span><br><span class="line"></span><br><span class="line"><span class="attr">speller:</span></span><br><span class="line">  <span class="attr">algebra:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#x27;abbrev/^([a-z]).+$/$1/&#x27;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#x27;abbrev/^([zcs]h).+$/$1/&#x27;</span></span><br></pre></td></tr></table></figure><p>如此 Rime 便知，除了码表里那些拼音，还有若干简码也是行得通的拼写形式。再输入 <code>xqw</code>，Rime 将他拆开 <code>x&#39;q&#39;w</code>，再默默对应到音节码 <code>xing&#39;qi&#39;wan</code>、<code>xing&#39;qi&#39;wu</code>、<code>xing&#39;qian&#39;wan</code> 等等，一翻词典就得到了一个好词「星期五」，而其他的组合都说不通。</p><p>现在有无有悟到，罗马字转写式输入法与码表式输入法理念上的不同？</p><p>哈，做中了。试试看 <code>sss,sss,sssss,sssss</code></p><p>却好像不是我要的「四是四，十是十，十四是十四，四十是四十」……</p><p>好办。如果某些词汇在方案里很重要，【八股文】又未收录，那么，请添加至码表：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">numbers</span></span><br><span class="line"><span class="attr">version:</span> <span class="string">&quot;2&quot;</span></span><br><span class="line"><span class="attr">sort:</span> <span class="string">by_weight</span></span><br><span class="line"><span class="attr">use_preset_vocabulary:</span> <span class="literal">true</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="string">四是四</span></span><br><span class="line"><span class="string">十是十</span></span><br><span class="line"><span class="string">十四是十四</span></span><br><span class="line"><span class="string">四十是四十</span></span><br></pre></td></tr></table></figure><p>善哉。演示完毕。当然休想就此把 Rime 全盘掌握了。一本《指南书》，若能让读者入门，我止说「善哉〜」</p><p>再往后，就只有多读代码，才能见识到各种新颖、有趣的玩法。</p><h2 id="【三】最高武艺"><a href="#【三】最高武艺" class="headerlink" title="【三】最高武艺"></a>【三】最高武艺</h2><p>〔警告〕最后这部戏，对智力、技术功底的要求不一般。如果读不下去，不要怪我、不要怀疑自己的智商！</p><p>即使跳过本节书也无妨，只是不可忽略了下文《关于调试》这一节！（重要哇……）</p><p>请检查是否：</p><ul><li>※ 已将前两组实例分析透彻</li><li>※ 学习完了《拼写运算》</li><li>※ 知道双拼是神码</li><li>※ 预习 Rime 默认输入法方案之【朙月拼音】</li></ul><p>设计一款【智能 ABC 双拼】输入法方案做练习！</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Rime schema</span></span><br><span class="line"><span class="comment"># encoding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="attr">schema:</span></span><br><span class="line">  <span class="attr">schema_id:</span> <span class="string">double_pinyin_abc</span>  <span class="comment"># 专有的方案标识</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">智能</span> <span class="string">ABC</span> <span class="string">双拼</span></span><br><span class="line">  <span class="attr">version:</span> <span class="string">&quot;0.9&quot;</span></span><br><span class="line">  <span class="attr">author:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">佛振</span> <span class="string">&lt;chen.sst@gmail.com&gt;</span></span><br><span class="line">  <span class="attr">description:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    朙月拼音，兼容智能 ABC 双拼方案。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="attr">switches:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ascii_mode</span></span><br><span class="line">    <span class="attr">reset:</span> <span class="number">0</span></span><br><span class="line">    <span class="attr">states:</span> [ <span class="string">中文</span>, <span class="string">西文</span> ]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">full_shape</span></span><br><span class="line">    <span class="attr">states:</span> [ <span class="string">半角</span>, <span class="string">全角</span> ]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">simplification</span></span><br><span class="line">    <span class="attr">states:</span> [ <span class="string">汉字</span>, <span class="string">汉字</span> ]</span><br><span class="line"></span><br><span class="line"><span class="attr">engine:</span></span><br><span class="line">  <span class="attr">processors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ascii_composer</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">recognizer</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">key_binder</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">speller</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">punctuator</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">selector</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">navigator</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">express_editor</span></span><br><span class="line">  <span class="attr">segmentors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ascii_segmentor</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">matcher</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">abc_segmentor</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">punct_segmentor</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">fallback_segmentor</span></span><br><span class="line">  <span class="attr">translators:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">echo_translator</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">punct_translator</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">script_translator</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">reverse_lookup_translator</span></span><br><span class="line">  <span class="attr">filters:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">simplifier</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">uniquifier</span></span><br><span class="line"></span><br><span class="line"><span class="attr">speller:</span></span><br><span class="line">  <span class="attr">alphabet:</span> <span class="string">zyxwvutsrqponmlkjihgfedcba</span>  <span class="comment"># 呃，倒背字母表完全是个人喜好</span></span><br><span class="line">  <span class="attr">delimiter:</span> <span class="string">&quot; &#x27;&quot;</span>  <span class="comment"># 隔音符号用「&#x27;」；第一位的空白用来自动插入到音节边界处</span></span><br><span class="line">  <span class="attr">algebra:</span>  <span class="comment"># 拼写运算规则，这个才是实现双拼方案的重点。写法有很多种，当然也可以把四百多个音节码一条一条地列举</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">erase/^xx$/</span>             <span class="comment"># 码表中有几个拼音不明的字，编码成 xx 了，消灭他</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">derive/^([jqxy])u$/$1v/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/^zh/A/</span>            <span class="comment"># 替换声母键，用大写以防与原有的字母混淆</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/^ch/E/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/^sh/V/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/^([aoe].*)$/O$1/</span>  <span class="comment"># 添上固定的零声母 o，先标记为大写 O</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/ei$/Q/</span>            <span class="comment"># 替换韵母键</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/ian$/W/</span>           <span class="comment"># ※2</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/er$|iu$/R/</span>        <span class="comment"># 对应两种韵母的；音节 er 现在变为 OR 了</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/[iu]ang$/T/</span>       <span class="comment"># ※1</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/ing$/Y/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/uo$/O/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/uan$/P/</span>           <span class="comment"># ※3</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/i?ong$/S/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/[iu]a$/D/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/en$/F/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/eng$/G/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/ang$/H/</span>           <span class="comment"># 检查一下在此之前是否已转换过了带介音的 ang；好，※1 处有了</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/an$/J/</span>            <span class="comment"># 如果※2、※3 还无有出现在上文中，应该把他们提到本行之前</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/iao$/Z/</span>           <span class="comment"># 对——像这样让 iao 提前出场</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/ao$/K/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/in$|uai$/C/</span>       <span class="comment"># 让 uai 提前出场</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/ai$/L/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/ie$/X/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/ou$/B/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/un$/N/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/[uv]e$|ui$/M/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xlit/QWERTYOPASDFGHJKLZXCVBNM/qwertyopasdfghjklzxcvbnm/</span>  <span class="comment"># 最后把双拼码全部变小写</span></span><br><span class="line"></span><br><span class="line"><span class="attr">translator:</span></span><br><span class="line">  <span class="attr">dictionary:</span> <span class="string">luna_pinyin</span>     <span class="comment"># 与【朙月拼音】共用词典</span></span><br><span class="line">  <span class="attr">prism:</span> <span class="string">double_pinyin_abc</span>    <span class="comment"># prism 要以本输入法方案的名称来命名，以免把朙月拼音的拼写映射表覆盖掉</span></span><br><span class="line">  <span class="attr">preedit_format:</span>             <span class="comment"># 这段代码用来将输入的双拼码反转为全拼显示；待见双拼码的可以把这段拿掉</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/o(\w)/0$1/</span>        <span class="comment"># 零声母先改为 0，以方便后面的转换</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/(\w)q/$1ei/</span>       <span class="comment"># 双拼第二码转换为韵母</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/(\w)n/$1un/</span>       <span class="comment"># 提前转换双拼码 n 和 g，因为转换后的拼音里就快要出现这两个字母了，那时将难以分辨出双拼码</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/(\w)g/$1eng/</span>      <span class="comment"># 当然也可以采取事先将双拼码变为大写的办法来与转换过的拼音做区分，可谁让我是高手呢</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/(\w)w/$1ian/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/([dtnljqx])r/$1iu/</span>  <span class="comment"># 对应多种韵母的双拼码，按搭配的声母做区分（最好别用排除式如 [^o]r 容易出状况）</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/0r/0er/</span>             <span class="comment"># 另一种情况，注意先不消除 0，以防后面把 e 当作声母转换为 ch</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/([nljqx])t/$1iang/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/(\w)t/$1uang/</span>       <span class="comment"># 上一行已经把对应到 iang 的双拼码 t 消灭，于是这里不用再列举相配的声母</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/(\w)y/$1ing/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/([dtnlgkhaevrzcs])o/$1uo/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/(\w)p/$1uan/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/([jqx])s/$1iong/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/(\w)s/$1ong/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/([gkhaevrzcs])d/$1ua/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/(\w)d/$1ia/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/(\w)f/$1en/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/(\w)h/$1ang/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/(\w)j/$1an/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/(\w)k/$1ao/</span>       <span class="comment"># 默默检查：双拼码 o 已经转换过了</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/(\w)l/$1ai/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/(\w)z/$1iao/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/(\w)x/$1ie/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/(\w)b/$1ou/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/([nl])m/$1ve/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/([jqxy])m/$1ue/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/(\w)m/$1ui/</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;xform/(^|[ &#x27;])a/$1zh/&quot;</span>  <span class="comment"># 复原声母，音节开始处的双拼字母 a 改写为 zh；其他位置的才真正是 a</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;xform/(^|[ &#x27;])e/$1ch/&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;xform/(^|[ &#x27;])v/$1sh/&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/0(\w)/$1/</span>          <span class="comment"># 好了，现在可以把零声母拿掉啦</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/([nljqxy])v/$1ü/</span>   <span class="comment"># 这样才是汉语拼音 :-)</span></span><br><span class="line"></span><br><span class="line"><span class="attr">reverse_lookup:</span></span><br><span class="line">  <span class="attr">dictionary:</span> <span class="string">cangjie5</span></span><br><span class="line">  <span class="attr">prefix:</span> <span class="string">&quot;`&quot;</span></span><br><span class="line">  <span class="attr">tips:</span> <span class="string">〔仓颉〕</span></span><br><span class="line">  <span class="attr">preedit_format:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;xlit|abcdefghijklmnopqrstuvwxyz|日月金木水火土竹戈十大中一弓人心手口尸廿山女田难卜符|&quot;</span></span><br><span class="line">  <span class="attr">comment_format:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">xform/([nl])v/$1ü/</span></span><br><span class="line"></span><br><span class="line"><span class="attr">punctuator:</span></span><br><span class="line">  <span class="attr">import_preset:</span> <span class="string">default</span></span><br><span class="line"></span><br><span class="line"><span class="attr">key_binder:</span></span><br><span class="line">  <span class="attr">import_preset:</span> <span class="string">default</span></span><br><span class="line"></span><br><span class="line"><span class="attr">recognizer:</span></span><br><span class="line">  <span class="attr">import_preset:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">patterns:</span></span><br><span class="line">    <span class="attr">reverse_lookup:</span> <span class="string">&quot;`[a-z]*$&quot;</span></span><br></pre></td></tr></table></figure><p>完毕。</p><p>这是一道大题。通过改造拼写法而创作出新的输入法方案。</p><h2 id="【四】标准库"><a href="#【四】标准库" class="headerlink" title="【四】标准库"></a>【四】标准库</h2><p>如果需要制作完全属于自己的输入法方案，少不了要了解 Rime 的标准库。此时，请客倌品读<a href="https://github.com/LEOYoon-Tsaw/Rime_collections/blob/master/Rime_description.md">《Rime 方案制作详解》</a>。更多新意，就在你的笔下！</p><h2 id="关于调试"><a href="#关于调试" class="headerlink" title="关于调试"></a>关于调试</h2><p>如此复杂的输入法方案，很可能需要反覆调试方可达到想要的结果。</p><p>请于试验时及时查看日志中是否包含错误信息。日志文件位于：</p><ul><li>【中州韵】 <code>/tmp/rime.ibus.*</code></li><li>【小狼毫】 <code>%TEMP%\rime.weasel.*</code></li><li>【鼠鬚管】 <code>$TMPDIR/rime.squirrel.*</code></li><li>各发行版的早期版本 <code>用户资料夹/rime.log</code></li></ul><p>按照日志的级别分为 INFO &#x2F; 信息、WARNING &#x2F; 警告、ERROR &#x2F; 错误。<br>后两类应重点关注，如果新方案部署后不可用或输出与设计不一致，原因可能在此。</p><p>没有任何错误信息，就是不好使，有可能是码表本身的问题，比如把码表中文字和编码两列弄颠倒了——Rime 等你输入由汉字组成的编码，然而键盘没有可能做到这一点（否则也不再需要输入法了）。</p><p>后续有计划为输入法方案创作者开发名为<a href="http://rime.github.io/blog/2013/08/28/spelling-algebra-debugger/">「拼写运算调试器」</a>的工具，能够较直观地看到每一步拼写运算的结果。有助于定义双拼这样大量使用拼写运算的方案。</p><h1 id="东风破"><a href="#东风破" class="headerlink" title="东风破"></a>东风破</h1><p>「东风破早梅，向暖一枝开。」</p><p>构想在 Rime 输入软件完善后，能够连结汉字字形、音韵、输入法爱好者的共同兴趣，形成稳定的使用者社羣，搭建一个分享知识的平台。</p><p><a href="https://github.com/rime/plum">【东风破】</a>，定义为配置管理器及 Rime 输入法方案仓库，是广大 Rime 用家分享配置和输入法方案的平台。</p><p>Rime 是一款强调个性的输入法。</p><p>Rime 不要定义输入法应当是哪个样、而要定义输入法可以玩出哪些花样。</p><p>Rime 不可能通过默认更多的输入法方案来满足玩家的需求；真正的玩家一定有一般人想不到的高招。</p><p>未来一定会有，<a href="https://github.com/rime/plum">【东风破】</a>（注：现已投入运行），让用家轻松地找到最新、最酷、最适合自己的 Rime 输入法方案。</p><!-- 用 Vim 命令在相邻中西文字符之间添加一个空隔：:%sm/[㐀-﨩]\zs\ze[a-zA-Z0-9`%]\|[a-zA-Z0-9`%]\zs\ze[㐀-﨩]/ /g若需要手动确认每处修改，请在末尾添加参数 c。-->]]></content>
      
      
      <categories>
          
          <category> 软件安装与使用 </category>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安装与使用 </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>音乐文件 Cue 切割方法</title>
      <link href="/vll-pages/posts/9a60fb94.html"/>
      <url>/vll-pages/posts/9a60fb94.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>【阅读建议】互联网上的无损音乐大多采用　.cue　文件格式分享。在　Ｗindows　中由很多可以做单曲分割的软件，但在　Linux 中需要在命令行下处理，本文介绍了一组可用的工具及其使用方法。</p><h2 id="1-什么是-Cue-文件？"><a href="#1-什么是-Cue-文件？" class="headerlink" title="1 什么是 Cue 文件？"></a>1 什么是 Cue 文件？</h2><p><code>Cue　文件</code> 指光盘映像（镜像）辅助文件或称标记文件，按照文本文件格式编制。它在刻录光盘映像文件时，起很重要的作用。它可以指挥刻录软件刻什么格式，刻录那些内容，从哪里开始，到哪里结束，附加什么信息等等。有了 cue 文件，既可以减少刻录的准备工作以提高刻录效率，又可以保证刻录的准确性。</p><p>通常网络上面下载无损 CD 抓取文件都是 <code>一个 cue 文件 + 一个音频文件 (wav、flac、ape)</code> 的形式。这两个文件组成了完整的专辑歌曲。</p><h2 id="2-音频格式转换"><a href="#2-音频格式转换" class="headerlink" title="2 音频格式转换"></a>2 音频格式转换</h2><p>如果音频文件是 <code>wav 格式</code>，就需要转换为 <code>flac 格式</code>。如果音频文件为　<code>ape 格式</code> ，可以先转换成 <code>wav</code>，然后转 <code>flac 格式</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ape 转　wav</span></span><br><span class="line">ffmpeg -i CDImage.ape CDImage.wav</span><br><span class="line"></span><br><span class="line"><span class="comment"># wav 转　flac</span></span><br><span class="line">flac CDImage.wav CDImage.flac</span><br></pre></td></tr></table></figure><h2 id="3-分割音频"><a href="#3-分割音频" class="headerlink" title="3 分割音频"></a>3 分割音频</h2><p>分割音频的工具为 <code>shntool</code>，使用方法为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 按歌名分割文件</span></span><br><span class="line">shntool <span class="built_in">split</span> -t %t -f CDImage.cue -o flac CDImage.flac -d .</span><br></pre></td></tr></table></figure><p><code>-t</code> 参数表示分割出来的文件采用什么文件名，<code>%t</code> 表示用歌曲名字命名；<code>-f</code> 表示输入的 <code>cue 文件</code>；<code>-o</code> 指定输出格式； <code>-d</code> 参数为输出目录，此例用点表示当前目录。</p><p>注：　上述　<code>ffmpeg</code>、<code>flac</code> 和　<code>shntool</code>　请自行安装。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> pacman -S ffmpeg flac shntool</span><br></pre></td></tr></table></figure><h2 id="4-歌名乱码问题"><a href="#4-歌名乱码问题" class="headerlink" title="4 歌名乱码问题"></a>4 歌名乱码问题</h2><p>很多 <code>cue 文件</code> 都是在 Windows 下面制作的，在 Linux 会导致乱码，需要进行转码，常见转码是从　<code>gbk　编码</code> 到 <code>utf8 编码</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iconv -f gbk -t utf8 CDImage.cue &gt; CDImage-1.cue</span><br></pre></td></tr></table></figure><p>改编自 <a href="https://blog.csdn.net/weixin_39849762/article/details/111794150">CSDN博文</a></p>]]></content>
      
      
      <categories>
          
          <category> 软件安装与使用 </category>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安装与使用 </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git 如何一次 Push 至多个远程仓库</title>
      <link href="/vll-pages/posts/ae3cd9c2.html"/>
      <url>/vll-pages/posts/ae3cd9c2.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h2 id="Git-如何一次-Push-至多个远程仓库"><a href="#Git-如何一次-Push-至多个远程仓库" class="headerlink" title="Git 如何一次 Push 至多个远程仓库"></a>Git 如何一次 Push 至多个远程仓库</h2><p>相信你一定不想看到辛苦写完的程序，因为意外造成 Source Code 遗失，当然还择良好的 Git server 服务是必要条件，但再稳固的服务也有可能出现被 DDOS 攻击或是服务管理员不小心把 DB 删除…等等，各式各样你意想不到的情境造成服务中断，所以为远程仓库进行备份也是应该的。</p><p>虽然备份很重要，但身为一个讲求效率的工程师，你一定也不想每次 push 到 remote 都要反覆操作 push 多次，就我们来看看可以如何设置，让一次动作就可以同时 push 到多个远程仓库。</p><h3 id="一、设置多个-push-的远程仓库"><a href="#一、设置多个-push-的远程仓库" class="headerlink" title="一、设置多个 push 的远程仓库"></a>一、设置多个 push 的远程仓库</h3><ul><li><p>原本工程的 remote 设置</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20220402205111-2ed2.webp" alt="1defaultremote"></p></li><li><p>使用指令加入其他 remote</p><ul><li>HTTPS 类型</li></ul><blockquote><p>指令：<br><code> git remote set-url --add --push origin https://gitserver/repository.git</code><br>范例：<br><code>git remote set-url --add --push origin https://github.com/yowko/TestMilestone.git</code></p></blockquote><ul><li>SSH</li></ul><blockquote><p>指令：<br> <code>git remote set-url --add --push origin ssh://git@gitserver:username/application.git</code><br>范例：<br><code>git remote set-url --add --push origin git@github.com:yowko/TestMilestone.git</code></p></blockquote></li><li><p>注意事项</p><ul><li>如果原本已有 remote 设置，则需要再手动 add 一次，否则原有 push 设置会被覆盖</li></ul></li></ul><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20220402205057-b7b4.webp" alt="2resetsetting"></p><h3 id="二、执行-push-操作"><a href="#二、执行-push-操作" class="headerlink" title="二、执行 push 操作"></a>二、执行 push 操作</h3><ul><li><p>push 前，先确认上述多个 remote 是否设置成功</p><blockquote><p><code>git remote -v</code></p></blockquote></li></ul><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20220402205128-e2e9.webp" alt="3multipleremote"></p><ul><li><p>将本地仓库 push 至多个 remote 仓库</p><blockquote><p> <code>git push origin</code></p></blockquote></li></ul><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20220402205123-867e.webp" alt="5success"></p>]]></content>
      
      
      <categories>
          
          <category> 软件安装与使用 </category>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安装与使用 </tag>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Youyun</title>
      <link href="/vll-pages/posts/7ea2da9d.html"/>
      <url>/vll-pages/posts/7ea2da9d.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20220401161431-ec7e.webp"></p>]]></content>
      
      
      <categories>
          
          <category> 软件安装与使用 </category>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安装与使用 </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 环境配置文件处理流程</title>
      <link href="/vll-pages/posts/f7905162.html"/>
      <url>/vll-pages/posts/f7905162.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h1 id="Linux-环境配置文件处理流程"><a href="#Linux-环境配置文件处理流程" class="headerlink" title="Linux 环境配置文件处理流程"></a>Linux 环境配置文件处理流程</h1><p>今天遇到一个问题，同样的软件从终端运行能够正常使用中文输入法，但从图形界面下运行就无法正常处理中文输入法了。思考了下，感觉应该是两者的环境配置文件不同导致的。查阅了下 Linux 的相关文档，确实存在这个问题，在此记录备忘。</p><h2 id="1-Shell-环境配置文件的处理流程"><a href="#1-Shell-环境配置文件的处理流程" class="headerlink" title="1 Shell 环境配置文件的处理流程"></a>1 Shell 环境配置文件的处理流程</h2><h3 id="1-1-配置加载流程"><a href="#1-1-配置加载流程" class="headerlink" title="1.1 配置加载流程"></a>1.1 配置加载流程</h3><ul><li><p>全局配置文件：位于 <code>/etc/profile</code> </p></li><li><p>私有配置文件：位于 <code> ~/.profile</code></p></li><li><p>Shell 自定义配置文件， 依据指定的 Shell 有所不同，例如：</p><ul><li>对于 <code>bash</code> ，有 <code> (~/.bash_profile | ~/.bash_login)</code> -&gt;  <code>~/.bashrc</code> -&gt; <code>/etc/bashrc</code></li><li>对于 <code>zsh</code>，有 <code>~/.zshrc</code></li></ul></li></ul><h3 id="1-2-环境变量的设置"><a href="#1-2-环境变量的设置" class="headerlink" title="1.2 环境变量的设置"></a>1.2 环境变量的设置</h3><p>除了在上述配置文件中，通过 <code>export 变量名=变量值</code> 这种方式设置环境变量外，Linux 还可以通过单独的配置文件设置环境变量。这些配置文件的结构也非常简单，每行设置一个环境变量，语法格式为  <code>变量名=变量值</code>。其中：</p><ul><li>全局环境变量： 位于 <code>/etc/environment</code> 中</li><li>私有环境变量： 位于 <code>~/.pam_environment</code> 中</li></ul><h3 id="1-3-到底使用哪个文件做配置？"><a href="#1-3-到底使用哪个文件做配置？" class="headerlink" title="1.3 到底使用哪个文件做配置？"></a>1.3 到底使用哪个文件做配置？</h3><ul><li><p>如果希望系统的所有用户都具有同一配置，则使用 <code>/etc/profile</code> 和 <code>/etc/environment</code> 。</p></li><li><p>如果希望某个用户的所有 Shell 具有同一配置，则使用 <code>~/.profile</code> 和 <code>~/.pam_environment</code>。</p></li><li><p>如果仅希望某个用户的某个特定 Shell 具有特定配置，则使用 <code>~/.bashrc</code> 、<code>~/.zshrc</code> 等。</p></li></ul><p>对于只有一个用户的个人电脑桌面应用场景来说，通常只需要根据使用的 Shell 在 <code>~/.bashrc</code> 等文件中配置即可，如果频繁在多个 Shell 之间切换，则可以考虑在 <code>~/profile</code> 和 <code>~/.pam_environment</code> 中一劳永逸地配置。</p><h2 id="2-X-图形管理器环境配置文件的处理流程"><a href="#2-X-图形管理器环境配置文件的处理流程" class="headerlink" title="2  X 图形管理器环境配置文件的处理流程"></a>2  X 图形管理器环境配置文件的处理流程</h2><h3 id="2-1-不同于-Shell-的环境配置"><a href="#2-1-不同于-Shell-的环境配置" class="headerlink" title="2.1 不同于 Shell 的环境配置"></a>2.1 不同于 Shell 的环境配置</h3><p>首先要理解，Linux 图形管理器的环境参数与 Shell 中环境参数是不同的两套体系。通常在一些图形管理器的启动过程中，会先加载 Shell 环境配置文件（如： <code>/etc/profile</code>、<code>~/.profile</code> 等），然后再加载图形管理器的环境配置文件（如： <code>/etc/xprofile</code> 、 <code>/usr/local/etc/xprofile</code> 、<code>~/.xprofile</code> 等 ），但为稳妥起见，最好将 Shell 和图形环境的配置分开设置。否则，很容易出现在 Shell 中运行良好，而在图形管理器中运行时却异常的现象（或反之）。</p><h3 id="2-2-主要的环境配置文件"><a href="#2-2-主要的环境配置文件" class="headerlink" title="2.2 主要的环境配置文件"></a>2.2 主要的环境配置文件</h3><p>涉及图形界面环境的配置文件常见的有三种：<code>.Xsession</code> 、<code>.xinitrc</code> 和 <code>.xprofile</code> 。其中 <code>.xinitrc</code> 是历史最悠久的配置文件，而 .Xsession 则被很多社区推荐与 <code>.xinitrc</code> 保持一致，例如：使用 <code>ln -s </code>.xinitrc<code> .xsession</code> 建立软链接。但不同的 Linux 发行版和图形管理器具体使用其中哪一个或哪一些，往往都有自己的约定。实际经验中，使用 <code>.xinitrc</code> 和 <code>.xprofile</code> 的比较普遍。例如，在原生情况下， <code>.xinitrc</code> 和 <code>.xprofile</code> 通常都会被以下图形管理器引用：</p><ul><li><p>GDM：在 &#x2F;etc&#x2F;gdm&#x2F;Xsession 中被引用；</p></li><li><p>KDM：在 &#x2F;usr&#x2F;share&#x2F;config&#x2F;kdm&#x2F;Xsession 中被引用；</p></li><li><p>LightDM：在 &#x2F;etc&#x2F;lightdm&#x2F;Xsession 中被引用；</p></li><li><p>LXDM：在 &#x2F;etc&#x2F;lxdm&#x2F;Xsession 中被引用；</p></li><li><p>SDDM：在 &#x2F;usr&#x2F;share&#x2F;sddm&#x2F;scripts&#x2F;Xsession 中被引用。</p></li></ul><p>那么，.xprofile 相较于 .xinitrc&#x2F;.xsession 有什么差別呢? </p><ul><li><p><code>.xprofile</code> 是被图形管理器（ GDM&#x2F;KDM等）调用的，所以它比较单纯，只需要在里面加载你的环境配置和命令即可， <code>.xprofile</code> 被执行結束之后，控制权会返回图形管理器。 </p></li><li><p><code>.xinitrc/.xsession</code> 比较复杂一些，因为 <code>.xinitrc</code> 或 <code>.xsession</code> 文件的最后一句应当是 <code>exec gnome-session</code> 或 <code>exec startkde</code> 或 <code>exec enlightenment</code> 或 <code>exec icewm</code> 之类的会话程序，无论哪种图形管理器，在调用这两个文件时，通常都意味着控制权会被相应的窗口管理器接管 。</p></li></ul><h3 id="2-3-配置文件加载过程"><a href="#2-3-配置文件加载过程" class="headerlink" title="2.3  配置文件加载过程"></a>2.3  配置文件加载过程</h3><p>对于大部分应用场景来说，我们只需要配置 <code>.xprofile</code> 即可。下面是某图形管理器启动时，调用环境配置文件的过程。</p><figure class="highlight plaintext"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[ -f /etc/xprofile ] &amp;&amp; . /etc/xprofile</span><br><span class="line">[ -f /usr/local/etc/xprofile ] &amp;&amp; . /usr/local/etc/xprofile</span><br><span class="line">[ -f $HOME/.xprofile ] &amp;&amp; . $HOME/.xprofile</span><br></pre></td></tr></table></figure><h3 id="2-4-到底该用哪个配置文件？"><a href="#2-4-到底该用哪个配置文件？" class="headerlink" title="2.4 到底该用哪个配置文件？"></a>2.4 到底该用哪个配置文件？</h3><ul><li><p>如果希望系统中所有用户的图形界面，都享有同一配置，使用 <code>/etc/xprofile</code> 或 <code>/usr/local/etc/xprofile</code></p></li><li><p>如果希望某个用户的图形界面有自己特定的配置，使用 <code>~/.xprofile</code></p></li></ul><p>对于只有一个用户的个人电脑桌面应用场景来说，通常只需要在 <code>~/.xprofile</code> 文件中配置即可。</p>]]></content>
      
      
      <categories>
          
          <category> 软件安装与使用 </category>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安装与使用 </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>解决 hexo-renderer-kramed 渲染冲突的部分问题</title>
      <link href="/vll-pages/posts/59ae30a0.html"/>
      <url>/vll-pages/posts/59ae30a0.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h1 id="解决-hexo-renderer-kramed-渲染冲突的部分问题"><a href="#解决-hexo-renderer-kramed-渲染冲突的部分问题" class="headerlink" title="解决 hexo-renderer-kramed 渲染冲突的部分问题"></a>解决 hexo-renderer-kramed 渲染冲突的部分问题</h1><p>【阅读建议】因 <a href="https://www.npmjs.com/package/hexo-renderer-marked">hexo-renderer-marked</a> 不支持数学公式的渲染，其他渲染器又有一些问题，如 <a href="https://www.npmjs.com/package/hexo-renderer-pandoc">hexo-renderer-pandoc</a> 过于沉重，<a href="https://www.npmjs.com/package/hexo-renderer-markdown-it">hexo-renderer-markdown-it</a> 对 NexT 主题支持不佳，因此选用 <a href="https://www.npmjs.com/package/hexo-renderer-kramed">hexo-renderer-kramed</a> 渲染器。本文解决了该渲染器在渲染 Markdown 及数学公式时遇到的部分问题。</p><p>【原文地址】：<a href="https://corecabin.cn/2021/08/14/solve-some-problems-of-hexo-renderer-kramed-rendering-conflicts/">https://corecabin.cn/2021/08/14/solve-some-problems-of-hexo-renderer-kramed-rendering-conflicts/</a></p><h2 id="1-hexo-renderer-kramed-不能渲染-Todo-List-的问题"><a href="#1-hexo-renderer-kramed-不能渲染-Todo-List-的问题" class="headerlink" title="1 hexo-renderer-kramed 不能渲染 Todo List 的问题"></a>1 hexo-renderer-kramed 不能渲染 Todo List 的问题</h2><p>原来的渲染器 hexo-renderer-marked 是支持 Todo List 的，翻了下 <a href="https://github.com/hexojs/hexo-renderer-marked">hexo-renderer-marked 的 GitHub 仓库</a> 的 Pull Request。发现在这个 <a href="https://github.com/hexojs/hexo-renderer-marked/pull/32">PR</a> 里，hexo-renderer-marked 加入了对 Todo List 的支持，那就拷贝这个 PR 中 <code>lib/renderer.js</code> 里新增的代码：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Support To-Do List</span></span><br><span class="line"><span class="title class_">Renderer</span>.<span class="property"><span class="keyword">prototype</span></span>.<span class="property">listitem</span> = <span class="keyword">function</span>(<span class="params">text</span>) &#123;</span><br><span class="line">  <span class="keyword">if</span> (<span class="regexp">/^\s*\[[x ]\]\s*/</span>.<span class="title function_">test</span>(text)) &#123;</span><br><span class="line">    text = text.<span class="title function_">replace</span>(<span class="regexp">/^\s*\[ \]\s*/</span>, <span class="string">&#x27;&lt;input type=&quot;checkbox&quot;&gt;&lt;/input&gt; &#x27;</span>).<span class="title function_">replace</span>(<span class="regexp">/^\s*\[x\]\s*/</span>, <span class="string">&#x27;&lt;input type=&quot;checkbox&quot; checked&gt;&lt;/input&gt; &#x27;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&lt;li style=&quot;list-style: none&quot;&gt;&#x27;</span> + text + <span class="string">&#x27;&lt;/li&gt;\n&#x27;</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&lt;li&gt;&#x27;</span> + text + <span class="string">&#x27;&lt;/li&gt;\n&#x27;</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>加入到本地的 <code>hexo</code> 文件夹的 <code>/node_modules/hexo-renderer-kramed/lib/renderer.js</code> 的第 19 行中。</p><p>保存后重新<code>hexo clean</code>、<code>hexo g</code>，就渲染成功了。</p><p>其实这个解决方案不是我翻 PR 找到的，而是我在 GitHub 乱搜索时 <a href="https://github.com/wafer-li/wafer-li.github.io/blob/source/src/blog-corners/tech/tinkering/hexo/Hexo%20Experience.md">在这里</a> 找到的。本来并不指望解决这个问题的，又学到一招。</p><h2 id="2-hexo-renderer-kramed-渲染-MathJax-时与-Markdown-语法冲突"><a href="#2-hexo-renderer-kramed-渲染-MathJax-时与-Markdown-语法冲突" class="headerlink" title="2 hexo-renderer-kramed 渲染 MathJax 时与 Markdown 语法冲突"></a>2 hexo-renderer-kramed 渲染 MathJax 时与 Markdown 语法冲突</h2><p>关于如何修改语义冲突，网上的教程讲得很详细了，比如：</p><ul><li><a href="https://www.jianshu.com/p/d95a4795f3a8">hexo 下 LaTeX 无法显示的解决方案 - zealscott - 简书</a></li><li><a href="https://murphypei.github.io/blog/2019/03/hexo-render-mathjax.html">解决 hexo-next 主题和 mathjax 下划线冲突问题 | 拾荒志</a></li></ul><p>但网上找到的教程通常只讲述了如何修改下划线 <code>_</code> 和反斜杠 <code>\\</code> 的冲突。我将遇到的所有问题一一整理如下。</p><p>下文的问题通常是行内公式的问题，目前看来公式块受到的影响比较小。</p><h3 id="2-1-行内公式与行内代码冲突"><a href="#2-1-行内公式与行内代码冲突" class="headerlink" title="2.1 行内公式与行内代码冲突"></a>2.1 行内公式与行内代码冲突</h3><p><strong>问题描述</strong>：把行内公式作为行内代码输入时（如下），会显示异常。例如：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">`$ a+b $`</span></span><br></pre></td></tr></table></figure><p>翻一下 <a href="https://github.com/sun11/hexo-renderer-kramed">hexo-renderer-kramed 的文档</a>，发现作者写在这里了：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/bayes_20211103_094215_cd36.webp"></p><p>所以要想输入行内代码中的公式，在<code>$</code>前后加上空格就行了：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">` $ a+b $ `</span></span><br></pre></td></tr></table></figure><h3 id="2-2-下划线-被转义为斜体而非-LaTeX-下标"><a href="#2-2-下划线-被转义为斜体而非-LaTeX-下标" class="headerlink" title="2.2 下划线_被转义为斜体而非 LaTeX 下标"></a>2.2 下划线<code>_</code>被转义为斜体而非 LaTeX 下标</h3><p><strong>问题描述</strong>：当公式中出现多个下划线时，会被 kramed 渲染为 Markdown 斜体，导致公式显示异常。</p><p>Markdown 本身的语法是支持<code>*</code>和<code>_</code>都被转义为_斜体_的，所以可以取消掉 kramed 对<code>_</code>的转义。</p><p>打开本地<code>hexo</code>文件夹下的<code>/node_modules/kramed/lib/rules/inline.js</code>，找到第 20 行如下代码：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">em</span>: <span class="regexp">/^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure><p>修改为：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">em</span>: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure><p>就取消了对下划线<code>_</code>的转义。</p><p>以后使用斜体的话只用<code>*</code>符号就够了。如果 LaTeX 要使用大量<code>*</code>符号，可用<code>\ast</code>代替。</p><h3 id="2-3-反斜杠-被转义为-而非-LaTeX-换行"><a href="#2-3-反斜杠-被转义为-而非-LaTeX-换行" class="headerlink" title="2.3 反斜杠\\被转义为\而非 LaTeX 换行"></a>2.3 反斜杠<code>\\</code>被转义为<code>\</code>而非 LaTeX 换行</h3><p><strong>问题描述</strong>：当公式中出现<code>\\</code>表示换行时，会被 kramed 渲染为<code>\</code>，导致公式显示异常。</p><p>取消掉对<code>\\</code>的转义就行了。</p><p>同上，找到<code>inline.js</code>中第 11 行如下代码：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">escape</span>: <span class="regexp">/^\\([\\`*\[\]()#$+\-.!_&gt;])/</span>,</span><br></pre></td></tr></table></figure><p>修改为：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">escape</span>: <span class="regexp">/^\\([`*\[\]()#$+\-.!_&gt;])/</span>,</span><br></pre></td></tr></table></figure><p>就取消了对反斜杠<code>\\</code>的转义。</p><h3 id="2-4-LaTeX-紧贴符-不被转义"><a href="#2-4-LaTeX-紧贴符-不被转义" class="headerlink" title="2.4 LaTeX 紧贴符\!不被转义"></a>2.4 LaTeX 紧贴符<code>\!</code>不被转义</h3><p><strong>问题描述</strong>：当公式中出现<code>\!</code>表示紧贴符号时，会被 kramed 渲染为<code>!</code>，导致公式显示异常。</p><p>同上，把<code>escape:</code>后的正则表达式中的<code>!</code>去掉即可，取消掉对<code>\!</code>的转义。</p><h3 id="2-5-反斜杠加竖线-被转义为-而非-LaTeX-双竖线"><a href="#2-5-反斜杠加竖线-被转义为-而非-LaTeX-双竖线" class="headerlink" title="2.5 反斜杠加竖线\|被转义为|而非 LaTeX 双竖线"></a>2.5 反斜杠加竖线<code>\|</code>被转义为<code>|</code>而非 LaTeX 双竖线</h3><p><strong>问题描述</strong>：当公式中出现<code>\|</code>表示紧贴符号时，会被 kramed 渲染为<code>|</code>，导致公式显示异常。</p><p>真是个困扰了半天的 bug，上面的<code>escape:</code>后面也没有<code>|</code>啊。</p><p>找了我 n 个小时，原来还是<code>inline.js</code>代码的问题。</p><p>找到第 64 行如下代码：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">escape</span>: <span class="title function_">replace</span>(inline.<span class="property">escape</span>)(<span class="string">&#x27;])&#x27;</span>, <span class="string">&#x27;~|])&#x27;</span>)(),</span><br></pre></td></tr></table></figure><p>修改为：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">escape</span>: <span class="title function_">replace</span>(inline.<span class="property">escape</span>)(<span class="string">&#x27;])&#x27;</span>, <span class="string">&#x27;~])&#x27;</span>)(),</span><br></pre></td></tr></table></figure><p>就取消了对<code>\|</code>的转义。</p><p>执行完以上步骤后，记得<code>hexo clean</code>、<code>hexo g</code>走一波。</p><p><strong>总结：哪里被 kramed 转义就检查<code>escape</code>对应的部分。</strong></p><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h2><ul><li>多上 GitHub 看看原仓库的文档、issues、PR，会有收获的，实在不行直接搜，比百度好使。</li><li>熟悉流程，<strong>Hexo 的原理是根据渲染器，把 Markdown 语法转为 HTML 语法</strong>。所以一些显示 bug 不一定是主题的问题，而很有可能是渲染器的问题。所以看看渲染器的源码总是有收获的。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 软件安装与使用 </category>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安装与使用 - Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Apache Spark简明教程</title>
      <link href="/vll-pages/posts/97e80854.html"/>
      <url>/vll-pages/posts/97e80854.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h1 id="Apache-Spark简明教程"><a href="#Apache-Spark简明教程" class="headerlink" title="Apache Spark简明教程"></a>Apache Spark简明教程</h1><h2 id="1-Spark集群物理结构"><a href="#1-Spark集群物理结构" class="headerlink" title="1 Spark集群物理结构"></a>1 Spark集群物理结构</h2><p>注：Spark的运行不一定依赖于Spark集群，还支持本地、YARN、MESOS。Spark集群只是Apache Spark提供的一种分布式计算环境。</p><h3 id="（1）Master"><a href="#（1）Master" class="headerlink" title="（1）Master"></a>（1）Master</h3><ul><li>（1）监听Worker，看Worker是否正常工作 </li><li>（2）接收worker的注册并管理所有的worker</li><li>（3）接收和调度client提交的application，并向worker分派任务</li></ul><h3 id="（2）Worker"><a href="#（2）Worker" class="headerlink" title="（2）Worker"></a>（2）Worker</h3><ul><li>（1）通过RegisterWorker注册到Master</li><li>（2）定时发送心跳给Master</li><li>（3）根据master分派的application配置进程环境，并启动执行Task所需的进程（StandaloneExecutorBackend）</li></ul><h2 id="2-系统运行时结构"><a href="#2-系统运行时结构" class="headerlink" title="2 系统运行时结构"></a>2 系统运行时结构</h2><h3 id="（1）Client角色"><a href="#（1）Client角色" class="headerlink" title="（1）Client角色"></a>（1）Client角色</h3><ul><li>提交任务者</li></ul><h3 id="（2）Driver角色"><a href="#（2）Driver角色" class="headerlink" title="（2）Driver角色"></a>（2）Driver角色</h3><ul><li><p>Driver是执行开发程序中main方法的进程</p></li><li><p>功能</p><ul><li>（1）把application转为task</li><li>（2）跟踪Executor的运行状况</li><li>（3）为Executor节点分派任务</li><li>（4）UI展示应用运行状况</li></ul></li></ul><h3 id="（3）Executor角色"><a href="#（3）Executor角色" class="headerlink" title="（3）Executor角色"></a>（3）Executor角色</h3><ul><li><p>Executor为工作进程，负责运行Task</p></li><li><p>功能</p><ul><li>（1）运行application中的task，并将状态信息返回给driver进程</li><li>（2）为用户application中要求缓存的RDD提供内存式存储。RDD是直接缓存在Executor进程内的，因此任务可以在运行时充分利用缓存数据加速运算。</li></ul></li></ul><h3 id="（4）ResourceManager角色"><a href="#（4）ResourceManager角色" class="headerlink" title="（4）ResourceManager角色"></a>（4）ResourceManager角色</h3><ul><li>在集群上为Application获取资源的外部服务</li><li>例如：Local、Standalone、Mesos或Yarn等资源管理系统，对应四种作业模式</li></ul><h3 id="（5）运行视图"><a href="#（5）运行视图" class="headerlink" title="（5）运行视图"></a>（5）运行视图</h3><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/articles/spark_20210409120039_3c.webp" style="zoom:150%;" /><h2 id="3-系统运行模式"><a href="#3-系统运行模式" class="headerlink" title="3 系统运行模式"></a>3 系统运行模式</h2><p>Spark根据可用资源及其调度系统，对应了四种运行（或者作业）模式：</p><ul><li>Local模式：本地运行模式，主要用于开发者在本地开发调试</li><li>StandAlone模式：在spark集群上运行程序的模式</li><li>Spark On YARN模式：在Hadoop集群上部署和运行程序的模式</li><li>Spark On Mesos模式：在Mesos资源管理器上运行程序的模式</li></ul><h2 id="4-Local运行模式"><a href="#4-Local运行模式" class="headerlink" title="4 Local运行模式"></a>4 Local运行模式</h2><ul><li><p>本地多线程并发，提交任务时通过–master参数指定</p></li><li><p>三种模式</p><ul><li><p>Local</p><ul><li>所有计算在一个进程中进行，没有并行计算</li></ul></li><li><p>Local[K]</p><ul><li><p>运行K个Worker线程，通常CPU有几个Core就执行几个线程，以便最大化利用CPU计算能力</p><ul><li>cat &#x2F;proc&#x2F;cpuinfo | grep ‘processor’ | wc -l</li></ul></li></ul></li><li><p>Local[*]</p><ul><li>直接根据CPU核数量来设置线程数，如果提交任务时不指定master参数，则默认为本模式</li></ul></li></ul></li></ul><h2 id="5-Standalone运行模式"><a href="#5-Standalone运行模式" class="headerlink" title="5 Standalone运行模式"></a>5 Standalone运行模式</h2><ul><li><p>模式特点</p><ul><li>采用Spark集群作为部署与运行环境，需要部署spark集群，并使用集群的Master作为资源调度器</li></ul></li><li><p>根据Driver角色部署位置的不同，分为client和cluster两种模式</p></li><li><p><strong>Standalone-cluster模式</strong></p><ul><li><p>Driver由Spark集群分配，适用于生产环境</p></li><li><p>任务提交方式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class $&#123;APP_MAIN_CLASS&#125;  \</span><br><span class="line">--master spark://$&#123;MASSTER&#125;:7077 \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \  </span><br><span class="line">$&#123;APP_JAR&#125;          </span><br><span class="line">$&#123;INPUT&#125; </span><br><span class="line">$&#123;OUTPUT&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>Standalone-client</strong></p><ul><li><p>Driver在Client端，适用于开发环境中的交互和调试</p></li><li><p>任务提交方式（默认模式）</p></li></ul></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class $&#123;APP_MAIN_CLASS&#125;  \</span><br><span class="line">--master spark://$&#123;MASSTER&#125;:7077 \</span><br><span class="line">--deploy-mode client</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">$&#123;APP_JAR&#125;          </span><br><span class="line">$&#123;INPUT&#125; </span><br><span class="line">$&#123;OUTPUT&#125;</span><br></pre></td></tr></table></figure><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/articles/spark_20210409120722_c6.webp" style="zoom:150%;" /><center>Standalone-client运行模式</center><h2 id="6-Spark-on-YARN运行模式"><a href="#6-Spark-on-YARN运行模式" class="headerlink" title="6 Spark on YARN运行模式"></a>6 Spark on YARN运行模式</h2><ul><li><p>采用Hadoop的YARN作为资源调度器，无需额外构建Spark集群</p></li><li><p>根据drvier的部署位置不同，分为client和cluster两种模式</p></li><li><p><strong>YARN-cluster</strong></p></li><li><p>Driver由YARN在集群中找一个节点自动分配，适用于生产环境</p></li><li><p>任务提交方式</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">spark-submit </span><br><span class="line">--queue dc\  </span><br><span class="line">--class $&#123;APP_MAIN_CLASS&#125;  \       </span><br><span class="line">--master yarn\</span><br><span class="line">--deploy-mode client\         </span><br><span class="line">--executor-memory 30g\         </span><br><span class="line">--driver-memory 10g\         </span><br><span class="line">--num-executors 200\</span><br><span class="line">--executor-cores 16\         </span><br><span class="line">$&#123;APP_JAR&#125;          </span><br><span class="line">$&#123;INPUT&#125; </span><br><span class="line">$&#123;OUTPUT&#125;</span><br></pre></td></tr></table></figure><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/articles/spark_20210409121545_1a.webp"></p><ul><li><strong>YARN-client</strong><ul><li><p>Driver运行在Client端，适用于开发环境中的交互和调试</p></li><li><p>任务提交方式</p></li></ul></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class $&#123;APP_MAIN_CLASS&#125;  \</span><br><span class="line">--master yarn\</span><br><span class="line">--deploy-mode cluster\</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">$&#123;APP_JAR&#125;          </span><br><span class="line">$&#123;INPUT&#125; </span><br><span class="line">$&#123;OUTPUT&#125;</span><br></pre></td></tr></table></figure><h2 id="7-Spark-on-Mesos运行模式"><a href="#7-Spark-on-Mesos运行模式" class="headerlink" title="7 Spark on Mesos运行模式"></a>7 Spark on Mesos运行模式</h2><p>采用Mesos作为资源调度器，Spark客户端直接连接Mesos，无需构建Spark集群，国内运用较少</p><h2 id="8-IDEA中的开发调试设置"><a href="#8-IDEA中的开发调试设置" class="headerlink" title="8 IDEA中的开发调试设置"></a>8 IDEA中的开发调试设置</h2><h3 id="（1）本地调试"><a href="#（1）本地调试" class="headerlink" title="（1）本地调试"></a>（1）本地调试</h3><ul><li>运行模式设置为loca[*]即可启动本地多线程并发调试</li></ul><h3 id="（2）远程调试"><a href="#（2）远程调试" class="headerlink" title="（2）远程调试"></a>（2）远程调试</h3><ul><li><p>第1步：提交任务时，启动Driver的JVM调试</p><ul><li><p>提交任务时设置driver远程调试参数</p><ul><li>spark-submit –driver-java-options “-Xdebug -Xrunjdwp:transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;y,address&#x3D;5005”</li></ul></li></ul></li><li><p>第2步：IDE增加远程调试配置</p><ul><li><p>以IDEA为例，Run&#x2F;Edit Configuration…</p><ul><li>添加Remote JVM Debug调试配置，设置好driver的地址和端口，参数应与上述drvier端的JVM参数一致</li></ul></li></ul></li><li><p>第3步：运行调试配置</p><ul><li>以IDEA为例，debug上述RemoteJVM配置，IDEA的debugger会自动连接driver的5005端口，并返回调试数据</li></ul></li></ul><p>【注意】</p><ul><li><p>必须先提交任务，才能启动第二、三步中的本地debugger远程连接和调试</p></li><li><p>当deploy-mode为cluster时，driver的IP地址是动态分配的，IDE的Remote JVM Debug的远程主机地址应当根据集群中分配的driver地址做相应修改，两种方式避免频繁地修改JVM Debug配置</p><ul><li>方法1：deploy-mode设置为client，则driver在本地，IP地址设置为localhost即可</li><li>方法2：spark-submit提交任务时，deploy-mode设置为cluster，并增加参数 conf “spark.drvier.host” “某个worker的地址”来禁止自动调度drvier所在的主机，而采用指定的地址</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 软件安装与使用 </category>
          
          <category> ApacheSpark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安装与使用 - Apache Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title> Power Shell的使用与配置 </title>
      <link href="/vll-pages/posts/7d9a7cbc.html"/>
      <url>/vll-pages/posts/7d9a7cbc.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h1 id="Power-Shell的使用与配置"><a href="#Power-Shell的使用与配置" class="headerlink" title="Power Shell的使用与配置"></a>Power Shell的使用与配置</h1><p>​Windows下终于有可用的shell了。Power Shell自5.0版本之后，可用性越来越高，对于经常使用windowns的人来说，是一大福音。</p><h2 id="1-Power-Shell的安装"><a href="#1-Power-Shell的安装" class="headerlink" title="1. Power Shell的安装"></a>1. Power Shell的安装</h2><p>见power shell的github主页，可以下载安装各种稳定版和预览版</p><p><a href="https://github.com/PowerShell/PowerShell">https://github.com/PowerShell/PowerShell</a></p><h2 id="2-Windows-Terminal的安装"><a href="#2-Windows-Terminal的安装" class="headerlink" title="2. Windows Terminal的安装"></a>2. Windows Terminal的安装</h2><p>Windows Terminal好用了很多，支持多种配置文件和界面配置，一经发布得到很多人的喜欢，尤其是配合wsl、power shell等效果非常好。可以通过微软商店来安装，也可以到其github主页下载安装</p><p><a href="https://github.com/microsoft/terminal/">https://github.com/microsoft/terminal/</a></p><h2 id="3-Power-Shell-的常用配置"><a href="#3-Power-Shell-的常用配置" class="headerlink" title="3. Power Shell 的常用配置"></a>3. Power Shell 的常用配置</h2><h3 id="（1）传输安全控制协议"><a href="#（1）传输安全控制协议" class="headerlink" title="（1）传输安全控制协议"></a>（1）传输安全控制协议</h3><ul><li><p>问题：</p><p>新版PowerShell改变了传输安全策略，导致在power shell中访问网络资源总是报SSL无法连接的错误</p></li><li><p>解决办法：</p><p>[Net.ServicePointManager]::SecurityProtocol &#x3D; [Net.SecurityProtocolType]::Tls12</p></li></ul><h3 id="（2）代理设置"><a href="#（2）代理设置" class="headerlink" title="（2）代理设置"></a>（2）代理设置</h3><ul><li>问题：<ul><li>很多同学希望在Power Shell中直接设置代理服务器，以访问墙外资源，但却不知道如何设置</li></ul></li><li>解决办法：<ul><li>HTTP代理：$env:HTTP_PROXY&#x3D;”http:&#x2F;&#x2F;代理服务器IP:端口号”</li><li>HTTP代理：$env:HTTPS_PROXY&#x3D;”https:&#x2F;&#x2F;代理服务器IP:端口号”</li><li>HTTP代理：$env:ALL_PROXY&#x3D;”socks5:&#x2F;&#x2F;代理服务器IP:端口号”</li><li>删除代理：上述环境变量等号右侧设置为空字符串””</li></ul></li></ul><h3 id="3-安装命令行安装工具-scoop"><a href="#3-安装命令行安装工具-scoop" class="headerlink" title="(3)安装命令行安装工具 scoop"></a>(3)安装命令行安装工具 scoop</h3><ul><li><p>问题：</p><ul><li>熟悉linux的同学对apt等命令行软件安装工具应该不陌生，windows下近年也出现了类似的工具，最出名的是scoop和choco，本人习惯于scoop。</li></ul></li><li><p>解决办法：</p><ul><li><p>scoop的安装：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">（1）设置安全策略</span><br><span class="line">Set-ExecutionPolicy RemoteSigned -scope CurrentUser</span><br><span class="line">（2）下载安装脚本并安装</span><br><span class="line">Invoke-Expression (New-Object System.Net.WebClient).DownloadString(&#x27;https://get.scoop.sh&#x27;)</span><br><span class="line">或</span><br><span class="line">iwr -useb get.scoop.sh | iex</span><br></pre></td></tr></table></figure></li><li><p>scoop的使用</p><ul><li>增加仓库：scoop bucket add 仓库名（可用scoop bucket known查看知名的仓库）</li><li>查询软件：scoop search 软件名</li><li>安装软件：scoop install 软件名</li><li>卸载软件：scoop uninstall 软件名</li></ul></li><li><p>注意：scoop及其安装的软件默认在”~&#x2F;scoop”目录下。</p></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 软件安装与使用 </category>
          
          <category> PowerShell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安装与使用 </tag>
            
            <tag> powershell </tag>
            
            <tag> scoop </tag>
            
            <tag> proxy 代理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title> Git命令大全 </title>
      <link href="/vll-pages/posts/b355ef0c.html"/>
      <url>/vll-pages/posts/b355ef0c.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/articles/git_20210331220155_b8.webp"></p>]]></content>
      
      
      <categories>
          
          <category> 软件安装与使用 </category>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安装与使用 </tag>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知识仓库</title>
      <link href="/vll-pages/posts/c6315948.html"/>
      <url>/vll-pages/posts/c6315948.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title> Git简明教程 </title>
      <link href="/vll-pages/posts/459f47c7.html"/>
      <url>/vll-pages/posts/459f47c7.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><h1 id="Git简明教程"><a href="#Git简明教程" class="headerlink" title="Git简明教程"></a>Git简明教程</h1><h2 id="1-个人基本故事线"><a href="#1-个人基本故事线" class="headerlink" title="1. 个人基本故事线"></a>1. 个人基本故事线</h2><p>下图是从个人开发者角度所能观察到的场景：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/git/image-20210317005948153.png" alt="image-20210317010024234"></p><h3 id="1-1-如何从远程仓库获取代码"><a href="#1-1-如何从远程仓库获取代码" class="headerlink" title="1.1 如何从远程仓库获取代码"></a>1.1 如何从远程仓库获取代码</h3><ul><li><p><code>git clone</code></p></li><li><p>第一次时用克隆</p></li><li><p><code>git fetch</code></p></li><li><p>第二次开始</p><ul><li>将远程仓库代码拉取到<strong>本地仓库</strong></li><li>无冲突时checkout到<strong>工作区</strong></li><li>有冲突时merge到<strong>工作区</strong></li></ul></li><li><p><code>git pull</code></p></li><li><p>第二次开始</p><ul><li>掌握pull和fetch的区别<ul><li>pull  &#x3D; fetch+merge</li></ul></li></ul></li></ul><h3 id="1-2-提交代码到远程仓库"><a href="#1-2-提交代码到远程仓库" class="headerlink" title="1.2 提交代码到远程仓库"></a>1.2 提交代码到远程仓库</h3><ul><li><p><code>git add .  </code></p></li><li><p>从<strong>工作区</strong>添加到<strong>暂存区</strong></p></li><li><p><code>git commit</code></p></li><li><p>从<strong>暂存区</strong>更新到<strong>本地仓库</strong></p></li><li><p><code>git commit -a</code> &#x3D; <code>git add .</code> + <code>git commit</code></p></li><li><p>git push</p><ul><li>从<strong>本地仓库</strong>更新到<strong>远程仓库</strong></li></ul></li></ul><h3 id="1-3-手工创建本地仓库并与远程仓库同步"><a href="#1-3-手工创建本地仓库并与远程仓库同步" class="headerlink" title="1.3 手工创建本地仓库并与远程仓库同步"></a>1.3 手工创建本地仓库并与远程仓库同步</h3><ul><li><code>git init</code><ul><li>初始化一个本地仓库</li></ul></li><li><code>git remote add origin</code> &lt;远程仓库地址&gt;<ul><li>将本地仓库关联到远程仓库</li><li>若有已经关联的远程仓库，使用git remote rm origin删除老的远程仓库</li></ul></li><li>拉取远程仓库，与本地merge<ul><li><code>git pull</code> 拉取分支</li></ul></li><li><code>git add .</code> &amp;&amp; <code>git commit</code><ul><li>本地修改后，提交到本地仓库</li></ul></li><li><code>git push</code>，在远端分支上创建新版本<ul><li>将本地仓库推送到远程仓库</li></ul></li></ul><h3 id="1-4-文件级别的操作"><a href="#1-4-文件级别的操作" class="headerlink" title="1.4 文件级别的操作"></a>1.4 文件级别的操作</h3><p>​上述操作大多是基于分支和提交版本的，又是需要对部分文件进行提交和检出：</p><ul><li><code>git add *files*</code> 把当前文件放入暂存区域。</li><li><code>git commit</code> 给暂存区域生成快照并提交到本地仓库</li><li><code>git reset -- *files*</code> 用来撤销最后一次<code>git add *files*</code>，你也可以用<code>git reset</code> 撤销所有暂存区域文件。</li><li><code>git checkout -- *files*</code> 把文件从暂存区域复制到工作目录，用来丢弃本地修改。</li></ul><p>[注：此处为文件的checkout操作，分支操作也用checkout但含义稍有区别]</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_32bf1.webp"></p><p>你也可以跳过暂存区域直接从仓库取出文件或者直接提交代码：</p><ul><li><code>git commit -a </code>相当于运行 <code>git add</code> 把所有当前目录下的文件加入暂存区域再运行<code>git commit</code></li><li><code>git commit *files*</code> 进行一次包含最后一次提交加上工作目录中文件快照的提交。并且文件被添加到暂存区域。</li><li><code>git checkout HEAD -- *files*</code> 回滚到复制最后一次提交。</li></ul><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_3b409.webp"></p><h2 id="2-理解远程仓库"><a href="#2-理解远程仓库" class="headerlink" title="2. 理解远程仓库"></a>2. 理解远程仓库</h2><h3 id="2-1-创建远程仓库（通过页面操作实现）"><a href="#2-1-创建远程仓库（通过页面操作实现）" class="headerlink" title="2.1 创建远程仓库（通过页面操作实现）"></a>2.1 创建远程仓库（通过页面操作实现）</h3><ul><li><p>创建操作create</p></li><li><p>复制操作fork</p><ul><li>fork的远程仓库如果与源仓库需要合并，则需要Pull Request操作</li></ul></li><li><p>导入操作import</p></li></ul><h3 id="2-2-克隆远程仓库（通过本地git命令操作）"><a href="#2-2-克隆远程仓库（通过本地git命令操作）" class="headerlink" title="2.2 克隆远程仓库（通过本地git命令操作）"></a>2.2 克隆远程仓库（通过本地git命令操作）</h3><ul><li><p>克隆命令git clone仓库地址</p><ul><li>要在没有本地仓库的地方执行</li><li>从本人可见的远程仓库复制到本地电脑</li><li>可以做本地修改，但仅有开发权限的项目成员才能向远程仓库推送</li></ul></li></ul><h3 id="2-3-拉取远程仓库代码（通过本地git命令操作）"><a href="#2-3-拉取远程仓库代码（通过本地git命令操作）" class="headerlink" title="2.3 拉取远程仓库代码（通过本地git命令操作）"></a>2.3 拉取远程仓库代码（通过本地git命令操作）</h3><ul><li><p>拉取命令git pull [分支名]</p><ul><li><p>要在本地仓库内执行</p><ul><li>拉取的内容会被合并到<strong>本地工作区</strong>（<strong>本地仓库</strong>同步合并）</li></ul></li><li><p>只有有开发权限的项目成员才能够拉取，并在修改后推送</p></li><li><p>git pull [分支名] &#x3D; git fetch [分支名]+ git merge</p></li></ul></li><li><p>拉取命令git fetch [分支名]</p><ul><li><p>要在本地仓库内执行</p><ul><li>拉去的内容会被合并到<strong>本地仓库</strong></li></ul></li><li><p>需要配套git checkout [分支名]</p><ul><li>从<strong>本地仓库</strong>拉取并覆盖到<strong>本地工作区</strong>后使用</li></ul></li><li><p>或者配套git mearge</p><ul><li>从<strong>本地仓库</strong>拉取并合并到<strong>本地工作区</strong>后使用</li></ul></li></ul></li></ul><h3 id="2-4-推送代码到远程仓库（通过本地git命令操作）"><a href="#2-4-推送代码到远程仓库（通过本地git命令操作）" class="headerlink" title="2.4 推送代码到远程仓库（通过本地git命令操作）"></a>2.4 推送代码到远程仓库（通过本地git命令操作）</h3><ul><li><p>推送命令git push [分支名]</p><ul><li>当<strong>远程仓库</strong>新于<strong>本地仓库</strong>时，需要先pull，使<strong>本地仓库</strong>与<strong>远程仓库</strong>同步，并merge解决所有冲突后，才能够push</li><li>要先配置好用户名和邮箱，以便远程仓库记录操作用户</li></ul></li></ul><h2 id="3-团队故事线"><a href="#3-团队故事线" class="headerlink" title="3 团队故事线"></a>3 团队故事线</h2><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/git/image-20210317010024234.png" alt="image-20210317010024234"></p><h3 id="3-1-角色区分"><a href="#3-1-角色区分" class="headerlink" title="3.1 角色区分"></a>3.1 角色区分</h3><ul><li><p>Team leader</p><ul><li><p>负责主远程仓库的创建</p></li><li><p>新版本开发过程中（可通过页面操作实现）</p><ul><li>收到程序员A和B的pull Request</li><li>检查程序员A和B的pull Request</li><li>当pull request出现冲突时，解决冲突</li><li>接受pull Request，合并至主仓库</li></ul></li><li><p>新版本开发完成（可通过页面操作实现）</p><ul><li>创建本地release分支</li><li>合并本地主分支到release分支</li><li>推送本地分支到远程仓库</li><li>版本的标签管理和推送</li></ul></li></ul></li><li><p>程序员A</p><ul><li><p>新任务生命周期</p><ul><li>fork创建个人远程仓库（可通过页面操作完成）</li><li>针对某个任务完成个人故事线</li><li>pull request远程操作（可通过页面操作完成）</li></ul></li></ul></li><li><p>程序员B</p><ul><li><p>新任务生命周期</p><ul><li>fork创建个人远程仓库（可通过页面操作完成）</li><li>针对某个任务完成个人故事线</li><li>pull request远程操作（可通过页面操作完成）</li></ul></li></ul></li></ul><h3 id="3-2-四种可能的协作方式"><a href="#3-2-四种可能的协作方式" class="headerlink" title="3.2 四种可能的协作方式"></a>3.2 四种可能的协作方式</h3><ul><li><p>集中式协作工作流</p><ul><li>**大家共用一个远程仓库 **</li><li>大家都工作在master分支上</li><li>不同角色通过pull和push操作远程仓库，他们各自在本地维护和控制冲突，并推送到远程仓库</li></ul></li><li><p>功能分支工作流</p><ul><li><strong>大家共用一个远程仓库</strong></li><li>只有团队leader可以操控master分支，开发者们工作在各自的功能分支上</li><li>允许开发者创建各自的功能分支，并推送到远程仓库</li><li>开发者完成开发后，向团队leader交Pull Request，由leader进行代码审查并合入master分支</li></ul><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/landscape/goodpic_55ac6.webp"></p></li><li><p>GitFlow工作流程</p><ul><li><strong>大家共用一个远程仓库</strong></li><li>设置master分支、develop分支、release分支和hotfix分支<ul><li><code>master</code>分支：<ul><li>只用来储存官方发布历史，通常我们会在<code>master</code>分支的提交中打上版本标签号</li><li>master分支由team leader管控</li></ul></li><li><code>develop</code>分支：<ul><li>开发者打交道最多的分支，源自<code>master</code>分支，用于整合各功能分支，类似于前面两种方式的<code>master</code>分支</li><li>develop分支由team leader管控</li></ul></li><li><code>feature</code>分支：<ul><li>每个新功能都放在自己的分支中，并以develop分支作为父分支，因此可以由很多功能分支</li><li>当一个功能完成后，开发这向团队leader提交Pull Request，由团队leader完成合并后推送到develop分支</li><li>功能分支永远不应该直接与master分支交互，而应先合会到develop分支，再由develop分支合并回master分支</li><li>命名规范遵循<code>feature-*</code>或者<code>feature/*</code></li><li>feature分支可由开发人员创建和维护</li></ul></li><li><code>release</code>分支： <ul><li>release分支是develop分支和master分支之间的一个过渡阶段，主要用于该发布版本的功能完善或bug修复</li><li>release分支基于develop分支创建，只有和发布相关的任务才在这个分支上进行，如修复bug，生成文档等等</li><li>release分支的所有功能均完善后，将并入master分支和develop分支（此时develop分支也许正在并行开发）</li><li>并入master分支和develop分支后，<strong><u>该release分支应当被删除</u></strong></li><li>release分支可以确保一个团队完善当前发布，而不影响其他团队继续在develop上开发下一个待发布的功能或在master上修复产品bug</li><li>命名规范遵循<code>release-*</code>或者<code>release/*</code></li><li>release分支由teame leader负责管控，也可由release版维护人员采用集中式工作流程方式自行解决冲突</li></ul></li><li><code>hotfix</code>分支：<ul><li>与release分支不同，hotfix分支基于master分支创建，主要用于master版本做快速的bug修复</li><li>hotfix分支完成后，应当合并入master分支和develop分支，会影响到下一个release分支，但不会影响当前release分支，以免带来更多bug</li><li>并入master分支和develop分支后，该hotfix分支应当被删除，而master分支应当打上更新的标签</li><li>hotfix分支可以被看作 <code>master</code> 分支的临时发布分支，它使得团队能够及时处理issues，而不打断其他工作流。</li></ul></li></ul></li></ul></li><li><p>Forking工作流程</p><ul><li><strong>每个角色有自己的远程仓库</strong>  <strong>—- 推荐得多人协作方式</strong></li><li>所有角色都有自己的远程仓库，其中team leader拥有主仓库的权限</li><li>不同角色通过fork和pull request保持与主仓库的协作，由team leader协调不同pull request的冲突</li></ul></li></ul><h2 id="4-冲突的控制"><a href="#4-冲突的控制" class="headerlink" title="4 冲突的控制"></a>4 冲突的控制</h2><h3 id="4-1-fork与pull-request中产生的冲突"><a href="#4-1-fork与pull-request中产生的冲突" class="headerlink" title="4.1 fork与pull request中产生的冲突"></a>4.1 fork与pull request中产生的冲突</h3><ul><li><p>冲突的产生：冲突在主仓库产生，由team leader处理冲突</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/git/image-20210317010100843.png"></p></li><li><p>由team leader将主仓库和PR仓库都拉取到本地，在本地通过merge tool合并有冲突的代码后，推送到主仓库，PR仓库可以通过与主仓库同步自动更新版本</p></li></ul><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/git/image-20210317010222818.png" alt="image-20210317010222818"></p><h3 id="4-2-pull和push过程中产生的冲突"><a href="#4-2-pull和push过程中产生的冲突" class="headerlink" title="4.2 pull和push过程中产生的冲突"></a>4.2 pull和push过程中产生的冲突</h3><ul><li><p>冲突的产生：冲突在角色本地产生，每个角色都可以在本地处理冲突，但会不断受到其他角色新push的影响</p><p>  <img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/git/image-20210317010326020.png" alt="image-20210317010326020"></p></li><li><p>冲突的解决</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/git/image-20210317010335025.png" alt="image-20210317010335025"></p></li></ul><h3 id="4-3-总结"><a href="#4-3-总结" class="headerlink" title="4.3 总结"></a>4.3 总结</h3><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/git/image-20210317010355629.png" alt="image-20210317010355629"></p><h2 id="5-分支的管理"><a href="#5-分支的管理" class="headerlink" title="5 分支的管理"></a>5 分支的管理</h2><h3 id="5-1-分支管理的目的"><a href="#5-1-分支管理的目的" class="headerlink" title="5.1 分支管理的目的"></a>5.1 分支管理的目的</h3><ul><li>在不中断主分支的情况下，同时开展其他工作</li><li>通常在完成该工作后，将成果合并到主分支</li></ul><h3 id="5-2-远程仓库分支管理"><a href="#5-2-远程仓库分支管理" class="headerlink" title="5.2 远程仓库分支管理"></a>5.2 远程仓库分支管理</h3><ul><li><p>创建远程分支（页面操作）</p></li><li><p>本地命令创建远程分支</p><ul><li>先创建本地分支：git branch [新分支名]</li><li>推送到远程仓库：git push -u origin [新分支名]</li></ul></li><li><p>删除远程分支（页面操作）</p></li><li><p>使用远程分支</p><ul><li>git pull&#x2F;fetch&#x2F;push [指定分支名称]</li></ul></li></ul><h3 id="5-3-本地仓库分支管理"><a href="#5-3-本地仓库分支管理" class="headerlink" title="5.3 本地仓库分支管理"></a>5.3 本地仓库分支管理</h3><ul><li><p>本地分支管理</p><ul><li><p>创建当前版本的本地分支</p></li><li><p>git branch [新分支名]</p></li><li><p>删除本地分支B的命令</p><ul><li>git branch -d分支B</li></ul></li></ul></li><li><p>切换本地分支 ，或者检出内容到工作区</p><ul><li>git checkout [已有分支名]</li><li>或者git checkout -b  [新分支名称]<ul><li>创建并检出一个新分支</li></ul></li></ul></li></ul><p><img src="C:%5CUsers%5C89877%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210328152736904.png" alt="image-20210328152736904"></p><ul><li><p>合并分支得大致流程（将B分支合并到A分支的过程）</p><ul><li>检出分支B：   git checkout 分支B</li><li>修改并提交分支B： 完成分支B工作区修改，并提交</li><li>检出分支A：  git checkout 分支A</li><li>合并分支B到当前分支A：   git merge B</li></ul></li><li><p>拉取远程仓库分支</p><ul><li>git pull &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt;</li></ul></li><li><p>推送远程仓库分支 </p><ul><li>git push &lt;远程主机名&gt; &lt;本地分支名&gt;:&lt;远程分支名&gt;</li></ul></li></ul><h2 id="6-标签管理"><a href="#6-标签管理" class="headerlink" title="6 标签管理"></a>6 标签管理</h2><h3 id="6-1-标签管理的目的"><a href="#6-1-标签管理的目的" class="headerlink" title="6.1 标签管理的目的"></a>6.1 标签管理的目的</h3><p>​用于版本管理</p><h3 id="6-2-常用命令"><a href="#6-2-常用命令" class="headerlink" title="6.2 常用命令"></a>6.2 常用命令</h3><ul><li><p>在本地创建标签</p><ul><li>git tag -a 标签名</li></ul></li><li><p>删除本地标签</p><ul><li>git tag -d 标签名</li></ul></li><li><p>列出本地标签</p><ul><li>git tag</li></ul></li><li><p>切换本地标签</p><ul><li>git checkout 标签名</li></ul></li><li><p>推送远程标签</p><ul><li>git push</li></ul></li><li><p>拉取远程标签</p><ul><li>git pull</li></ul></li></ul><h2 id="7-查看历史记录"><a href="#7-查看历史记录" class="headerlink" title="7 查看历史记录"></a>7 查看历史记录</h2><h3 id="7-1-命令行"><a href="#7-1-命令行" class="headerlink" title="7.1 命令行"></a>7.1 命令行</h3><ul><li><p>git log</p><ul><li>查看所有commit行为</li><li>输入q退出git log</li></ul></li></ul><h3 id="7-2-web页面"><a href="#7-2-web页面" class="headerlink" title="7.2 web页面"></a>7.2 web页面</h3><ul><li>选择仓库的“统计”按钮查看</li></ul><h3 id="7-3-IDE工具"><a href="#7-3-IDE工具" class="headerlink" title="7.3 IDE工具"></a>7.3 IDE工具</h3><ul><li>不同IDE通常会提供可视化的git历史记录</li></ul><h2 id="8-常见场景速查手册"><a href="#8-常见场景速查手册" class="headerlink" title="8 常见场景速查手册"></a>8 常见场景速查手册</h2><p>以下为一些常见的小场景及其对策：</p><h3 id="场景1-本地已经存在的项目-分支与如何远程仓库关联"><a href="#场景1-本地已经存在的项目-分支与如何远程仓库关联" class="headerlink" title="场景1. 本地已经存在的项目&#x2F;分支与如何远程仓库关联"></a>场景1. 本地已经存在的项目&#x2F;分支与如何远程仓库关联</h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin &lt;your-repo-git-url&gt;</span><br></pre></td></tr></table></figure><h3 id="场景2-刚刚提交了的commit-log发现错了，想修改"><a href="#场景2-刚刚提交了的commit-log发现错了，想修改" class="headerlink" title="场景2. 刚刚提交了的commit log发现错了，想修改"></a>场景2. 刚刚提交了的commit log发现错了，想修改</h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit --amend -m &quot;your new log&quot;</span><br></pre></td></tr></table></figure><h3 id="场景3-查看某次提交的日志和ID"><a href="#场景3-查看某次提交的日志和ID" class="headerlink" title="场景3. 查看某次提交的日志和ID"></a>场景3. 查看某次提交的日志和ID</h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reflog</span><br></pre></td></tr></table></figure><h3 id="场景4-查看某次提交的内容"><a href="#场景4-查看某次提交的内容" class="headerlink" title="场景4. 查看某次提交的内容"></a>场景4. 查看某次提交的内容</h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git show &lt;commit_id&gt;</span><br></pre></td></tr></table></figure><h3 id="场景5-只是修改了工作区的文件，想恢复到原来修改前的样子"><a href="#场景5-只是修改了工作区的文件，想恢复到原来修改前的样子" class="headerlink" title="场景5. 只是修改了工作区的文件，想恢复到原来修改前的样子"></a>场景5. 只是修改了工作区的文件，想恢复到原来修改前的样子</h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard HEAD</span><br><span class="line">git checkout -- &lt;file_name&gt;</span><br></pre></td></tr></table></figure><h3 id="场景6-被修改的文件已经添加到了暂存区，想撤销添加"><a href="#场景6-被修改的文件已经添加到了暂存区，想撤销添加" class="headerlink" title="场景6. 被修改的文件已经添加到了暂存区，想撤销添加"></a>场景6. 被修改的文件已经添加到了暂存区，想撤销添加</h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset --mixed HEAD</span><br></pre></td></tr></table></figure><h3 id="场景7-被修改的文件已经commit提交，想撤销提交"><a href="#场景7-被修改的文件已经commit提交，想撤销提交" class="headerlink" title="场景7. 被修改的文件已经commit提交，想撤销提交"></a>场景7. 被修改的文件已经commit提交，想撤销提交</h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset --soft HEAD^</span><br></pre></td></tr></table></figure><h3 id="场景8-已经提交到远程主机的文件，想撤销"><a href="#场景8-已经提交到远程主机的文件，想撤销" class="headerlink" title="场景8. 已经提交到远程主机的文件，想撤销"></a>场景8. 已经提交到远程主机的文件，想撤销</h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git revert &lt;commit_id&gt;</span><br><span class="line">git revert HEAD</span><br></pre></td></tr></table></figure><h3 id="场景9-已经开发一半的功能，但是没有开发完，这时候有个bug要紧急处理，需要放下手头的功能，赶去修改BUG"><a href="#场景9-已经开发一半的功能，但是没有开发完，这时候有个bug要紧急处理，需要放下手头的功能，赶去修改BUG" class="headerlink" title="场景9. 已经开发一半的功能，但是没有开发完，这时候有个bug要紧急处理，需要放下手头的功能，赶去修改BUG"></a>场景9. 已经开发一半的功能，但是没有开发完，这时候有个bug要紧急处理，需要放下手头的功能，赶去修改BUG</h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">// 保存现场</span><br><span class="line">git add .</span><br><span class="line">git stash  </span><br><span class="line">// 恢复现场</span><br><span class="line">git stash pop [stash_num] </span><br><span class="line">或者恢复但不删除工作现场</span><br><span class="line">git stash apply [stash_num]</span><br></pre></td></tr></table></figure><h3 id="场景10-加入过历史版本的文件，因某些原因被删除了想恢复"><a href="#场景10-加入过历史版本的文件，因某些原因被删除了想恢复" class="headerlink" title="场景10. 加入过历史版本的文件，因某些原因被删除了想恢复"></a>场景10. 加入过历史版本的文件，因某些原因被删除了想恢复</h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout &lt;commit_id&gt; -- &lt;file_name&gt;</span><br></pre></td></tr></table></figure><p>另外你也可以用reset命令来完成</p><h3 id="场景11-需要单独把多次提交中的某一次提交从你的分支迁移到另外一个分支上，即跨分支应用commit"><a href="#场景11-需要单独把多次提交中的某一次提交从你的分支迁移到另外一个分支上，即跨分支应用commit" class="headerlink" title="场景11. 需要单独把多次提交中的某一次提交从你的分支迁移到另外一个分支上，即跨分支应用commit"></a>场景11. 需要单独把多次提交中的某一次提交从你的分支迁移到另外一个分支上，即跨分支应用commit</h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git cherry-pick &lt;commit_id&gt;</span><br></pre></td></tr></table></figure><p>比如：我想把以下分支</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A-B  master</span><br><span class="line">   \</span><br><span class="line">    C-D-E-F-G develop</span><br></pre></td></tr></table></figure><p>中的D，F两次提交移动到master分支，而保持其他commit不变，结果就像这样</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A-B-D-F  master</span><br><span class="line">       \</span><br><span class="line">        C-E-G develop</span><br></pre></td></tr></table></figure><p>那么，思路是将D，F用cherry-pick应用到master分支上，然后将develop分支对master分支变基。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout master  </span><br><span class="line">$ git cherry-pick D  </span><br><span class="line">$ git cherry-pick F  </span><br><span class="line">$ git checkout develop  </span><br><span class="line">$ git rebase master</span><br></pre></td></tr></table></figure><p>注意有些情况下使用cherry-pick会存在冲突，解决方法和我们平时合并分支遇到冲突一样。</p><h3 id="场景12-遇到文件冲突，可以手动解决，或者用你配置的工具解决，记得把文件标位resolved：add-rm"><a href="#场景12-遇到文件冲突，可以手动解决，或者用你配置的工具解决，记得把文件标位resolved：add-rm" class="headerlink" title="场景12. 遇到文件冲突，可以手动解决，或者用你配置的工具解决，记得把文件标位resolved：add&#x2F;rm"></a>场景12. 遇到文件冲突，可以手动解决，或者用你配置的工具解决，记得把文件标位resolved：add&#x2F;rm</h3><p>如：常见的拉取同事的代码合并引起冲突</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. 手动处理冲突</span><br><span class="line">2. 文件标志位置为resolved：git add &lt;file_name&gt;</span><br><span class="line">3. 继续合并  git merge --continue</span><br><span class="line">当然也可以选择放弃合并：git merge --abort</span><br></pre></td></tr></table></figure><h3 id="场景13-让自己本地分支上面的每一次提交日志变得更有意义，有时候需要我们选择有意义的提交日志信息合并上去"><a href="#场景13-让自己本地分支上面的每一次提交日志变得更有意义，有时候需要我们选择有意义的提交日志信息合并上去" class="headerlink" title="场景13. 让自己本地分支上面的每一次提交日志变得更有意义，有时候需要我们选择有意义的提交日志信息合并上去"></a>场景13. 让自己本地分支上面的每一次提交日志变得更有意义，有时候需要我们选择有意义的提交日志信息合并上去</h3><p>比如我们在bugfix分支上面由于修改bug提交了很多次，修复好了之后，我们想把这些提交合并入我们的master分支</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git checkout master</span><br><span class="line">git merge --squash bugfix</span><br><span class="line">git commit -m &quot;bug fixed&quot;</span><br></pre></td></tr></table></figure><p>上面操作会将bugfix分支上的所有commit都合并为一个commit，并把它并入我们的master分支上去。这里还有一点需要注意的是：–squash含义代表的是本地内容与不使用该选项的合并结果相同，但是不提交，不移动HEAD指针，所以我们要另外多一条语句来移动我们的HEAD指针，即最后的commit。</p><h3 id="场景14-有时候需要整理我们本地的commits，可以使用Squash"><a href="#场景14-有时候需要整理我们本地的commits，可以使用Squash" class="headerlink" title="场景14. 有时候需要整理我们本地的commits，可以使用Squash"></a>场景14. 有时候需要整理我们本地的commits，可以使用Squash</h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git rebase -i &lt;commit&gt;</span><br></pre></td></tr></table></figure><p>举例：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">git rebase -i HEAD~5</span><br><span class="line"></span><br><span class="line">执行完后，Git会把所有commit列出来，让你进行一些修改，修改完成之后会根据你的修改来rebase。HEAD-5的意思是只修改最近的5个commit。</span><br><span class="line"></span><br><span class="line">pick 033beb4 b1</span><br><span class="line">pick b426a8a b2</span><br><span class="line">pick c216era b3</span><br><span class="line">pick d627c9a b4</span><br><span class="line">pick e416c8b b5</span><br><span class="line"></span><br><span class="line"># Rebase 033beb4..e416c8b onto 033beb4</span><br><span class="line">#</span><br><span class="line"># Commands:</span><br><span class="line">#  p, pick = use commit</span><br><span class="line">#  r, reword = use commit, but edit the commit message</span><br><span class="line">#  e, edit = use commit, but stop for amending</span><br><span class="line">#  s, squash = use commit, but meld into previous commit</span><br><span class="line">#  f, fixup = like &quot;squash&quot;, but discard this commit&#x27;s log message</span><br><span class="line">#  x, exec = run command (the rest of the line) using shell</span><br><span class="line">#</span><br><span class="line"># If you remove a line here THAT COMMIT WILL BE LOST.</span><br><span class="line"># However, if you remove everything, the rebase will be aborted.</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p>上面pick是要执行的commit指令，另外还有reword、edit、squash、fixup、exec这5个，具体的含义可以看上面的注释解释，比较简单，这里就不说了。 我们要合并就需要修改前面的pick指令：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pick 033beb4 b1</span><br><span class="line">squash b426a8a b2</span><br><span class="line">squash c216era b3</span><br><span class="line">squash d627c9a b4</span><br><span class="line">squash e416c8b b5</span><br></pre></td></tr></table></figure><p>也就是下面这4个提交合并到最前面的那个提交里面，按esc，打上:wq提交保存离开。 接着是输入新的commit message</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">b</span><br><span class="line"># This is a combination of 2 commits.</span><br><span class="line"># The first commit&#x27;s message is:</span><br><span class="line"># b1</span><br><span class="line">#</span><br><span class="line"># This is the 2nd commit message:</span><br><span class="line">#</span><br><span class="line"># b2</span><br><span class="line">#</span><br><span class="line"># This is the 3rd commit message:</span><br><span class="line">#</span><br><span class="line"># b3</span><br><span class="line">#</span><br><span class="line"># This is the 4th commit message:</span><br><span class="line">#</span><br><span class="line"># b4</span><br><span class="line">#</span><br><span class="line"># This is the 5th commit message:</span><br><span class="line">#</span><br><span class="line"># b5</span><br><span class="line">#</span><br><span class="line"># Please enter the commit message for your changes. Lines starting</span><br><span class="line"># with &#x27;#&#x27; will be ignored, and an empty message aborts the commit.</span><br><span class="line"># Not currently on any branch.</span><br><span class="line"># Changes to be committed:</span><br><span class="line"># (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)</span><br><span class="line">#</span><br><span class="line"># modified:   a.txt</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p>其中第一行的b就是需要我们输入的新信息，同样编辑完保存，出现类似下面的信息：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Successfully rebased and updated refs/heads/develop.</span><br></pre></td></tr></table></figure><p>最后可以用git log指令来验证commits是不是我们要变成的样子。</p><h3 id="场景15-多人协作开发项目，想知道某个文件的当前改动情况"><a href="#场景15-多人协作开发项目，想知道某个文件的当前改动情况" class="headerlink" title="场景15. 多人协作开发项目，想知道某个文件的当前改动情况"></a>场景15. 多人协作开发项目，想知道某个文件的当前改动情况</h3><p>通常查问题时想知道某个文件的某部分代码是谁改动的，那么git blame就派上用场了。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git blame &lt;file_name&gt;</span><br></pre></td></tr></table></figure><p>你也可以具体指定到某一行或者某几行代码</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git blame -L &lt;start_line&gt;,&lt;end_line&gt; &lt;file_name&gt;</span><br></pre></td></tr></table></figure><h3 id="场景16-执行push命令向多个仓库同时提交代码"><a href="#场景16-执行push命令向多个仓库同时提交代码" class="headerlink" title="场景16. 执行push命令向多个仓库同时提交代码"></a>场景16. 执行push命令向多个仓库同时提交代码</h3><p>有时候会做代码备份，将代码保存在几个不同的Git代码管理平台，这时候就需要用到了</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">修改本地仓库目录下.git/config文件</span><br><span class="line"></span><br><span class="line">[core]</span><br><span class="line">repositoryformatversion = 0</span><br><span class="line">filemode = true</span><br><span class="line">bare = false</span><br><span class="line">logallrefupdates = true</span><br><span class="line">ignorecase = true</span><br><span class="line">precomposeunicode = true</span><br><span class="line">[remote &quot;origin&quot;]</span><br><span class="line">url = git@github.com:yuxingxin/blog.git</span><br><span class="line">        url = ……</span><br><span class="line">        url = ……</span><br><span class="line">fetch = +refs/heads/*:refs/remotes/origin/*</span><br></pre></td></tr></table></figure><p>如上在remote处可以添加多个远程地址。</p><h3 id="场景17-从多次提交中快速定位某一次提交的bug"><a href="#场景17-从多次提交中快速定位某一次提交的bug" class="headerlink" title="场景17. 从多次提交中快速定位某一次提交的bug"></a>场景17. 从多次提交中快速定位某一次提交的bug</h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 开始 bisect</span><br><span class="line">$ git bisect start</span><br><span class="line"></span><br><span class="line"># 录入正确的 commit</span><br><span class="line">$ git bisect good xxxxxx</span><br><span class="line"></span><br><span class="line"># 录入出错的 commit</span><br><span class="line">$ git bisect bad xxxxxx</span><br><span class="line"></span><br><span class="line"># 然后 git 开始在出错的 commit 与正确的 commit 之间开始二分查找，这个过程中你需要不断的验证你的应用是否正常</span><br><span class="line">$ git bisect bad</span><br><span class="line">$ git bisect good</span><br><span class="line">$ git bisect good</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"># 直到定位到出错的 commit，退出 bisect</span><br><span class="line">$ git bisect reset</span><br></pre></td></tr></table></figure><h2 id="9-常用命令图解"><a href="#9-常用命令图解" class="headerlink" title="9 常用命令图解"></a>9 常用命令图解</h2><p>后文中以下面的形式使用图片。绿色的5位字符表示提交的ID，分别指向父节点。分支用橘色显示，分别指向特定的提交。当前分支由附在其上的<em>HEAD</em>标识。 这张图片里显示最后5次提交，<em>ed489</em>是最新提交。 <em>main</em>分支指向此次提交，另一个<em>stable</em>分支指向祖父提交节点。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_9c0df.webp"></p><h3 id="（1）Diff"><a href="#（1）Diff" class="headerlink" title="（1）Diff"></a>（1）Diff</h3><p>有许多种方法查看两次提交之间的变动。下面是一些示例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_e2cba.webp"></p><h3 id="（2）Commit"><a href="#（2）Commit" class="headerlink" title="（2）Commit"></a>（2）Commit</h3><p>提交时，git用暂存区域的文件创建一个新的提交，并把此时的节点设为父节点。然后把当前分支指向新的提交节点。下图中，当前分支是<em>main</em>。 在运行命令之前，<em>main</em>指向<em>ed489</em>，提交后，<em>main</em>指向新的节点<em>f0cec</em>并以<em>ed489</em>作为父节点。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_ad05f.webp"></p><p>即便当前分支是某次提交的祖父节点，git会同样操作。下图中，在<em>main</em>分支的祖父节点<em>stable</em>分支进行一次提交，生成了<em>1800b</em>。 这样，<em>stable</em>分支就不再是<em>main</em>分支的祖父节点。此时，<a href="http://marklodato.github.io/visual-git-guide/index-zh-cn.html#merge">合并</a> (或者 <a href="http://marklodato.github.io/visual-git-guide/index-zh-cn.html#rebase">衍合</a>) 是必须的。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_199c4.webp"></p><p>如果想更改一次提交，使用 <code>git commit --amend</code>。git会使用与当前提交相同的父节点进行一次新提交，旧的提交会被取消。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_17088.webp"></p><h3 id="（3）Checkout"><a href="#（3）Checkout" class="headerlink" title="（3）Checkout"></a>（3）Checkout</h3><p>​checkout命令用于从历史提交（或者暂存区域）中拷贝文件到工作目录，也可用于切换分支。</p><p>​当给定某个文件名（或者打开-p选项，或者文件名和-p选项同时打开）时，git会从指定的提交中拷贝文件到暂存区域和工作目录。比如，<code>git checkout HEAD~ foo.c</code>会将提交节点<em>HEAD~</em>(即当前提交节点的父节点)中的<code>foo.c</code>复制到工作目录并且加到暂存区域中。（如果命令中没有指定提交节点，则会从暂存区域中拷贝内容。）注意当前分支不会发生变化。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_31005.webp"></p><p>​当不指定文件名，而是给出一个（本地）分支时，那么<em>HEAD</em>标识会移动到那个分支（也就是说，我们“切换”到那个分支了），然后暂存区域和工作目录中的内容会和<em>HEAD</em>对应的提交节点一致。新提交节点（下图中的a47c3）中的所有文件都会被复制（到暂存区域和工作目录中）；只存在于老的提交节点（ed489）中的文件会被删除；不属于上述两者的文件会被忽略，不受影响。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_d0768.webp"></p><p>如果既没有指定文件名，也没有指定分支名，而是一个标签、远程分支、SHA-1值或者是像<em>main~3</em>类似的东西，就得到一个匿名分支，称作<em>detached HEAD</em>（被分离的<em>HEAD</em>标识）。这样可以很方便地在历史版本之间互相切换。比如说你想要编译1.6.6.1版本的git，你可以运行<code>git checkout v1.6.6.1</code>（这是一个标签，而非分支名），编译，安装，然后切换回另一个分支，比如说<code>git checkout main</code>。然而，当提交操作涉及到“分离的HEAD”时，其行为会略有不同，详情见在<a href="http://marklodato.github.io/visual-git-guide/index-zh-cn.html#detached">下面</a>。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_d681c.webp"></p><h3 id="（4）HEAD标识处于分离状态时的提交和检出操作"><a href="#（4）HEAD标识处于分离状态时的提交和检出操作" class="headerlink" title="（4）HEAD标识处于分离状态时的提交和检出操作"></a>（4）HEAD标识处于分离状态时的提交和检出操作</h3><p>当<em>HEAD</em>处于分离状态（不依附于任一分支）时，提交操作可以正常进行，但是不会更新任何已命名的分支。(你可以认为这是在更新一个匿名分支。)</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_c8abd.webp"></p><p>一旦此后你切换到别的分支，比如说<em>main</em>，那么这个提交节点（可能）再也不会被引用到，然后就会被丢弃掉了。注意这个命令之后就不会有东西引用<em>2eecb</em>。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_cbd1d.webp"></p><p>但是，如果你想保存这个状态，可以用命令<code>git checkout -b *name*</code>来创建一个新的分支。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_16db1.webp"></p><h3 id="（5）Reset"><a href="#（5）Reset" class="headerlink" title="（5）Reset"></a>（5）Reset</h3><p>reset命令把当前分支指向另一个位置，并且有选择的变动工作目录和索引。也用来在从历史仓库中复制文件到索引，而不动工作目录。</p><p>如果不给选项，那么当前分支指向到那个提交。如果用–hard选项，那么工作目录也更新，如果用–soft选项，那么都不变。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_5cd2c.webp"></p><p>如果没有给出提交点的版本号，那么默认用HEAD。这样，分支指向不变，但是索引会回滚到最后一次提交，如果用–hard选项，工作目录也同样。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_f9aa9.webp"></p><p>如果给了文件名(或者 -p选项), 那么工作效果和带文件名的checkout差不多，除了索引被更新。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_d44c9.webp"></p><h3 id="（6）Merge"><a href="#（6）Merge" class="headerlink" title="（6）Merge"></a>（6）Merge</h3><p>merge命令把不同分支合并起来。合并前，索引必须和当前提交相同。如果另一个分支是当前提交的祖父节点，那么合并命令将什么也不做。 另一种情况是如果当前提交是另一个分支的祖父节点，就导致fast-forward合并。指向只是简单的移动，并生成一个新的提交。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_4fc04.webp" alt="image-20210328174604793"></p><p>否则就是一次真正的合并。默认把当前提交(ed489如下所示)和另一个提交(33104)以及他们的共同祖父节点(b325c)进行一次三方合并。结果是先保存当前目录和索引，然后和父节点33104一起做一次新提交。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_54833.webp"></p><h3 id="（7）Cherry-Pick"><a href="#（7）Cherry-Pick" class="headerlink" title="（7）Cherry Pick"></a>（7）Cherry Pick</h3><p>cherry-pick命令”复制”一个提交节点并在当前分支做一次完全一样的新提交。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_34716.webp"></p><h3 id="（9）Rebase"><a href="#（9）Rebase" class="headerlink" title="（9）Rebase"></a>（9）Rebase</h3><p>衍合是合并命令的另一种选择。合并把两个父分支合并进行一次提交，提交历史不是线性的。衍合在当前分支上重演另一个分支的历史，提交历史是线性的。 本质上，这是线性化的自动的 <a href="http://marklodato.github.io/visual-git-guide/index-zh-cn.html#cherry-pick">cherry-pick</a></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_7ce9b.webp"></p><p>上面的命令都在<em>topic</em>分支中进行，而不是<em>main</em>分支，在<em>main</em>分支上重演，并且把分支指向新的节点。注意旧提交没有被引用，将被回收。</p><p>要限制回滚范围，使用<code>--onto</code>选项。下面的命令在<em>main</em>分支上重演当前分支从<em>169a6</em>以来的最近几个提交，即<em>2c33a</em>。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/software/git_56bb8.webp"></p><p>同样有<code>git rebase --interactive</code>让你更方便的完成一些复杂操作，比如丢弃、重排、修改、合并提交。没有图片体现这些，细节看这里:<a href="http://www.kernel.org/pub/software/scm/git/docs/git-rebase.html#_interactive_mode">git-rebase(1)</a></p><h2 id="推荐链接"><a href="#推荐链接" class="headerlink" title="推荐链接"></a>推荐链接</h2><ol><li><a href="https://learngitbranching.js.org/?locale=zh_CN">在线图解练习</a></li><li><a href="http://marklodato.github.io/visual-git-guide/index-zh-cn.html">图解Git</a></li><li><a href="https://www.shortcutfoo.com/app/dojos/git/git-basics/practice">shortcutFoo在线练习</a></li><li><a href="http://git-scm.com/book/zh/v2">Pro Git Book</a></li><li><a href="http://git.oschina.net/progit/">Pro Git Book（Gitee）</a></li><li><a href="http://www.worldhello.net/gotgithub/">GitHub中文指南</a></li><li><a href="http://alx.github.io/gitbook/">Git community book</a></li><li><a href="https://backlog.com/git-tutorial/cn/">猴子都能懂得Git入门</a></li><li><a href="https://gitee.com/DTSDY/git-recipes">Git菜单</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 软件安装与使用 </category>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安装与使用 </tag>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>空域结构概述</title>
      <link href="/vll-pages/posts/ce8f62ac.html"/>
      <url>/vll-pages/posts/ce8f62ac.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 空域管理 </category>
          
          <category> 空域结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>空域结构概述</title>
      <link href="/vll-pages/posts/ce8f62ac.html"/>
      <url>/vll-pages/posts/ce8f62ac.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 空域管理 </category>
          
          <category> 空域管理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 空域管理 </category>
          
          <category> 空域法规 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 空域管理 </category>
          
          <category> 相关项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0️⃣  概率图模型简介</title>
      <link href="/vll-pages/posts/1790fd21.html"/>
      <url>/vll-pages/posts/1790fd21.html</url>
      
        <content type="html"><![CDATA[<script src='https://unpkg.com/tippy.js@2.0.2/dist/tippy.all.min.js'></script><script src='/js/attachTooltips.js'></script><link rel='stylesheet' href='/css/tippy.css'><p>〖摘要〗概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>〖原文〗<a href="https://ermongroup.github.io/cs228-notes/preliminaries/introduction/">Stanford’s CS228</a></p><p>〖参考〗</p><ul><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction.pdf">CMU 10-708 Slides</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/slides/lecture1-Introduction-annotated.pdf">CMU 10-708 Notes</a></li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-17/reading/graphical_model_Jordan.pdf">Jordan’s Textbook</a></li><li><a href="https://dash.harvard.edu/bitstream/handle/1/2757496/Airoldi_GettingStarted.pdf?sequence=4">Airoldi’s Tutorial</a></li></ul><style>p{text-indent:2em}</style><div class="markmap-container" style="height:300px">  <svg data="{&quot;t&quot;:&quot;root&quot;,&quot;d&quot;:0,&quot;v&quot;:&quot;&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;主要模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;有向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;静态贝叶斯网络&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;动态贝叶斯网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;隐马尔可夫模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;卡尔曼滤波器&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;高斯贝叶斯网络&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;无向概率图模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;马尔可夫网络&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;吉布斯/玻尔兹曼机模型&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;条件随机场&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;高斯马尔科夫随机场&quot;}]}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;混合模型&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;主要任务&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;概率图表示&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;有向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;无向概率图&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;混合概率图&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;概率图学习&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;参数/隐变量学习&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;模型结构学习&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;概率图推断&quot;}]}],&quot;p&quot;:{}}"></svg></div><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>概率图模型是机器学习的一个分支，它研究如何使用概率分布来描述世界并对其做出有用的预测。</p><p>学习概率建模的原因有很多。</p><ul><li>一方面，这是一个引人入胜的科学领域，有一个美丽的理论，它以惊人的方式连接了两个非常不同的数学分支：概率论和图论。概率建模也与哲学有着有趣的联系，尤其是因果关系问题。</li><li>同时，概率建模在机器学习和许多实际应用中得到广泛应用。这些技术可用于解决医学、语言处理、视觉和许多其他领域的问题。</li></ul><p>这种优雅的理论与应用相结合，使概率图模型成为现代人工智能和计算机科学中最引人入胜的话题之一。2011 年图灵奖（被认为是计算机科学”“诺贝尔奖”）最近被授予 <a href="http://amturing.acm.org/award_winners/pearl_2658896.cfm">Judea Pearl</a> 以表彰其在概率图建模领域的创立。</p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2 概念"></a>2 概念</h2><p>但是，究竟什么是概率建模？</p><p>当试图用数学解决现实世界的问题时，以方程的形式定义世界的数学模型是很常见的。 也许最简单的模型是以下形式的线性方程</p><p>$$ y &#x3D; \beta^T x $$</p><p>其中 $y$ 是我们想要预测的结果变量，$x$ 是影响结果的已知（给定）变量。例如，$y$ 可能是房子的价格，$x$ 是影响这个价格的一系列因素，例如位置、卧室数量、房子的年龄等。我们假设 $y$ 是这个输入的线性函数（由 $\beta$ 参数化）。</p><p>通常，我们试图建模的现实世界非常复杂。特别是，它往往涉及大量的<em>不确定性</em>（例如，如果新的地铁站在一定距离内开放，房子的价格有一定的上涨机会）。因此，通过以概率分布的形式对世界建模来处理这种不确定性是非常自然的。关于为什么应该使用概率论而不是其他的，请参阅 <a href="http://plato.stanford.edu/entries/dutch-book/">Dutch Book Argument</a> 了解概率。</p><p>$$p(x,y)$$</p><p>给定这样一个模型，我们可以提出诸如“房价在未来五年内上涨的概率是多少？”或“假设房子售价 100,000 美元，它有三间卧室的概率是多少？”之类的问题。建模的概率方面非常重要，因为：</p><ul><li>通常，我们无法完美地预测未来。我们常常对世界没有足够的了解，而且世界本身往往是随机的。</li><li>我们需要评估我们预测的可信度；通常，预测单个值是不够的，我们需要系统输出其对世界上正在发生的事情的信念。</li></ul><p>在本课程中，我们将研究推断不确定性的原则方法，并使用概率论和图论的思想来为这项任务推导出有效的机器学习算法。我们将找到许多有趣问题的答案，例如：</p><ul><li>计算复杂性和概率模型的丰富性之间的权衡是什么？</li><li>在给定固定数据集和计算预算的情况下，推断未来事实的最佳模型是什么？</li><li>如何以一种原则性的方式将先验知识与观测到的证据结合起来进行预测？</li><li>我们如何严格分析$A$是否是$B$的原因，反之亦然？</li></ul><p>此外，我们还将看到许多如何将概率技术应用于各种问题的示例，例如疾病预测、图像理解、语言分析等。</p><h2 id="3-主要难点"><a href="#3-主要难点" class="headerlink" title="3 主要难点"></a>3 主要难点</h2><p>为了初步了解摆在我们面前的挑战，请考虑概率建模的一个简单应用：垃圾邮件分类。</p><p>假设我们有一个模型 $\pt(y, x_1, \dotsc, x_n)$ 在垃圾邮件和非垃圾邮件中出现的单词。每个二进制变量 $x_i$ 对电子邮件中是否存在第 $i$ 个英文单词进行编码；二进制变量 $y$ 指示电子邮件是否为垃圾邮件。为了对一封新邮件进行分类，我们可以查看 $ P(y&#x3D;1 \mid x_1, \dotsc, x_n) $ 的概率。</p><p>我们刚刚定义的函数 $\pt$ 的“大小”是多少？我们的模型为每个输入组合 $y, x_1, \dotsc, x_n$ 定义了一个以 $[0,1]$ 为单位的概率；指定所有这些概率将需要我们写下惊人的 $2^{n+1}$ 不同的值，每个赋值给我们的 $n+1$ 二进制变量。由于 $n$ 是英语词汇的大小，从计算（我们如何存储这个大列表？）和统计（我们如何有效地从有限的数据估计参数？）点，这显然是不切实际的。观点。更一般地说，我们的示例说明了本课程将处理的主要挑战之一：概率本质上是指数大小的对象；我们可以操纵它们的唯一方法是对它们的结构进行简化假设。</p><p>我们将在本课程中做出的主要简化假设是变量之间的<em>条件独立</em>。例如，假设给定 $Y$，英语单词都是条件独立的。换句话说，鉴于一条消息是垃圾邮件，看到两个单词的概率是独立的。这显然过于简单化了，因为“药丸”和“购买”这两个词的概率明显相关；然而，对于大多数单词（例如，“企鹅”和“松饼”）来说，概率确实是独立的，我们的假设不会显著降低模型的准确性。</p><p>我们将这种特定的独立性选择称为<em>朴素贝叶斯</em>假设。给定这个假设，我们可以将模型概率写成因子的乘积</p><p>$$P(y, x_1, \ldots, x_n) &#x3D; p(y) \prod_{i&#x3D;1}^n p(x_i \mid y)$$</p><p>每个因子 $p(x_i \mid y)$ 可以用少量参数（准确地说是 2 个自由度的 4 个参数）完全描述。整个分布由 $O(n)$ 参数参数化，我们可以从数据中轻松估计并做出预测。</p><h2 id="4-技术途径"><a href="#4-技术途径" class="headerlink" title="4 技术途径"></a>4 技术途径</h2><p>我们的独立性假设可以方便地以图的形式表示。朴素贝叶斯垃圾邮件分类模型的图表示。我们可以将有向图解释为表明数据是如何生成的：首先，随机选择垃圾邮件&#x2F;非垃圾邮件标签；然后独立随机抽取 $n$ 个可能的英语单词的子集。</p><p>这种表示具有易于理解的直接优势。它可以解释为告诉我们一个故事：首先随机选择该电子邮件是否为垃圾邮件（以 $y$ 表示），然后一次一个地抽取单词，从而生成一封电子邮件。相反，如果我们有关于如何生成数据集的故事，我们可以自然地将其表示为具有相关概率分布的图。</p><p>更重要的是，我们希望向模型提交各种查询（例如，鉴于我看到“药丸”这个词，垃圾邮件的概率是多少？）；回答这些问题将需要使用图论概念最自然地定义的专门算法。我们还将使用图论来分析学习算法的速度并量化不同学习任务的计算复杂度（例如，NP-hardness）。</p><p>我们想要了解的要点是，概率分布和图之间存在密切联系，我们将在整个课程中利用这些联系来定义、学习和使用概率模型，这就是概率图模型。</p><h3 id="4-1-什么是图模型（Graphical-Model-GM）？"><a href="#4-1-什么是图模型（Graphical-Model-GM）？" class="headerlink" title="4.1 什么是图模型（Graphical Model, GM）？"></a>4.1 什么是图模型（Graphical Model, GM）？</h3><p>图模型用于表示高维空间中的多元分布，图中的结构可以表示变量之间的依赖关系，进而能够有效简化分布的表示。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009114233-3b71.webp" alt="图模型示例"></p><h3 id="4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？"><a href="#4-2-什么是概率图模型（Probabilistic-Graphical-Model，PGM）？" class="headerlink" title="4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？"></a>4.2 什么是概率图模型（Probabilistic Graphical Model，PGM）？</h3><p>如果图模型中的每个节点 $X_i$ 都是条件独立的，则该图模型被称为概率图模型（Probabilistic Graphical Model，PGM）。在概率图模型中，多元变量的联合分布可以被因子化为简单项的乘积，例如： 如果上图是一个概率图的话，则其表示的多元联合概率分布可以因子化为：</p><p>$$<br>P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8) \<br>&#x3D; P(X_1)P(X_2)P(X_3| X_1) P(X_4| X_2)P(X_5| X_2)P(X_6| X_3, X_4) P(X_7| X_6) P(X_8| X_5, X_6)<br>$$</p><div class="note info no-icon flat"><p>请继续关注这些独立性！</p></div><h3 id="4-3-概率图的优势"><a href="#4-3-概率图的优势" class="headerlink" title="4.3 概率图的优势"></a>4.3 概率图的优势</h3><p>（1）优势 1： 能够结合领域知识和因果（逻辑）结构<br>分布的表示成本从 $2^8$ 减少了 $16$ 倍。图模型天然支持数据集成，<br>（2）优势 2：支持异构部件的模块化组合，即数据融合</p><p>（3）优势 3：贝叶斯哲学的天然支持者</p><p>概率图模型 &#x3D; 多元统计     + 结构<br>图模型     &#x3D; 多元对象函数 + 结构</p><h3 id="4-4-概率图到底是什么？"><a href="#4-4-概率图到底是什么？" class="headerlink" title="4.4 概率图到底是什么？"></a>4.4 概率图到底是什么？</h3><p>非正式的简介：概率图模型是一种无需付出指数级成本，即可 『编写&#x2F;指定&#x2F;组合&#x2F;设计』 指数级概率分布的聪明方法。与此同时，它为联合概率分布赋予了结构化的语义。</p><p>更正式的描述：概率图模型指一组随机变量上的分布族，该分布族与连接这些随机变量的图所编码的概率独立性命题之间相互兼容。</p><h3 id="4-5-概率图的类型"><a href="#4-5-概率图的类型" class="headerlink" title="4.5 概率图的类型"></a>4.5 概率图的类型</h3><p>（1）有向图模型：有向边给出因果关系（贝叶斯网络）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121459-80ea.webp" alt="有向图模型"></p><p>图结构： 有向无环图。</p><ul><li>含义：一个节点有条件地独立于其马尔可夫毯之外的网络中的每个其他节点</li><li>局部条件分布 (CPD) 和 DAG 完全确定联合分布。</li><li>提供因果关系，并促进生成过程</li></ul><p>（2）无向图模型：无向边仅给出变量之间的相关性（马尔可夫随机场）</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009121646-3841.webp" alt="无向图模型"></p><p>图结构：无向图</p><ul><li>含义：一个节点在给定其有向邻居的情况下，有条件地独立于网络中的所有其他节点</li><li>局部应急函数（势）和图中的团完全确定了联合分布。</li><li>给出变量之间的相关性，但没有明确的方式来生成样本</li></ul><h3 id="4-6-概率图的结构规范"><a href="#4-6-概率图的结构规范" class="headerlink" title="4.6 概率图的结构规范"></a>4.6 概率图的结构规范</h3><ul><li>概率图中的分离性质表示了相关变量之间的独立性</li><li>为了使概率图有效，需要保证：从概率图中得出的所有条件独立性，都应当与该图所表示的概率分布保持一致。</li><li><strong>等价定理</strong><ul><li>对于图 $G$，令 $D_1$ 表示满足 $I(G)$ 的所有分布族，令 $D_2$ 表示根据 $G$ 分解得出的所有分布族，应当有 $D_1 \equiv D_2$。</li></ul></li></ul><h3 id="4-7-常见概率图模型"><a href="#4-7-常见概率图模型" class="headerlink" title="4.7 常见概率图模型"></a>4.7 常见概率图模型</h3><!-- ![常见模型的概率图表示](https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009122603-6899.webp) --><p>（1）传统的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123128-5f3b.webp" alt="Zoubin 的概率图模型"></p><p>（2）更高级的概率图模型</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123723-b38d.webp" alt="强化学习"></p><figcaption>强化学习模型：部分观测的马尔可夫决策过程（POMDP）</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009123853-ca26.webp" alt="机器翻译"></p><figcaption>机器翻译模型：The HM-BiTAM model</figcaption><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221009124033-2eec.webp" alt="固态物理"></p><figcaption>固态物理： Ising/Potts 模型</figcaption><h3 id="4-8-为何选用概率图？"><a href="#4-8-为何选用概率图？" class="headerlink" title="4.8 为何选用概率图？"></a>4.8 为何选用概率图？</h3><ul><li><p>概率论提供了将各部分结合起来的粘合剂，确保系统作为一个整体是一致的，并提供了将模型与数据接口的方法。</p></li><li><p>图模型的图论支持既提供了一个直观的、吸引人的界面，人类可以通过该界面对高度交互的变量集进行建模，也提供了一种数据结构，可以自然地用于设计高效的通用算法。</p></li><li><p>在统计学、系统工程、信息论、模式识别和统计力学等领域研究的许多经典多元概率系统都是广义图模型形式主义的特例。</p></li><li><p>图模型框架提供了一种将所有这些系统视为某种基本形式主义实例的方法。</p></li></ul><h2 id="5-主要任务"><a href="#5-主要任务" class="headerlink" title="5 主要任务"></a>5 主要任务</h2><p>我们对概率图模型的讨论将分为三个主要部分：表示（如何指定模型）、推断（如何从模型获得答案）和学习（如何使模型适应现实世界的数据）。这三个主题也将密切相关：为了获得有效的推断和学习算法，模型需要被充分表示；此外，学习模型需要将推断作为子程序。因此，最好始终牢记这三个任务，而不是孤立地关注它们。</p><h3 id="5-1-表示"><a href="#5-1-表示" class="headerlink" title="5.1 表示"></a>5.1 表示</h3><p><strong>任务 1：我们如何捕捉（模拟）世界中的不确定性？如何体现我们的领域知识&#x2F;假设&#x2F;约束？</strong></p><p>此类问题的本质是获得关于 <strong>多变量的联合概率分布</strong> 的表示，即 $P(X_1,X_2,&#x2F;lots,X_n)$。</p><p>这并非一个简单问题：我们已经看到一个简单的垃圾邮件分类模型。对于 $n$ 个可能的词通常需要我们指定 $O(2^n)$ 个参数。我们将通过构建易处理的模型来解决这个困难。这些方法将大量使用到图论；概率将由图结构来描述，其属性（例如，连通性、树宽）将揭示模型的一些概率和算法特性（例如：独立性、学习复杂性等）。</p><h3 id="5-2-推断"><a href="#5-2-推断" class="headerlink" title="5.2 推断"></a>5.2 推断</h3><p><strong>任务 2：依据我们的概率模型和（或）给定的数据，如何能够得到世界中关于问题的答案？</strong></p><p>此类问题通常简化为查询某些感兴趣事件的边缘概率或条件概率，例如： $P(X_i \mi \mathcal{D})$。再具体一点，我们通常会对向系统提出两种类型的问题感兴趣：</p><p><strong>（ 1 ）边缘推断 (Marginal Inference)</strong>：在我们将其他所有内容相加后，模型中指定变量的概率是多少？一个典型示例是查询随机选择的房屋中，拥有三间以上卧室的概率。</p><p>$$<br>p(x_1) &#x3D; \sum_{x_2} \sum_{x_3} \cdots \sum_{x_n} p(x_1, x_2, \dotsc, x_n).<br>$$</p><p><strong>（ 2 ）最大后验推断 (MAP)</strong>：寻求最可能的变量赋值。例如，我们可以尝试确定最可能的垃圾邮件，以便解决问题</p><p>$$<br>\underset{x_1, \dots, x_n}{\operatorname{arg max}},p(x_1,\dotsc,x_n, y&#x3D;1).<br>$$</p><p>通常查询将涉及证据（如上面的 MAP 示例），在这种情况下，我们将固定某些变量的赋值。</p><p>事实证明，推断是一项非常具有挑战性的任务。对于许多感兴趣的概率，回答这些问题中的任何一个都是 NP 难的。特别是： <strong>推断是否易于处理将取决于描述该概率的图结构！</strong></p><p>需要提醒的是：当面临非常棘手的问题时，我们还能够通过近似推断方法来获得有价值的结果。有趣的是，这部分课程中描述的算法将在很大程度上基于 20 世纪中叶统计物理学领域所做的工作。</p><h3 id="5-3-学习"><a href="#5-3-学习" class="headerlink" title="5.3 学习"></a>5.3 学习</h3><p><strong>任务 3:如何将模型拟合到数据集？或者说，什么样的模型对于我们的数据（如大量标记的垃圾邮件样本）是 “正确的”？</strong> 例如：  $\mathcal{M} &#x3D; \text{arg max}_{\mathcal{M} \in M} F(\mathcal{D};\mathcal{M})$</p><p>通过分析数据，我们可以推断出一些有用的模式（例如，哪些词在垃圾邮件中出现的频率更高），然后可以使用这些模式来预测未来。不过，学习和推断也能够以某种更微妙的方式内在地联系在一起，后面我们将会看到：推断将成为学习算法中一个反复被调用的关键子程序。</p><p>此外，学习问题与计算学习理论领域（处理有限数据的泛化和过拟合等问题）、贝叶斯统计领域（告诉我们如何以有原则的方式将先验知识和观测到的证据结合在一起）有着重要的联系。</p><h2 id="6-实际应用"><a href="#6-实际应用" class="headerlink" title="6 实际应用"></a>6 实际应用</h2><p>概率图模型在现实世界中有许多应用。在此，我们简单概述以下应用，并且仅仅给出了众多用途中的几个例子。</p><h3 id="6-1-图像"><a href="#6-1-图像" class="headerlink" title="6.1 图像"></a>6.1 图像</h3><p>考虑图像上的分布 $p(\mathbf{x})$，其中 $\mathbf{x}$ 是表示为像素向量的图像，它将高概率分配给看起来逼真的图像，而将低概率分配给其他所有图像。给定这样的模型，我们可以解决如下有趣的问题。</p><h3 id="（1）图像生成"><a href="#（1）图像生成" class="headerlink" title="（1）图像生成"></a>（1）图像生成</h3><p><a href="https://arxiv.org/abs/1511.06434">Radford 等人</a> 训练了一个概率模型 $ p(\mathbf{x}) $，该模型将高概率分配给看起来像卧室的图像。为此，他们在卧室图像数据集上训练了模型，其样本如下所示：</p><p><strong>训练数据</strong> <br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123242-b4cc.webp" alt="卧室 1"></p><p>现在有了这个卧室的概率模型，我们可以通过从这个分布中采样来 <strong>生成</strong> 新的卧室图像。具体来说，新的采样图像 $\hat{\mathbf{x}} \sim p(\mathbf{x})$ 是直接从我们的模型 $p(\mathbf{x})$ 创建的，现在可以生成类似于训练数据集的数据。</p><p>此外，生成模型强大的原因之一在于：其参数比训练它们的数据量少得多（因此，模型必须有效地提取训练数据的精华，以便能够生成新的样本）。可以看到，我们特定的卧室概率模型在捕获数据精华方面做得很好，因此能够生成高度逼真的图像，其中一些示例如下所示：</p><p><strong>生成的数据</strong><br></p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123247-40be.webp" alt="卧室 2"></p><p>同样，我们也可以学习人脸模型。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123250-ca7d.webp" alt="faces1"></p><p>与卧室图像一样，这些面孔完全是合成的，图像中中的并不是真人。</p><p>相同方法可以用于其他对象。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123255-a8d3.webp" alt="antbird"></p><p>请注意，图像并不完美，可能需要细化；但是，采样生成的图像与人们可能期望的非常相似。</p><h3 id="（2）图像填充"><a href="#（2）图像填充" class="headerlink" title="（2）图像填充"></a>（2）图像填充</h3><p>使用相同的面部模型 $p(\mathbf{x})$ ，我们还可以 “填充” 图像的其余部分。例如，给定 $p(\mathbf{x})$ 和某个现有图像的补丁，我们可以从 $p(\textsf{image} \mid \textsf{patch} )$ 中采样，并以不同的可能方式生成补全图像：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123302-9048.webp" alt="inpainting2"></p><p>请注意能够捕获不确定性的概率模型具有非常重要的作用：他们有多种可能的方法来补全图像！</p><h3 id="（3）图像去噪"><a href="#（3）图像去噪" class="headerlink" title="（3）图像去噪"></a>（3）图像去噪</h3><p>同样，给定一张被噪声破坏的图像（例如一张旧照片），我们可以尝试根据图像的概率模型来恢复它。具体来说，我们想要得到一个能够对后验分布 $p(\textsf{original image} \mid \textsf{noisy image})$ 进行良好建模的概率图模型，有了它之后，我们就可以通过采样或精确推断，从观测到的含噪声图像中预测出原始图像。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123305-acb4.webp" alt="图像去噪"></p><h3 id="6-2-自然语言"><a href="#6-2-自然语言" class="headerlink" title="6.2 自然语言"></a>6.2 自然语言</h3><p>了解概率分布还可以帮助我们对自然语言进行建模。在这种情况下，我们想要在单词或字符 $x$ 的序列上构建概率分布 $p(x)$，将高概率分配给正确的（英语）句子。我们可以从各种来源（例如 Wikipedia 文章）中了解此分布。</p><h3 id="（1）生成"><a href="#（1）生成" class="headerlink" title="（1）生成"></a>（1）生成</h3><p>假设我们已经从 Wikipedia 文章中构建了单词序列的概率分布。那么我们就可以从这个分布中进行采样，以生成类似 Wikipedia 的新文章，如下所示。源自： <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008135702-b9af.webp" alt="生成的文章"></p><h3 id="（2）翻译"><a href="#（2）翻译" class="headerlink" title="（2）翻译"></a>（2）翻译</h3><p>假设我们收集了一组用英文和中文转录的段落训练集。我们可以建立一个概率模型 $p(y \mid x)$，以对应的中文句子 $x$ 为条件生成一个英文句子 $y$；这是 <strong>机器翻译</strong> 的一个实例。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123837-5942.webp" alt="神经机器翻译"></p><h3 id="6-3-音频"><a href="#6-3-音频" class="headerlink" title="6.3 音频"></a>6.3 音频</h3><p>我们还可以将概率图模型用于音频应用程序。假设我们在音频信号上构建一个概率分布 $p(x)$，它将高概率分配给听起来像人类语音的信号。</p><h3 id="（1）上采样或超分辨率"><a href="#（1）上采样或超分辨率" class="headerlink" title="（1）上采样或超分辨率"></a>（1）上采样或超分辨率</h3><p>给定音频信号的低分辨率版本，我们可以尝试提高其分辨率。可以将这个问题表述为：给定语音的概率分布 $p(x)$，它 “知道” 典型的人类语音听起来像什么，并且输入了音频信号的一些观测值，我们的目标是计算中间时间点的信号值。在下图中，给定观测到的音频信号（蓝色）和音频概率模型，我们的目的是通过预测中间信号（白色）来重建原始信号（虚线）的更高保真版本。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123330-8349.webp" alt="音频超分辨率"></p><p>我们可以通过对 $p(\textbf{I} \mid \textbf{O})$ 进行采样或执行推断来解决此问题，其中 $\textbf{I}$ 是我们想要预测的中间信号，而 $\textbf{O}$ 是观测到的低分辨率音频信号。</p><p><a href="https://kuleshov.github.io/audio-super-res/">音频信号的超分辨率演示</a></p><h3 id="（2）语音合成"><a href="#（2）语音合成" class="headerlink" title="（2）语音合成"></a>（2）语音合成</h3><p>正如在图像处理中所做的那样，我们还可以对模型进行采样并生成或合成语音信号（文本 → 音频） 。</p><p><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio">音频信号生成模型</a></p><h3 id="（3）语音识别"><a href="#（3）语音识别" class="headerlink" title="（3）语音识别"></a>（3）语音识别</h3><p>给定语音信号和语言（文本形式）的（联合）模型，我们可以尝试从音频信号中推断出语言（文本），实现音频的语音识别。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123340-db11.webp" alt="演讲"></p><h3 id="6-4-科学"><a href="#6-4-科学" class="headerlink" title="6.4 科学"></a>6.4 科学</h3><h3 id="（1）纠错码"><a href="#（1）纠错码" class="headerlink" title="（1）纠错码"></a>（1）纠错码</h3><p>在非理论的世界中，概率模型常用于对通信通道（例如以太网或 Wifi）进行建模。即，如果通过频道发送消息，则由于噪音，您可能会在另一端得到不同的东西。纠错码以及基于概率图模型的技术常被用于检测和纠正通信错误。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123346-d079.webp" alt="编码"></p><h3 id="（2）计算生物学"><a href="#（2）计算生物学" class="headerlink" title="（2）计算生物学"></a>（2）计算生物学</h3><p>概率图模型也广泛用于计算生物学。例如，给定一个 DNA 序列随时间演变的模型，就可以从一组给定物种的 DNA 序列中重建系统发育树。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123356-c303.webp" alt="philo"></p><h3 id="（3）生态"><a href="#（3）生态" class="headerlink" title="（3）生态"></a>（3）生态</h3><p>概率图模型用于研究随空间和时间演变的现象，捕捉空间和时间依赖性。例如，它们可用于研究鸟类迁徙。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008123805-560c.webp" alt="鸟"></p><h3 id="（4）经济学"><a href="#（4）经济学" class="headerlink" title="（4）经济学"></a>（4）经济学</h3><p>概率图模型可用于模拟兴趣量（如基于资产或支出的财富测量值）的空间分布。</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124043-42bf.webp" alt="鸟"></p><p>此处的生态和经济学应用都是所谓的时空模型，它们依赖于跨时间和跨空间收集的数据。</p><h3 id="6-5-健康与医药"><a href="#6-5-健康与医药" class="headerlink" title="6.5 健康与医药"></a>6.5 健康与医药</h3><h3 id="（1）医学诊断"><a href="#（1）医学诊断" class="headerlink" title="（1）医学诊断"></a>（1）医学诊断</h3><p>概率图模型可以帮助医生诊断疾病和预测不良后果。例如，1998 年犹他州盐湖城的 LDS 医院开发了一个用于诊断肺炎的贝叶斯网络模型。他们的模型能够以高敏感性（0.95）和特异性（0.965）区分肺炎患者和其他疾病患者，并在临床上使用了很多年。他们的网络模型概述如下：</p><p><img src="https://xishansnowblog.oss-cn-beijing.aliyuncs.com/images/images/stats-20221008124048-a458.webp" alt="诊断"></p><p>你可以 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232064/">在这里</a> 阅读更多关于他们模型开发的信息。<br><br/></p>]]></content>
      
      
      <categories>
          
          <category> 贝叶斯统计 </category>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯统计 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 简介 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
